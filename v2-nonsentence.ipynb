{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55e2298",
   "metadata": {},
   "source": [
    "# V2: SEQ2SEQ\n",
    "\n",
    "This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention#create_a_tfdata_dataset).\n",
    "\n",
    "Other resources that may be helpful in the future: https://arxiv.org/pdf/2111.10746, https://aclanthology.org/2021.findings-emnlp.393.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71899cc9-86b4-4eda-b525-d818661c70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 22:23:44.633762: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-10 22:23:44.637183: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-10 22:23:44.647186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-10 22:23:44.670967: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-10 22:23:44.678442: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-10 22:23:44.702345: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-10 22:23:45.976658: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa459db2-5a48-4bac-8cbd-38496888faee",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention) (and [at least one other](https://www.tensorflow.org/text/tutorials/text_generation) of the Tensorflow tutorials).\n",
    "- This [blog post](https://janakiev.com/blog/jupyter-virtual-envs/) was referenced to set up the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b150ba-48c0-4f33-8d08-b9f363d6fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2722afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'It and the other noble gases - helium, neon, krypton, xenon, and radon - will react with other substances only under extreme conditions. The noble gases The noble, or inert, gases are helium, neon, argon, krypton, xenon and radon. The rare gases are helium, neon, argon, krypton or xenon. The noble \"gases\" are helium, neon, argon, krypton and xenon. The noble gases are helium, neon, argon, krypton, xenon and radon. These occur for the noble gases helium, neon, argon, krypton, xenon, radon and ununoctium. Because of their chemical inertness, the elements in this group are called the Nobel Gases : Helium Neon Argon Krypton Xenon Radon Group I Elements The elements in this group have one electron in their outer electronic shell. The inert, or noble, gases (helium, neon, argon, krypton, xenon, and radon) all have completely filled outer shells. The other noble gases, which together make about 1% of the Earth\\'s atmosphere, are neon, argon, krypton, xenon and radon. Most of the noble gases-helium, neon, argon, krypton, xenon, and radon-are rare.', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 22:23:47.498980: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-09-10 22:23:47.501014: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "dataset_raw_train = tfds.load('ai2_arc_with_ir', split='train', shuffle_files=True)\n",
    "dataset_raw_test = tfds.load('ai2_arc_with_ir', split='test', shuffle_files=True)\n",
    "dataset_raw = dataset_raw_train.concatenate(dataset_raw_test)\n",
    "dataset_raw = dataset_raw.map(lambda x: x['paragraph'])\n",
    "for i in dataset_raw.take(1):\n",
    "\tprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "100bc5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_paths = list(Path('data/processed/en/').glob('*.txt'))\n",
    "dataset_raw = dataset_raw.concatenate(\n",
    "\ttf.data.TextLineDataset(data_file_paths)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a92fd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw_mlqa = tfds.load('mlqa/en', split='test', shuffle_files=True) # \"train\" not available\n",
    "dataset_raw_mlqa = dataset_raw_mlqa.map(lambda x: x['context'])\n",
    "dataset_raw = dataset_raw_mlqa.concatenate(dataset_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5faca0eb-c672-4d1c-884f-b5ba38a22b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_unsupported_characters(s):\n",
    "\treturn tf.strings.regex_replace(s, r'[^A-Za-z0-9ùúûüÿàâæçéèêëïîôœÙÚÛÜŸÀÂÆÇÉÈÊËÏÎÔŒ \\t\\n.,?!\\-\\':;&]', '')\n",
    "dataset_raw = dataset_raw.map(remove_unsupported_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890fa88",
   "metadata": {},
   "source": [
    "We've now loaded the `.txt` training data files using [`tf.data.TextLineDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset). Each line in the source files is mapped to a new training example. \n",
    "\n",
    "Although some preprocessing has been done by `/data/process_data.py`, paragraphs aren't filtered out based on length/content. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5c607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item tf.Tensor(b'after completing the journey around south america on 23 february 2006 queen mary 2 met her namesake the original rms queen mary which is permanently docked at long beach california', shape=(), dtype=string) tf.Tensor(b'After completing the journey around South America, on 23 February 2006, Queen Mary 2 met her namesake, the original RMS Queen Mary, which is permanently docked at Long Beach, California. ', shape=(), dtype=string)\n",
      "item tf.Tensor(b'escorted by a flotilla of smaller ships the two queens exchanged a whistle salute which was heard throughout the city of long beach', shape=(), dtype=string) tf.Tensor(b' Escorted by a flotilla of smaller ships, the two Queens exchanged a whistle salute which was heard throughout the city of Long Beach. ', shape=(), dtype=string)\n",
      "item tf.Tensor(b'queen mary 2 met the other serving cunard liners queen victoria and queen elizabeth 2 on 13 january 2008 near the statue of liberty in new york city harbour with a celebratory fireworks display queen elizabeth 2 and queen victoria made a tandem crossing of the atlantic for the meeting', shape=(), dtype=string) tf.Tensor(b' Queen Mary 2 met the other serving Cunard liners Queen Victoria and Queen Elizabeth 2 on 13 January 2008 near the Statue of Liberty in New York City harbour, with a celebratory fireworks display; Queen Elizabeth 2 and Queen Victoria made a tandem crossing of the Atlantic for the meeting. ', shape=(), dtype=string)\n",
      "item tf.Tensor(b'this marked the first time three cunard queens have been present in the same location', shape=(), dtype=string) tf.Tensor(b' This marked the first time three Cunard Queens have been present in the same location. ', shape=(), dtype=string)\n",
      "item tf.Tensor(b'cunard stated this would be the last time these three ships would ever meet due to queen elizabeth 2s impending retirement from service in late 2008', shape=(), dtype=string) tf.Tensor(b\" Cunard stated this would be the last time these three ships would ever meet, due to Queen Elizabeth 2's impending retirement from service in late 2008. \", shape=(), dtype=string)\n",
      "item tf.Tensor(b'however this would prove not to be the case as the three queens met in southampton on 22 april 2008', shape=(), dtype=string) tf.Tensor(b' However this would prove not to be the case, as the three Queens met in Southampton on 22 April 2008. ', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 22:23:48.153853: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-09-10 22:23:48.162247: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def break_long_sequences(paragraph):\n",
    "\treturn tf.strings.regex_replace(paragraph, r'.{0,164}[.?]', r'\\0 [SEP]')\n",
    "\n",
    "def split_on_separators(text):\n",
    "\treturn tf.data.Dataset.from_tensor_slices(tf.strings.split(text, '[SEP]'))\n",
    "\n",
    "def filter_paragraphs(context, target):\n",
    "\treturn tf.strings.length(context) > 5 and tf.strings.length(target) > 5\n",
    "\n",
    "punctuation_chars = r'\\?!.,\"\\-\\':;'\n",
    "def add_context(target):\n",
    "\tcontext = tf.strings.regex_replace(target, r'[\\-]{2,}', ' - ')\n",
    "\tcontext = tf.strings.regex_replace(context, r'[\\-\\']', '')\n",
    "\tcontext = tf.strings.regex_replace(context, '[{}]+'.format(punctuation_chars), ' ')\n",
    "\tcontext = tf.strings.strip(\n",
    "\t\ttf.strings.regex_replace(context, '[ ]+', ' ')\n",
    "\t)\n",
    "\tcontext = tf.strings.lower(context)\n",
    "\treturn context, target\n",
    "\n",
    "dataset_raw = dataset_raw.map(break_long_sequences).flat_map(split_on_separators).map(add_context).filter(filter_paragraphs)\n",
    "for text, label in dataset_raw.take(6):\n",
    "\tprint('item', text, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b9d94",
   "metadata": {},
   "source": [
    "Now let's inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3646c389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item b'after completing the journey around south america on 23 february 2006 queen mary 2 met her namesake the original rms queen mary which is permanently docked at long beach california' b'After completing the journey around South America, on 23 February 2006, Queen Mary 2 met her namesake, the original RMS Queen Mary, which is permanently docked at Long Beach, California. '\n",
      "item b'escorted by a flotilla of smaller ships the two queens exchanged a whistle salute which was heard throughout the city of long beach' b' Escorted by a flotilla of smaller ships, the two Queens exchanged a whistle salute which was heard throughout the city of Long Beach. '\n",
      "item b'queen mary 2 met the other serving cunard liners queen victoria and queen elizabeth 2 on 13 january 2008 near the statue of liberty in new york city harbour with a celebratory fireworks display queen elizabeth 2 and queen victoria made a tandem crossing of the atlantic for the meeting' b' Queen Mary 2 met the other serving Cunard liners Queen Victoria and Queen Elizabeth 2 on 13 January 2008 near the Statue of Liberty in New York City harbour, with a celebratory fireworks display; Queen Elizabeth 2 and Queen Victoria made a tandem crossing of the Atlantic for the meeting. '\n",
      "item b'this marked the first time three cunard queens have been present in the same location' b' This marked the first time three Cunard Queens have been present in the same location. '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 22:23:48.442871: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for text, label in dataset_raw.take(4).as_numpy_iterator():\n",
    "\tprint('item', text, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24075a",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "197d8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100_000\n",
    "BATCH_SIZE = 32\n",
    "dataset_train = dataset_raw.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Inspired by https://stackoverflow.com/a/74609848.\n",
    "validate_size = 2\n",
    "dataset_validate = dataset_train.take(validate_size)\n",
    "dataset_train = dataset_train.skip(validate_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532db30",
   "metadata": {},
   "source": [
    "### Preparing to process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa159e6",
   "metadata": {},
   "source": [
    "The [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer takes a `standardize` option that preprocesses input data. The default removes punctuation, but we don't want that. Let's redefine it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f74b9fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"[START] thi [s] is a test , it ' s work [ing] , , [END]\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def standardize_tf_text(text):\n",
    "\tpunctuation_regex = '[{p}]'.format(p = punctuation_chars)\n",
    "\n",
    "\t# Clean up &quot artifacts\n",
    "\ttext = tf.strings.regex_replace(text, r'&quot;?', r' \" ')\n",
    "\n",
    "\t# Surround punctuation with spaces for easier tokenization\n",
    "\ttext = tf.strings.regex_replace(text, punctuation_regex, r' \\0 ')\n",
    "\n",
    "\t# Replace punctuation this task isn't concerned with\n",
    "\ttext = tf.strings.regex_replace(text, r' [.?!] ', '')\n",
    "\n",
    "\t# Remove repeated spaces\n",
    "\ttext = tf.strings.regex_replace(text, r'\\s+', ' ')\n",
    "\n",
    "\t# Add a special \"capitalize the next letter\" token\n",
    "\t# Disabled for this part:\n",
    "\t# text = tf.strings.regex_replace(text, r'(\\s|^)([A-Z])', r' [CAP] \\2')\n",
    "\n",
    "\t# Lowercase everything\n",
    "\ttext = tf.strings.lower(text)\n",
    "\n",
    "\t# English specific: Move -ing, -s suffixes to new words\n",
    "\ttext = tf.strings.regex_replace(text, r'([a-z]{3,})(ing|er|ed|ily|ly|ish|s)(\\s|$)', r'\\1 [\\2]\\3')\n",
    "\n",
    "\t# Remove leading and trailing spaces\n",
    "\ttext = tf.strings.strip(text)\n",
    "\n",
    "\t# Add sequence markings\n",
    "\treturn tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "\n",
    "print(standardize_tf_text('This is a test! It\\'s working?!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018456b9",
   "metadata": {},
   "source": [
    "The text standardization function can now be used to preprocess text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87815e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 22:23:58.837566: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:63: Filling up shuffle buffer (this may take a while): 53084 of 100000\n",
      "2024-09-10 22:24:07.591801: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 14 target words: ['', '[UNK]', ',', '[s]', 'the', '[ed]', '[START]', '[END]', 'of', 'and', '[er]', 'to', '[ing]', 'in']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 22:25:04.800246: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Keep only the 4000 most commonly used tokens\n",
    "max_vocab_size = 4000\n",
    "\n",
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "\tstandardize=standardize_tf_text,\n",
    "\tmax_tokens=max_vocab_size,\n",
    "\t# Allow entries of different lengths\n",
    "\tragged=True,\n",
    ")\n",
    "target_text_processor.adapt(dataset_train.map(lambda context, target: target))\n",
    "\n",
    "print('First 14 target words:', target_text_processor.get_vocabulary()[:14])\n",
    "\n",
    "# The target data should be roughly equivalent to the context data, except have additional (punctuation)\n",
    "# tokens.\n",
    "context_text_processor = target_text_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d66c3",
   "metadata": {},
   "source": [
    "We can use these layers to convert to/from token IDs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b4789f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokens tf.Tensor(\n",
      "[   6    1  125   43    3   20   14  546    1   20 3835   12   43    3\n",
      "    7], shape=(15,), dtype=int64)\n",
      "Back to text [START] [UNK] world thi [s] is a test [UNK] is process [ing] thi [s] [END]\n"
     ]
    }
   ],
   "source": [
    "example_text = 'hello world this is a test tensorflow is processing this'\n",
    "example_tokens = context_text_processor(example_text)\n",
    "print('Example tokens', example_tokens)\n",
    "\n",
    "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
    "tokens = context_vocab[example_tokens.numpy()]\n",
    "print('Back to text', ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d8e67",
   "metadata": {},
   "source": [
    "### Processing the data\n",
    "\n",
    "Now, we'll:\n",
    "1. Map the data through the text processors we just made.\n",
    "2. Shift the target data, so that our network is provided with a history of generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2efef357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre tf.Tensor([  6  43   3  20  14 546   7], shape=(7,), dtype=int64)\n",
      "<_MapDataset element_spec=(RaggedTensorSpec(TensorShape([None, None]), tf.int64, 1, tf.int64), RaggedTensorSpec(TensorShape([None, None]), tf.int64, 1, tf.int64))>\n",
      "post\n"
     ]
    }
   ],
   "source": [
    "def process_text(context, target):\n",
    "\treturn context_text_processor(context), target_text_processor(target)\n",
    "\n",
    "print('pre', context_text_processor('this is a test'))\n",
    "print(dataset_train.map(process_text))\n",
    "print('post')\n",
    "\n",
    "def add_target_history(context, target):\n",
    "\t# .to_tensor(): Converts from RaggedTensors to Tensors.\n",
    "\t# We give our network the history as target_in\n",
    "\ttarget_in = target[:, :-1].to_tensor()\n",
    "\ttarget_out = target[:, 1:].to_tensor()\n",
    "\treturn (context.to_tensor(), target_in), target_out\n",
    "dataset_train = dataset_train.map(process_text).map(add_target_history).repeat()\n",
    "dataset_validate = dataset_validate.map(process_text).map(add_target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50594897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 22:25:15.873282: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:63: Filling up shuffle buffer (this may take a while): 52483 of 100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context [START],in,ancient,time,[s],greek,[s],and,persian,[s],had,been,at,war,for,centurie,[s],and,the,persian,priest,[s],call,[ed],[UNK],in,persian,came,to,be,known,as,[UNK],in,greek,[END],,,,,,,,,,,,,,,,,,,,,,\n",
      "target_in [START],in,ancient,time,[s],,,greek,[s],and,persian,[s],had,been,at,war,for,centurie,[s],,,and,the,persian,priest,[s],,,call,[ed],[UNK],in,persian,,,came,to,be,known,as,[UNK],in,greek,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "target_out in,ancient,time,[s],,,greek,[s],and,persian,[s],had,been,at,war,for,centurie,[s],,,and,the,persian,priest,[s],,,call,[ed],[UNK],in,persian,,,came,to,be,known,as,[UNK],in,greek,,,[END],,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 22:25:20.275417: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def inspect_dataset(dataset: tf.data.Dataset):\n",
    "\ttarget_vocab = np.array(target_text_processor.get_vocabulary())\n",
    "\tfor (context, target_in), target_out in dataset.take(1):\n",
    "\t\tcontext_words = context_vocab[context[0]]\n",
    "\t\tprint('context', ','.join(context_words))\n",
    "\t\tprint('target_in', ','.join(target_vocab[target_in[0]]))\n",
    "\t\tprint('target_out', ','.join(target_vocab[target_out[0]]))\n",
    "\n",
    "inspect_dataset(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63bc6c",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### The encoder\n",
    "\n",
    "See https://www.tensorflow.org/text/tutorials/nmt_with_attention#the_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83f026f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Layer):\n",
    "\tdef __init__(self, text_processor, units: int):\n",
    "\t\t\"\"\"\n",
    "\t\tCreates a new Encoder layer. [dimen] is the maxiumum number of elements of the input\n",
    "\t\tthat can be processed by the encoder.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.text_processor = text_processor\n",
    "\t\tself.vocab_size = text_processor.vocabulary_size()\n",
    "\t\tself.units = units\n",
    "\n",
    "\t\t# Converts tokens -> vectors\n",
    "\t\tself.embedding = tf.keras.layers.Embedding(\n",
    "\t\t\t# mask_zero: Treats zero as a padding value that should be ignored\n",
    "\t\t\tself.vocab_size, units, mask_zero = True,\n",
    "\t\t)\n",
    "\t\tgru = tf.keras.layers.GRU(\n",
    "\t\t\tunits, return_sequences = True,\n",
    "\t\t\t# Use the recurrent_initializer suggested by the tutorial (& the default\n",
    "\t\t\t# for kernel_initializer).\n",
    "\t\t\trecurrent_initializer='glorot_uniform'\n",
    "\t\t)\n",
    "\t\tself.rnn = tf.keras.layers.Bidirectional(\n",
    "\t\t\t# merge_mode determines how the forward and backward layers are combined\n",
    "\t\t\t#            'concat' is another option here\n",
    "\t\t\tmerge_mode = 'sum',\n",
    "\t\t\tlayer=gru,\n",
    "\t\t)\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\tx = self.embedding(x)\n",
    "\t\tx = self.rnn(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef prepare_for_input(self, texts):\n",
    "\t\t\"\"\"\n",
    "\t\tUtility method that converts `texts` to a form that can be provided to the `call` method.\n",
    "\t\t\"\"\"\n",
    "\t\ttexts = tf.convert_to_tensor(texts)\n",
    "\t\tif len(texts.shape) == 0:\n",
    "\t\t\ttexts = texts[None]\n",
    "\t\tcontext = self.text_processor(texts).to_tensor()\n",
    "\t\treturn context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea196ff",
   "metadata": {},
   "source": [
    "Try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "331e474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens shape (batch, s): (16, 57)\n",
      "Encoder output shape (batch, s, ENCODER_UNITS): (16, 57, 256)\n"
     ]
    }
   ],
   "source": [
    "ENCODER_UNITS = 256\n",
    "encoder = Encoder(context_text_processor, ENCODER_UNITS)\n",
    "\n",
    "for (context, target_history), target_next in dataset_validate.take(1):\n",
    "\tencoder_result = encoder(context)\n",
    "\tprint('Context tokens shape (batch, s):', context.shape)\n",
    "\tprint('Encoder output shape (batch, s, ENCODER_UNITS):', encoder_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd7fb6",
   "metadata": {},
   "source": [
    "### The attention layer\n",
    "\n",
    "Attention can be thought of as training a lookup table with keys and values. The lookup table has inputs `values` and `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "781b2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.Layer):\n",
    "\tdef __init__(self, units, **kwargs):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.attention_layer = tf.keras.layers.MultiHeadAttention(\n",
    "\t\t\tkey_dim=units,\n",
    "\t\t\tnum_heads=1,\n",
    "\t\t\t**kwargs\n",
    "\t\t)\n",
    "\t\t# Keeps \"the mean activation within each example close to 0 and the\n",
    "\t\t# activation standard deviation close to 1\" -- https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization?hl=en\n",
    "\t\tself.norm_layer = tf.keras.layers.LayerNormalization()\n",
    "\t\tself.add_layer = tf.keras.layers.Add()\n",
    "\t\tself.supports_masking = True\n",
    "\n",
    "\tdef call(self, query, value):\n",
    "\t\tattention_output = self.attention_layer(\n",
    "\t\t\tquery = query,\n",
    "\t\t\tvalue = value,\n",
    "\t\t\t#use_causal_mask=True,\n",
    "\t\t\t# Return the attention scores for latter plotting\n",
    "\t\t\t# return_attention_scores = True,\n",
    "\t\t)\n",
    "\n",
    "\t\tx = self.add_layer([ query, attention_output ])\n",
    "\t\tx = self.norm_layer(x)\n",
    "\t\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c8857ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded context sequence shape (batch, s, units): (16, 47, 256)\n",
      "Target history sequence shape (batch, t, units): (16, 52, 256)\n",
      "Attention result shape (batch, t, units): (16, 52, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(ENCODER_UNITS)\n",
    "\n",
    "# Test with an example\n",
    "for (context, target_history), target_next in dataset_validate.take(1):\n",
    "\tembed_layer = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(), output_dim=ENCODER_UNITS, mask_zero=True)\n",
    "\ttarget_embed = embed_layer(target_history)\n",
    "\tencoded_context = encoder(context)\n",
    "\tattention_result = attention_layer(target_embed, encoded_context)\n",
    "\n",
    "\tprint('Encoded context sequence shape (batch, s, units):', encoded_context.shape)\n",
    "\tprint('Target history sequence shape (batch, t, units):', target_embed.shape)\n",
    "\tprint('Attention result shape (batch, t, units):', attention_result.shape)\n",
    "\n",
    "\t# Used later \n",
    "\ttest_encoded_context = encoded_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5577a1",
   "metadata": {},
   "source": [
    "### The decoder\n",
    "\n",
    "The decoder produces queries for the attention layer. The decoder operates on `target_history`. At each step during training, it should have no information about future target output (that's what we're trying to determine). As such, we use a unidirectional RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2454259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDense(tf.keras.layers.Dense):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tsuper(CustomDense, self).__init__(*args, **kwargs)\n",
    "\t\n",
    "\tdef compute_mask(self, _inputs, mask=None):\n",
    "\t\treturn mask\n",
    "\n",
    "class Decoder(tf.keras.Layer):\n",
    "\tdef __init__(self, text_processor, units):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.text_processor = text_processor\n",
    "\t\tself.vocab_size = text_processor.vocabulary_size()\n",
    "\t\tself.units = units\n",
    "\n",
    "\t\tself.embedding_layer = tf.keras.layers.Embedding(\n",
    "\t\t\t# mask_zero: Treats zero as a padding value that should be ignored\n",
    "\t\t\tself.vocab_size, units, mask_zero = True,\n",
    "\t\t)\n",
    "\t\tself.rnn_layer = tf.keras.layers.GRU(\n",
    "\t\t\tunits, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform',\n",
    "\t\t)\n",
    "\t\tself.attention_layer = CrossAttention(units)\n",
    "\n",
    "\t\t# Creates logits with the estimated probability of each output token\n",
    "\t\tself.output_layer = CustomDense(self.vocab_size)\n",
    "\n",
    "\t\t# Conversion:\n",
    "\t\tself.word_to_id = tf.keras.layers.StringLookup(\n",
    "\t\t\tvocabulary = text_processor.get_vocabulary(),\n",
    "\t\t\tmask_token = '',\n",
    "\t\t\toov_token = '[UNK]',\n",
    "\t\t)\n",
    "\t\tself.id_to_word = tf.keras.layers.StringLookup(\n",
    "\t\t\tvocabulary = text_processor.get_vocabulary(),\n",
    "\t\t\tmask_token = '',\n",
    "\t\t\toov_token = '[UNK]',\n",
    "\t\t\tinvert = True,\n",
    "\t\t)\n",
    "\t\t# Pre-computing these simplifies exporting\n",
    "\t\tself.start_id = self.word_to_id('[START]')\n",
    "\t\tself.end_id = self.word_to_id('[END]')\n",
    "\n",
    "\t\tself.supports_masking = True\n",
    "\t\n",
    "\tdef build(self, input_shape):\n",
    "\t\t# Nothing tha needs a size allocation based on the input shape\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef call(self, context, target_history, state = None, return_state = False):\n",
    "\t\tx = self.embedding_layer(target_history)\n",
    "\t\tx, state = self.rnn_layer(x, initial_state = state)\n",
    "\t\tx = self.attention_layer(x, context)\n",
    "\n",
    "\t\tlogits = self.output_layer(x)\n",
    "\t\tif return_state:\n",
    "\t\t\treturn logits, state\n",
    "\t\telse:\n",
    "\t\t\treturn logits\n",
    "\t\n",
    "\t## Conversion/testing ##\n",
    "\n",
    "\tdef tokens_to_text(self, tokens):\n",
    "\t\ttext = tf.strings.reduce_join(self.id_to_word(tokens), separator = ' ')\n",
    "\t\ttext = tf.strings.regex_replace(text, r'\\s*\\[START\\]\\s*', '')\n",
    "\t\ttext = tf.strings.regex_replace(text, r'\\s*\\[END\\].*$', '')\n",
    "\t\treturn text\n",
    "\n",
    "\tdef generate_next_token(self, context, target_history, done_vec, state, temperature = 0.0):\n",
    "\t\t# Note: is_done is a vector, indicating whether each item in the batch is done\n",
    "\n",
    "\t\tlogits, state = self(context, target_history, state = state, return_state = True)\n",
    "\n",
    "\t\t# logits has shape (batch, t, target_vocab_size). Only generate the token corresponding\n",
    "\t\t# to the last logits in the sequence (at t - 1)\n",
    "\t\tif temperature > 0:\n",
    "\t\t\tnext_token = tf.where(\n",
    "\t\t\t\tdone_vec,\n",
    "\t\t\t\ttf.constant(0, dtype=tf.int64), # Emit 0 after a sequence is done\n",
    "\t\t\t\ttf.random.categorical(logits[:, -1, :] / temperature, num_samples = 1), # Otherwise, pick the token from a categorical distribution\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tnext_token = tf.math.argmax(logits, axis=-1)\n",
    "\t\tdone_vec = done_vec|(next_token == self.end_id)\n",
    "\t\treturn next_token, done_vec, state\n",
    "\t\n",
    "\tdef get_initial_state(self, context):\n",
    "\t\t# context has shape (batch_size, s, units)\n",
    "\t\tbatch_size = tf.shape(context)[0]\n",
    "\t\tstart_tokens = tf.fill([batch_size, 1], self.start_id)\n",
    "\t\tdone_vec = tf.zeros([batch_size, 1], dtype = tf.bool)\n",
    "\n",
    "\t\t# From the Tensorflow source code:\n",
    "\t\t# > RNN expect the states in a list, even if single state.\n",
    "\t\t# Note: Without the [0] we get a type mismatch while exporting.\n",
    "\t\tinitial_state = self.rnn_layer.get_initial_state(batch_size)[0]\n",
    "\n",
    "\t\treturn start_tokens, done_vec, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d5ccb1",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "219a3522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: b'healthy finish fetch strong 400 37 sake slipphealthy finish fetch strong 400 37 sake slipphealthy finish fetch strong 400 37 sake slipp'\n"
     ]
    }
   ],
   "source": [
    "def test_generation_loop():\n",
    "\tdecoder = Decoder(target_text_processor, ENCODER_UNITS)\n",
    "\tnext_token, done_vec, state = decoder.get_initial_state(test_encoded_context[:3, :, :])\n",
    "\ttokens = [next_token]\n",
    "\n",
    "\tfor i in range(8):\n",
    "\t\tnext_token, done_vec, state = decoder.generate_next_token(test_encoded_context[:3, :, :], next_token, done_vec, state)\n",
    "\t\ttokens.append(next_token)\n",
    "\t\n",
    "\t# Merge all batch outputs into a single dimension\n",
    "\ttokens = tf.concat(tokens, -1) # -1 = last axis\n",
    "\n",
    "\tprint('Output:', decoder.tokens_to_text(tokens).numpy())\n",
    "\n",
    "test_generation_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3411a",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b3a40",
   "metadata": {},
   "source": [
    "We can now build a model for training and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1520bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Punctuator(tf.keras.Model):\n",
    "\tdef __init__(self, units, context_text_processor, target_text_processor):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encoder = Encoder(context_text_processor, units)\n",
    "\t\tself.decoder = Decoder(target_text_processor, units)\n",
    "\t\n",
    "\tdef call(self, inputs):\n",
    "\t\tcontext, target_history = inputs\n",
    "\t\tcontext = self.encoder(context)\n",
    "\t\tlogits = self.decoder(context, target_history)\n",
    "\t\treturn logits\n",
    "\t\n",
    "\tdef fix_punctuation_raw(self, input):\n",
    "\t\t\"\"\"\n",
    "\t\tAdds punctuation to `input`, where `input` is a `Tensor` with shape (batch_size, s) where s is the\n",
    "\t\tcontext length.\n",
    "\t\t\"\"\"\n",
    "\t\tcontext = self.encoder(input)\n",
    "\n",
    "\t\tnext_token, done_vec, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "\t\t# Although a TensorArray would allow more efficient exporting, the ONNX exporter seems to\n",
    "\t\t# have trouble with it. For now, use a Python list.\n",
    "\t\ttokens = []\n",
    "\t\tmax_iterations = 56\n",
    "\n",
    "\t\tfor i in range(max_iterations):\n",
    "\t\t\t# token_history has size: (batch, t, target_vocab_size)\n",
    "\t\t\t# token_history = tf.concat(tokens, 1)\n",
    "\t\t\t# print('history', model.decoder.id_to_word(token_history))\n",
    "\t\t\tnext_token, done_vec, state = self.decoder.generate_next_token(context, next_token, done_vec, state, temperature=0)\n",
    "\t\t\t#tokens = tokens.write(i + 1, next_token)\n",
    "\t\t\ttokens.append(next_token)\n",
    "\n",
    "\t\t\tif tf.executing_eagerly() and tf.reduce_all(done_vec):\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\ttokens = tf.concat(tokens, -1)\n",
    "\t\treturn tokens\n",
    "\n",
    "\tdef fix_punctuation(self, text: list[str]):\n",
    "\t\tinputs = self.encoder.prepare_for_input(text)\n",
    "\t\ttokens = self.fix_punctuation_raw(inputs)\n",
    "\t\treturn self.decoder.tokens_to_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "253c670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens shape (batch, s): (16, 83)\n",
      "Target history tokens shape (batch, t): (16, 92)\n",
      "Logits shape (batch, t, vocab_size) (16, 92, 4000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 22:25:50.110662: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "model = Punctuator(ENCODER_UNITS, context_text_processor, target_text_processor)\n",
    "\n",
    "for (example_context_tok, example_target_hist), _ in dataset_validate.take(1):\n",
    "\ttest_logits = model((example_context_tok, example_target_hist))\n",
    "\tprint('Context tokens shape (batch, s):', example_context_tok.shape)\n",
    "\tprint('Target history tokens shape (batch, t):', example_target_hist.shape)\n",
    "\tprint('Logits shape (batch, t, vocab_size)', test_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68e81b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"punctuator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"punctuator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,813,504</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,710,432</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m1,813,504\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m2,710,432\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,523,936</span> (17.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,523,936\u001b[0m (17.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,523,936</span> (17.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,523,936\u001b[0m (17.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85551c17",
   "metadata": {},
   "source": [
    "To avoid penalizing masked outputs, we use a custom loss function (see the tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ca4d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss(y_true, y_predict):\n",
    "\tloss = base_loss_fn(y_true, y_predict)\n",
    "\t\n",
    "\tunmasked = y_true != 0\n",
    "\tunmasked = tf.cast(unmasked, loss.dtype)\n",
    "\t# Only consider output with a corresponding label.\n",
    "\tloss *= unmasked\n",
    "\n",
    "\tcount_unmasked = tf.math.reduce_sum(unmasked)\n",
    "\n",
    "\t# reduce_sum: Adds all entries of a vector.\n",
    "\treturn tf.math.reduce_sum(loss)/count_unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ba83b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, predict_logits):\n",
    "\tpredicted_index = tf.math.argmax(predict_logits, axis=-1)\n",
    "\tpredicted_index = tf.cast(predicted_index, y_true.dtype)\n",
    "\n",
    "\tmatch = tf.cast(y_true == predicted_index, tf.float32)\n",
    "\tunmasked = tf.cast(y_true != 0, tf.float32)\n",
    "\tcount_unmasked = tf.math.reduce_sum(unmasked)\n",
    "\n",
    "\treturn tf.math.reduce_sum(match * unmasked) / count_unmasked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d9fde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[masked_accuracy, masked_loss])\n",
    "model.compile(optimizer='adam', loss=masked_loss, metrics=[masked_accuracy, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95ae7f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the tutorial:\n",
      "expected loss 8.294049\n",
      "expected accuracy 0.00025\n"
     ]
    }
   ],
   "source": [
    "print('From the tutorial:')\n",
    "vocab_size = float(target_text_processor.vocabulary_size())\n",
    "\n",
    "print('expected loss', tf.math.log(vocab_size).numpy())\n",
    "print('expected accuracy', 1/vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "091b83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 8.3182 - masked_accuracy: 0.0013 - masked_loss: 5.8228 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 8.317972183227539,\n",
       " 'masked_accuracy': 0.001192605821415782,\n",
       " 'masked_loss': 5.545314788818359}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset_validate, steps=20, return_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a179e6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[test]: suppose fate cristo velocity bless tie beside meet route hollow walt stair dar movement writer oh resemble windsor gate 1996 hundred anxiety survey 1980 x struck cricket septemb eight within coastal formation handkerchief decision globe mysteriou earn symbol establ examination 13 1948 test sharp try 1985 talk afraid attitude worse knot promote gratitude preserve catholic reaction'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def test_punctuation(text):\n",
    "\treturn '[test]: ' + model.fix_punctuation(text).numpy().decode('utf-8')\n",
    "\n",
    "class DemoCallback(tf.keras.callbacks.Callback):\n",
    "\tdef on_epoch_end(self, epoch_index: int, logs = None):\n",
    "\t\tprint('\\r', test_punctuation([ 'im testing this models performance how well is it working' ]))\n",
    "\t\tprint(test_punctuation([ 'i think its working well but its really hard to tell why are the question marks missing' ]))\n",
    "\t\tif epoch_index % 3 == 0:\n",
    "\t\t\t# From the test data\n",
    "\t\t\tprint(test_punctuation([\n",
    "\t\t\t\t'not that alice had any idea of doing that she felt as if she would never be able to talk again she was getting so much out of breath and still the queen cried faster faster and dragged her along'\n",
    "\t\t\t]))\n",
    "\t\t\tprint(test_punctuation([ 'tensorflow is a library that is used for machine learning it is available for more languages than just python' ]))\n",
    "\t\t\tprint(test_punctuation([ 'the joplin note taking app can be used to take multimedia notes' ]))\n",
    "\t\t\tprint(test_punctuation([ 'here are a few words javascript typescript python joplin interesting loud and sequence these words are all very useful' ]))\n",
    "\n",
    "test_punctuation(tf.constant([ 'this is an example they said' ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2db06e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 22:26:16.769648: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:63: Filling up shuffle buffer (this may take a while): 97268 of 100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/1200\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:12:46\u001b[0m 16s/step - loss: 8.3111 - masked_accuracy: 0.0000e+00 - masked_loss: 8.3111"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 22:26:16.998084: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 620/1200\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2:31\u001b[0m 262ms/step - loss: 4.7711 - masked_accuracy: 0.2643 - masked_loss: 4.7711"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m\t\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_validate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mDemoCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "\tdataset_train,\n",
    "\tepochs = 30,\n",
    "\tsteps_per_epoch = 1200,\n",
    "\tvalidation_data = dataset_validate,\n",
    "\tcallbacks=[DemoCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f68a9fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would nev [er] be able to talk again , she was gett [ing] so much out of breath , and still the queen ! cri [ed] fast [er] , him , and her [ed] her of her along .\n",
      "[test]: [cap] thi [s] is a test of the [UNK] system for [cap] i am curiou [s] how well it work [s] will it work .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_punctuation([\n",
    "\t'not that alice had any idea of doing that she felt as if she would never be able to talk again she was getting so much out of breath and still the queen cried faster faster and dragged her along'\n",
    "]))\n",
    "print(test_punctuation([ 'this is a test of the punctuation system for i am curious how well it works will it work' ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e9cc9",
   "metadata": {},
   "source": [
    "## Exporting\n",
    "\n",
    "Based on the [Export](https://www.tensorflow.org/text/tutorials/nmt_with_attention#export) section of the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9885c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Export(tf.Module):\n",
    "\tdef __init__(self, model):\n",
    "\t\tself.model = model\n",
    "\t\n",
    "\t@tf.function(input_signature=[tf.RaggedTensorSpec(dtype=tf.int64, shape=[None])])\n",
    "\tdef fix_punctuation(self, input):\n",
    "\t\t# Returns encoded tokens\n",
    "\t\treturn model.fix_punctuation_raw(\n",
    "\t\t\ttf.reshape(input, [1, -1])\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcac14",
   "metadata": {},
   "source": [
    "Run `fix_punctuation` once to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa037d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = Export(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8af27599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'[cap] thi [s] sentence shall be [UNK] [ed] for the follow [ing] reason [s] , first [UNK] make [s] thing [s] easi [er] to read second [UNK] .'>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_inputs = context_text_processor('this sentence shall be punctuated for the following reasons first punctatuion makes things easier to read second um')\n",
    "model.decoder.tokens_to_text(export.fix_punctuation(sample_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70247292",
   "metadata": {},
   "source": [
    "Now we save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "badb8d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: punctuator-seq2seq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: punctuator-seq2seq/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(export, 'punctuator-seq2seq', signatures={ 'serving_default': export.fix_punctuation })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197976b4",
   "metadata": {},
   "source": [
    "See [the documentation](https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export) for information about the `signatures` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "56d91fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43714"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "web_output_dir = Path('web')\n",
    "vocab_output_file = web_output_dir / 'wordEncodings.ts'\n",
    "\n",
    "vocab_output_file.write_text('''\n",
    "// Auto-generated file!\n",
    "// Created by v2-seq2seq.ipynb\n",
    "export default {};\n",
    "'''.format(json.dumps(target_text_processor.get_vocabulary(), indent = '\\t')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d873f2",
   "metadata": {},
   "source": [
    "### Testing the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c39fbc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported and warmed up!\n"
     ]
    }
   ],
   "source": [
    "reloaded = tf.saved_model.load('punctuator-seq2seq')\n",
    "# Warmup\n",
    "reloaded.fix_punctuation(sample_inputs)\n",
    "print('Imported and warmed up!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca974c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 135 ms, sys: 43.9 ms, total: 179 ms\n",
      "Wall time: 93.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'[cap] thi [s] sentence shall be [UNK] [ed] for the follow [ing] reason [s] , first [UNK] make [s] thing [s] easi [er] to read second [UNK] .'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.decoder.tokens_to_text(reloaded.fix_punctuation(sample_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "afb2771e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(27,), dtype=int64, numpy=\n",
       "array([   6,   46,    3, 2257,  173,   39,    1,    9,   31,    5,  159,\n",
       "         14,  608,    3,  130,    1,  119,    3,  220,    3,  792,   12,\n",
       "         13,  459,  313,    1,    7])>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text_processor('this sentence shall be punctuated for the following reasons first punctatuion makes things easier to read second um')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa1bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
