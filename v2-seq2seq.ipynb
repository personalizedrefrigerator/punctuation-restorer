{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55e2298",
   "metadata": {},
   "source": [
    "# V2: SEQ2SEQ\n",
    "\n",
    "This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention#create_a_tfdata_dataset).\n",
    "\n",
    "Other resources that may be helpful in the future: https://arxiv.org/pdf/2111.10746, https://aclanthology.org/2021.findings-emnlp.393.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71899cc9-86b4-4eda-b525-d818661c70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 18:47:17.463055: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-06 18:47:17.467372: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-06 18:47:17.476769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-06 18:47:17.496064: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-06 18:47:17.507407: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-06 18:47:17.531750: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-06 18:47:18.484989: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa459db2-5a48-4bac-8cbd-38496888faee",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention) (and [at least one other](https://www.tensorflow.org/text/tutorials/text_generation) of the Tensorflow tutorials).\n",
    "- This [blog post](https://janakiev.com/blog/jupyter-virtual-envs/) was referenced to set up the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b150ba-48c0-4f33-8d08-b9f363d6fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Tuple\n",
    "from IPython.display import display, Markdown\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5faca0eb-c672-4d1c-884f-b5ba38a22b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_paths = list(Path('data/processed/en/').glob('*.txt'))\n",
    "dataset_raw = tf.data.TextLineDataset(\n",
    "\tdata_file_paths,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890fa88",
   "metadata": {},
   "source": [
    "We've now loaded the `.txt` training data files using [`tf.data.TextLineDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset). Each line in the source files is mapped to a new training example. \n",
    "\n",
    "Although some preprocessing has been done by `/data/process_data.py`, paragraphs aren't filtered out based on length/content. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5c607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_paragraphs(context, target):\n",
    "\treturn tf.strings.length(context) > 5\n",
    "\n",
    "punctuation_chars = r'\\?!.,\"\\-\\':;'\n",
    "def add_context(target):\n",
    "\tcontext = tf.strings.regex_replace(target, '[{}]+'.format(punctuation_chars), '')\n",
    "\tcontext = tf.strings.strip(\n",
    "\t\ttf.strings.regex_replace(context, '[ ]+', ' ')\n",
    "\t)\n",
    "\tcontext = tf.strings.lower(context)\n",
    "\treturn context, target\n",
    "\n",
    "dataset_raw = dataset_raw.map(add_context).filter(filter_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b9d94",
   "metadata": {},
   "source": [
    "Now let's inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3646c389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'illustration' b'Illustration '\n",
      "b'alices adventures in wonderland' b\"Alice's Adventures in Wonderland\"\n",
      "b'by lewis carroll' b'by Lewis Carroll'\n",
      "b'the millennium fulcrum edition 30' b'THE MILLENNIUM FULCRUM EDITION 3.0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 18:47:19.213772: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for text, label in dataset_raw.take(4).as_numpy_iterator():\n",
    "\tprint(text, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24075a",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197d8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100_000\n",
    "BATCH_SIZE = 32\n",
    "dataset_train = dataset_raw.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Inspired by https://stackoverflow.com/a/74609848.\n",
    "validate_size = 64\n",
    "dataset_validate = dataset_train.take(validate_size)\n",
    "dataset_train = dataset_train.skip(validate_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532db30",
   "metadata": {},
   "source": [
    "### Preparing to process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa159e6",
   "metadata": {},
   "source": [
    "The [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer takes a `standardize` option that preprocesses input data. The default removes punctuation, but we don't want that. Let's redefine it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f74b9fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"[START] [cap] thi [s] is a test ! [cap] it ' s work [ing] ? ! [END]\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def standardize_tf_text(text):\n",
    "\tpunctuation_regex = '[{p}]'.format(p = punctuation_chars)\n",
    "\n",
    "\t# Surround punctuation with spaces for easier tokenization\n",
    "\ttext = tf.strings.regex_replace(text, punctuation_regex, r' \\0 ')\n",
    "\n",
    "\t# Remove repeated spaces\n",
    "\ttext = tf.strings.regex_replace(text, r'\\s+', ' ')\n",
    "\n",
    "\t# Add a special \"capitalize the next letter\" token\n",
    "\ttext = tf.strings.regex_replace(text, r'(\\s|^)([A-Z])', r' [CAP] \\2')\n",
    "\n",
    "\t# Lowercase everything\n",
    "\ttext = tf.strings.lower(text)\n",
    "\n",
    "\t# English specific: Move -ing, -s suffixes to new words\n",
    "\ttext = tf.strings.regex_replace(text, r'([a-z]{3,})(ing|er|ed|ily|ly|ish|s)(\\s|$)', r'\\1 [\\2]\\3')\n",
    "\n",
    "\t# Remove leading and trailing spaces\n",
    "\ttext = tf.strings.strip(text)\n",
    "\n",
    "\t# Add sequence markings\n",
    "\treturn tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "\n",
    "print(standardize_tf_text('This is a test! It\\'s working?!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018456b9",
   "metadata": {},
   "source": [
    "The text standardization function can now be used to preprocess text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87815e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 14 target words: ['', '[UNK]', '[cap]', ',', '[s]', 'the', '.', '[ed]', 'and', '[START]', '[END]', 'to', 'of', '[ing]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 18:47:31.046864: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Keep only the 4000 most commonly used tokens\n",
    "max_vocab_size = 4000\n",
    "\n",
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "\tstandardize=standardize_tf_text,\n",
    "\tmax_tokens=max_vocab_size,\n",
    "\t# Allow entries of different lengths\n",
    "\tragged=True,\n",
    ")\n",
    "target_text_processor.adapt(dataset_train.map(lambda context, target: target))\n",
    "\n",
    "print('First 14 target words:', target_text_processor.get_vocabulary()[:14])\n",
    "\n",
    "# The target data should be roughly equivalent to the context data, except have additional (punctuation)\n",
    "# tokens.\n",
    "context_text_processor = target_text_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d66c3",
   "metadata": {},
   "source": [
    "We can use these layers to convert to/from token IDs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b4789f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokens tf.Tensor([  9 142 359  36   4  29  16 560   1  29   1  13  36   4  10], shape=(15,), dtype=int64)\n",
      "Back to text [START] hello world thi [s] is a test [UNK] is [UNK] [ing] thi [s] [END]\n"
     ]
    }
   ],
   "source": [
    "example_text = 'hello world this is a test tensorflow is processing this'\n",
    "example_tokens = context_text_processor(example_text)\n",
    "print('Example tokens', example_tokens)\n",
    "\n",
    "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
    "tokens = context_vocab[example_tokens.numpy()]\n",
    "print('Back to text', ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d8e67",
   "metadata": {},
   "source": [
    "### Processing the data\n",
    "\n",
    "Now, we'll:\n",
    "1. Map the data through the text processors we just made.\n",
    "2. Shift the target data, so that our network is provided with a history of generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2efef357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "\treturn context_text_processor(context), target_text_processor(target)\n",
    "\n",
    "def add_target_history(context, target):\n",
    "\t# .to_tensor(): Converts from RaggedTensors to Tensors.\n",
    "\t# We give our network the history as target_in\n",
    "\ttarget_in = target[:, :-1].to_tensor()\n",
    "\ttarget_out = target[:, 1:].to_tensor()\n",
    "\treturn (context.to_tensor(), target_in), target_out\n",
    "dataset_train = dataset_train.map(process_text).map(add_target_history).repeat()\n",
    "dataset_validate = dataset_validate.map(process_text).map(add_target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50594897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context [START],but,moth,[er],i,did,not,come,to,hear,mr,[UNK],[s],fortune,i,came,to,hear,my,own,and,you,have,told,me,noth,[ing],of,it,[END],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "target_in [START],[cap],but,,,moth,[er],,,[cap],i,did,not,come,to,hear,[cap],mr,.,[cap],rochest,[er],',s,fortune,:,[cap],i,came,to,hear,my,own,;,and,you,have,told,me,noth,[ing],of,it,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "target_out [cap],but,,,moth,[er],,,[cap],i,did,not,come,to,hear,[cap],mr,.,[cap],rochest,[er],',s,fortune,:,[cap],i,came,to,hear,my,own,;,and,you,have,told,me,noth,[ing],of,it,.,[END],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def inspect_dataset(dataset: tf.data.Dataset):\n",
    "\ttarget_vocab = np.array(target_text_processor.get_vocabulary())\n",
    "\tfor (context, target_in), target_out in dataset.take(1):\n",
    "\t\tcontext_words = context_vocab[context[0]]\n",
    "\t\tprint('context', ','.join(context_words))\n",
    "\t\tprint('target_in', ','.join(target_vocab[target_in[0]]))\n",
    "\t\tprint('target_out', ','.join(target_vocab[target_out[0]]))\n",
    "\n",
    "inspect_dataset(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63bc6c",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### The encoder\n",
    "\n",
    "See https://www.tensorflow.org/text/tutorials/nmt_with_attention#the_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83f026f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Layer):\n",
    "\tdef __init__(self, text_processor, units: int):\n",
    "\t\t\"\"\"\n",
    "\t\tCreates a new Encoder layer. [dimen] is the maxiumum number of elements of the input\n",
    "\t\tthat can be processed by the encoder.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.text_processor = text_processor\n",
    "\t\tself.vocab_size = text_processor.vocabulary_size()\n",
    "\t\tself.units = units\n",
    "\n",
    "\t\t# Converts tokens -> vectors\n",
    "\t\tself.embedding = tf.keras.layers.Embedding(\n",
    "\t\t\t# mask_zero: Treats zero as a padding value that should be ignored\n",
    "\t\t\tself.vocab_size, units, mask_zero = True,\n",
    "\t\t)\n",
    "\t\tgru = tf.keras.layers.GRU(\n",
    "\t\t\tunits, return_sequences = True,\n",
    "\t\t\t# Use the recurrent_initializer suggested by the tutorial (& the default\n",
    "\t\t\t# for kernel_initializer).\n",
    "\t\t\trecurrent_initializer='glorot_uniform'\n",
    "\t\t)\n",
    "\t\tself.rnn = tf.keras.layers.Bidirectional(\n",
    "\t\t\t# merge_mode determines how the forward and backward layers are combined\n",
    "\t\t\t#            'concat' is another option here\n",
    "\t\t\tmerge_mode = 'sum',\n",
    "\t\t\tlayer=gru,\n",
    "\t\t)\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\tx = self.embedding(x)\n",
    "\t\tx = self.rnn(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef prepare_for_input(self, texts):\n",
    "\t\t\"\"\"\n",
    "\t\tUtility method that converts `texts` to a form that can be provided to the `call` method.\n",
    "\t\t\"\"\"\n",
    "\t\ttexts = tf.convert_to_tensor(texts)\n",
    "\t\tif len(texts.shape) == 0:\n",
    "\t\t\ttexts = texts[None]\n",
    "\t\tcontext = self.text_processor(texts).to_tensor()\n",
    "\t\treturn context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea196ff",
   "metadata": {},
   "source": [
    "Try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331e474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens shape (batch, s): (32, 82)\n",
      "Encoder output shape (batch, s, ENCODER_UNITS): (32, 82, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 18:47:41.139670: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "ENCODER_UNITS = 96\n",
    "encoder = Encoder(context_text_processor, ENCODER_UNITS)\n",
    "\n",
    "for (context, target_history), target_next in dataset_validate.take(1):\n",
    "\tencoder_result = encoder(context)\n",
    "\tprint('Context tokens shape (batch, s):', context.shape)\n",
    "\tprint('Encoder output shape (batch, s, ENCODER_UNITS):', encoder_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd7fb6",
   "metadata": {},
   "source": [
    "### The attention layer\n",
    "\n",
    "Attention can be thought of as training a lookup table with keys and values. The lookup table has inputs `values` and `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "781b2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.Layer):\n",
    "\tdef __init__(self, units, **kwargs):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.attention_layer = tf.keras.layers.MultiHeadAttention(\n",
    "\t\t\tkey_dim=units,\n",
    "\t\t\tnum_heads=1,\n",
    "\t\t\t**kwargs\n",
    "\t\t)\n",
    "\t\t# Keeps \"the mean activation within each example close to 0 and the\n",
    "\t\t# activation standard deviation close to 1\" -- https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization?hl=en\n",
    "\t\tself.norm_layer = tf.keras.layers.LayerNormalization()\n",
    "\t\tself.add_layer = tf.keras.layers.Add()\n",
    "\t\tself.supports_masking = True\n",
    "\n",
    "\tdef call(self, query, value):\n",
    "\t\tattention_output = self.attention_layer(\n",
    "\t\t\tquery = query,\n",
    "\t\t\tvalue = value,\n",
    "\t\t\t#use_causal_mask=True,\n",
    "\t\t\t# Return the attention scores for latter plotting\n",
    "\t\t\t# return_attention_scores = True,\n",
    "\t\t)\n",
    "\n",
    "\t\tx = self.add_layer([ query, attention_output ])\n",
    "\t\tx = self.norm_layer(x)\n",
    "\t\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8857ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded context sequence shape (batch, s, units): (32, 81, 96)\n",
      "Target history sequence shape (batch, t, units): (32, 98, 96)\n",
      "Attention result shape (batch, t, units): (32, 98, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(ENCODER_UNITS)\n",
    "\n",
    "# Test with an example\n",
    "for (context, target_history), target_next in dataset_validate.take(1):\n",
    "\tembed_layer = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(), output_dim=ENCODER_UNITS, mask_zero=True)\n",
    "\ttarget_embed = embed_layer(target_history)\n",
    "\tencoded_context = encoder(context)\n",
    "\tattention_result = attention_layer(target_embed, encoded_context)\n",
    "\n",
    "\tprint('Encoded context sequence shape (batch, s, units):', encoded_context.shape)\n",
    "\tprint('Target history sequence shape (batch, t, units):', target_embed.shape)\n",
    "\tprint('Attention result shape (batch, t, units):', attention_result.shape)\n",
    "\n",
    "\t# Used later \n",
    "\ttest_encoded_context = encoded_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5577a1",
   "metadata": {},
   "source": [
    "### The decoder\n",
    "\n",
    "The decoder produces queries for the attention layer. The decoder operates on `target_history`. At each step during training, it should have no information about future target output (that's what we're trying to determine). As such, we use a unidirectional RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2454259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDense(tf.keras.layers.Dense):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tsuper(CustomDense, self).__init__(*args, **kwargs)\n",
    "\t\n",
    "\tdef compute_mask(self, _inputs, mask=None):\n",
    "\t\treturn mask\n",
    "\n",
    "class Decoder(tf.keras.Layer):\n",
    "\tdef __init__(self, text_processor, units):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.text_processor = text_processor\n",
    "\t\tself.vocab_size = text_processor.vocabulary_size()\n",
    "\t\tself.units = units\n",
    "\n",
    "\t\tself.embedding_layer = tf.keras.layers.Embedding(\n",
    "\t\t\t# mask_zero: Treats zero as a padding value that should be ignored\n",
    "\t\t\tself.vocab_size, units, mask_zero = True,\n",
    "\t\t)\n",
    "\t\tself.rnn_layer = tf.keras.layers.GRU(\n",
    "\t\t\tunits, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform',\n",
    "\t\t)\n",
    "\t\tself.attention_layer = CrossAttention(units)\n",
    "\n",
    "\t\t# Creates logits with the estimated probability of each output token\n",
    "\t\tself.output_layer = CustomDense(self.vocab_size)\n",
    "\n",
    "\t\t# Conversion:\n",
    "\t\tself.word_to_id = tf.keras.layers.StringLookup(\n",
    "\t\t\tvocabulary = text_processor.get_vocabulary(),\n",
    "\t\t\tmask_token = '',\n",
    "\t\t\toov_token = '[UNK]',\n",
    "\t\t)\n",
    "\t\tself.id_to_word = tf.keras.layers.StringLookup(\n",
    "\t\t\tvocabulary = text_processor.get_vocabulary(),\n",
    "\t\t\tmask_token = '',\n",
    "\t\t\toov_token = '[UNK]',\n",
    "\t\t\tinvert = True,\n",
    "\t\t)\n",
    "\t\t# Pre-computing these simplifies exporting\n",
    "\t\tself.start_id = self.word_to_id('[START]')\n",
    "\t\tself.end_id = self.word_to_id('[END]')\n",
    "\n",
    "\t\tself.supports_masking = True\n",
    "\t\n",
    "\tdef build(self, input_shape):\n",
    "\t\t# Nothing tha needs a size allocation based on the input shape\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef call(self, context, target_history, state = None, return_state = False):\n",
    "\t\tx = self.embedding_layer(target_history)\n",
    "\t\tx, state = self.rnn_layer(x, initial_state = state)\n",
    "\t\tx = self.attention_layer(x, context)\n",
    "\n",
    "\t\tlogits = self.output_layer(x)\n",
    "\t\tif return_state:\n",
    "\t\t\treturn logits, state\n",
    "\t\telse:\n",
    "\t\t\treturn logits\n",
    "\t\n",
    "\t## Conversion/testing ##\n",
    "\n",
    "\tdef tokens_to_text(self, tokens):\n",
    "\t\ttext = tf.strings.reduce_join(self.id_to_word(tokens), separator = ' ')\n",
    "\t\ttext = tf.strings.regex_replace(text, r'\\s*\\[START\\]\\s*', '')\n",
    "\t\ttext = tf.strings.regex_replace(text, r'\\s*\\[END\\].*$', '')\n",
    "\t\treturn text\n",
    "\n",
    "\tdef generate_next_token(self, context, target_history, done_vec, state, temperature = 0.0):\n",
    "\t\t# Note: is_done is a vector, indicating whether each item in the batch is done\n",
    "\n",
    "\t\tlogits, state = self(context, target_history, state = state, return_state = True)\n",
    "\n",
    "\t\t# logits has shape (batch, t, target_vocab_size). Only generate the token corresponding\n",
    "\t\t# to the last logits in the sequence (at t - 1)\n",
    "\t\tif temperature > 0:\n",
    "\t\t\tnext_token = tf.where(\n",
    "\t\t\t\tdone_vec,\n",
    "\t\t\t\ttf.constant(0, dtype=tf.int64), # Emit 0 after a sequence is done\n",
    "\t\t\t\ttf.random.categorical(logits[:, -1, :] / temperature, num_samples = 1), # Otherwise, pick the token from a categorical distribution\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tnext_token = tf.math.argmax(logits, axis=-1)\n",
    "\t\tdone_vec = done_vec|(next_token == self.end_id)\n",
    "\t\treturn next_token, done_vec, state\n",
    "\t\n",
    "\tdef get_initial_state(self, context):\n",
    "\t\t# context has shape (batch_size, s, units)\n",
    "\t\tbatch_size = tf.shape(context)[0]\n",
    "\t\tstart_tokens = tf.fill([batch_size, 1], self.start_id)\n",
    "\t\tdone_vec = tf.zeros([batch_size, 1], dtype = tf.bool)\n",
    "\n",
    "\t\t# From the Tensorflow source code:\n",
    "\t\t# > RNN expect the states in a list, even if single state.\n",
    "\t\t# Note: Without the [0] we get a type mismatch while exporting.\n",
    "\t\tinitial_state = self.rnn_layer.get_initial_state(batch_size)[0]\n",
    "\n",
    "\t\treturn start_tokens, done_vec, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d5ccb1",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "219a3522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: b'elbow basha wheel increase shaken pac why whyelbow basha wheel increase shaken pac why whyelbow basha wheel increase shaken pac why why'\n"
     ]
    }
   ],
   "source": [
    "def test_generation_loop():\n",
    "\tdecoder = Decoder(target_text_processor, ENCODER_UNITS)\n",
    "\tnext_token, done_vec, state = decoder.get_initial_state(test_encoded_context[:3, :, :])\n",
    "\ttokens = [next_token]\n",
    "\n",
    "\tfor i in range(8):\n",
    "\t\tnext_token, done_vec, state = decoder.generate_next_token(test_encoded_context[:3, :, :], next_token, done_vec, state)\n",
    "\t\ttokens.append(next_token)\n",
    "\t\n",
    "\t# Merge all batch outputs into a single dimension\n",
    "\ttokens = tf.concat(tokens, -1) # -1 = last axis\n",
    "\n",
    "\tprint('Output:', decoder.tokens_to_text(tokens).numpy())\n",
    "\n",
    "test_generation_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3411a",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b3a40",
   "metadata": {},
   "source": [
    "We can now build a model for training and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1520bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Punctuator(tf.keras.Model):\n",
    "\tdef __init__(self, units, context_text_processor, target_text_processor):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encoder = Encoder(context_text_processor, units)\n",
    "\t\tself.decoder = Decoder(target_text_processor, units)\n",
    "\t\n",
    "\tdef call(self, inputs):\n",
    "\t\tcontext, target_history = inputs\n",
    "\t\tcontext = self.encoder(context)\n",
    "\t\tlogits = self.decoder(context, target_history)\n",
    "\t\treturn logits\n",
    "\t\n",
    "\tdef fix_punctuation_raw(self, input):\n",
    "\t\t\"\"\"\n",
    "\t\tAdds punctuation to `input`, where `input` is a `Tensor` with shape (batch_size, s) where s is the\n",
    "\t\tcontext length.\n",
    "\t\t\"\"\"\n",
    "\t\tcontext = self.encoder(input)\n",
    "\n",
    "\t\tnext_token, done_vec, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "\t\t# Although a TensorArray would allow more efficient exporting, the ONNX exporter seems to\n",
    "\t\t# have trouble with it. For now, use a Python list.\n",
    "\t\ttokens = []\n",
    "\t\tmax_iterations = 56\n",
    "\n",
    "\t\tfor i in range(max_iterations):\n",
    "\t\t\t# token_history has size: (batch, t, target_vocab_size)\n",
    "\t\t\t# token_history = tf.concat(tokens, 1)\n",
    "\t\t\t# print('history', model.decoder.id_to_word(token_history))\n",
    "\t\t\tnext_token, done_vec, state = self.decoder.generate_next_token(context, next_token, done_vec, state, temperature=0)\n",
    "\t\t\t#tokens = tokens.write(i + 1, next_token)\n",
    "\t\t\ttokens.append(next_token)\n",
    "\n",
    "\t\t\tif tf.executing_eagerly() and tf.reduce_all(done_vec):\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\ttokens = tf.concat(tokens, -1)\n",
    "\t\treturn tokens\n",
    "\n",
    "\tdef fix_punctuation(self, text: list[str]):\n",
    "\t\tinputs = self.encoder.prepare_for_input(text)\n",
    "\t\ttokens = self.fix_punctuation_raw(inputs)\n",
    "\t\treturn self.decoder.tokens_to_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "253c670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens shape (batch, s): (32, 76)\n",
      "Target history tokens shape (batch, t): (32, 111)\n",
      "Logits shape (batch, t, vocab_size) (32, 111, 4000)\n"
     ]
    }
   ],
   "source": [
    "model = Punctuator(ENCODER_UNITS, context_text_processor, target_text_processor)\n",
    "\n",
    "for (example_context_tok, example_target_hist), _ in dataset_validate.take(1):\n",
    "\ttest_logits = model((example_context_tok, example_target_hist))\n",
    "\tprint('Context tokens shape (batch, s):', example_context_tok.shape)\n",
    "\tprint('Target history tokens shape (batch, t):', example_target_hist.shape)\n",
    "\tprint('Logits shape (batch, t, vocab_size)', test_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68e81b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"punctuator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"punctuator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">495,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">865,312</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │       \u001b[38;5;34m495,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │       \u001b[38;5;34m865,312\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,361,056</span> (5.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,361,056\u001b[0m (5.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,361,056</span> (5.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,361,056\u001b[0m (5.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85551c17",
   "metadata": {},
   "source": [
    "To avoid penalizing masked outputs, we use a custom loss function (see the tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ca4d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss(y_true, y_predict):\n",
    "\tloss = base_loss_fn(y_true, y_predict)\n",
    "\t\n",
    "\tunmasked = y_true != 0\n",
    "\tunmasked = tf.cast(unmasked, loss.dtype)\n",
    "\t# Only consider output with a corresponding label.\n",
    "\tloss *= unmasked\n",
    "\n",
    "\tcount_unmasked = tf.math.reduce_sum(unmasked)\n",
    "\n",
    "\t# reduce_sum: Adds all entries of a vector.\n",
    "\treturn tf.math.reduce_sum(loss)/count_unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ba83b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, predict_logits):\n",
    "\tpredicted_index = tf.math.argmax(predict_logits, axis=-1)\n",
    "\tpredicted_index = tf.cast(predicted_index, y_true.dtype)\n",
    "\n",
    "\tmatch = tf.cast(y_true == predicted_index, tf.float32)\n",
    "\tunmasked = tf.cast(y_true != 0, tf.float32)\n",
    "\tcount_unmasked = tf.math.reduce_sum(unmasked)\n",
    "\n",
    "\treturn tf.math.reduce_sum(match * unmasked) / count_unmasked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d9fde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[masked_accuracy, masked_loss])\n",
    "model.compile(optimizer='adam', loss=masked_loss, metrics=[masked_accuracy, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95ae7f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the tutorial:\n",
      "expected loss 8.294049\n",
      "expected accuracy 0.00025\n"
     ]
    }
   ],
   "source": [
    "print('From the tutorial:')\n",
    "vocab_size = float(target_text_processor.vocabulary_size())\n",
    "\n",
    "print('expected loss', tf.math.log(vocab_size).numpy())\n",
    "print('expected accuracy', 1/vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "091b83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 114ms/step - loss: 8.2979 - masked_accuracy: 5.9045e-04 - masked_loss: 8.2979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 8.29760456085205,\n",
       " 'masked_accuracy': 0.0004955152398906648,\n",
       " 'masked_loss': 8.29760456085205}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset_validate, steps=20, return_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a179e6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[test]: [cap] thi [s] is an example , they said .'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def test_punctuation(text):\n",
    "\treturn '[test]: ' + model.fix_punctuation(text).numpy().decode('utf-8')\n",
    "\n",
    "class DemoCallback(tf.keras.callbacks.Callback):\n",
    "\tdef on_epoch_end(self, epoch_index: int, logs = None):\n",
    "\t\tprint('\\r', test_punctuation([ 'i m testing this models performance how well is it working' ]))\n",
    "\t\tif epoch_index % 3 == 0:\n",
    "\t\t\t# From the test data\n",
    "\t\t\tprint(test_punctuation([\n",
    "\t\t\t\t'not that alice had any idea of doing that she felt as if she would never be able to talk again she was getting so much out of breath and still the queen cried faster faster and dragged her along'\n",
    "\t\t\t]))\n",
    "\t\t\tprint(test_punctuation([ 'tensorflow is a library that is used for machine learning it is available for more languages than just python' ]))\n",
    "\t\t\tprint(test_punctuation([ 'the joplin note taking app can be used to take multimedia notes' ]))\n",
    "\t\t\tprint(test_punctuation([ 'here are a few words javascript typescript python joplin interesting loud and sequence these words are all very useful' ]))\n",
    "\n",
    "test_punctuation(tf.constant([ 'this is an example they said' ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2db06e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " [test]: [cap] i m ' test [ing] thi [s] model ' s [UNK] . [cap] how well is it work [ing] ?63 - masked_accuracy: 0.9395 - masked_loss: 0.2163\n",
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would nev [er] be able to talk again . [cap] she was gett [ing] so much out of breath , and still the [cap] queen cri [ed] fast [er] fast [er] and dragg [ed] her along .\n",
      "[test]: [cap] [UNK] is a library , that is used for machine learn [ing] . [cap] it is [UNK] for more language [s] than just [UNK] .\n",
      "[test]: [cap] the [cap] joplin note tak [ing] app can be used to take [UNK] note [s] .\n",
      "[test]: [cap] here are a few word [s] , [cap] [UNK] [cap] [UNK] [UNK] [cap] [UNK] , [cap] i , interest [ing] loud and [UNK] . [cap] these word [s] are all very useful .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 229ms/step - loss: 0.2163 - masked_accuracy: 0.9395 - masked_loss: 0.2163 - val_loss: 0.2075 - val_masked_accuracy: 0.9274 - val_masked_loss: 0.2043\n",
      "Epoch 2/20\n",
      " [test]: [cap] i m ' test [ing] thi [s] model ' s [UNK] . [cap] how well is it work [ing] ?48 - masked_accuracy: 0.9380 - masked_loss: 0.2248\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 236ms/step - loss: 0.2248 - masked_accuracy: 0.9380 - masked_loss: 0.2248 - val_loss: 0.2009 - val_masked_accuracy: 0.9281 - val_masked_loss: 0.1978\n",
      "Epoch 3/20\n",
      " [test]: [cap] i m test [ing] thi [s] model ' s [UNK] . [cap] how well is it work [ing] ?2074 - masked_accuracy: 0.9421 - masked_loss: 0.2074\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 228ms/step - loss: 0.2074 - masked_accuracy: 0.9421 - masked_loss: 0.2074 - val_loss: 0.2051 - val_masked_accuracy: 0.9280 - val_masked_loss: 0.2019\n",
      "Epoch 4/20\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - loss: 0.2216 - masked_accuracy: 0.9386 - masked_loss: 0.2216"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 23:09:16.045637: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [test]: [cap] i m test [ing] thi [s] model ' s [UNK] how well is it work [ing] .\n",
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would nev [er] be able to talk again : she was gett [ing] so much out of breath , and still , the [cap] queen cri [ed] , [cap] fast [er] , fast [er] and dragg [ed] her along .\n",
      "[test]: [cap] [UNK] is a library , that is used for machine ; learn [ing] it is [UNK] for more language [s] than just [UNK] .\n",
      "[test]: [cap] the [cap] joplin note , tak [ing] app can be used to take five - and note [s] - - a note [s] - -\n",
      "[test]: [cap] here are a few word [s] , [UNK] [UNK] [UNK] , [cap] rome , interest [ing] loud and [UNK] . [cap] these word [s] are all very useful .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 243ms/step - loss: 0.2216 - masked_accuracy: 0.9386 - masked_loss: 0.2216 - val_loss: 0.1959 - val_masked_accuracy: 0.9297 - val_masked_loss: 0.1929\n",
      "Epoch 5/20\n",
      " [test]: [cap] i m test [ing] thi [s] model ' s [UNK] how well is it work [ing] .loss: 0.2036 - masked_accuracy: 0.9431 - masked_loss: 0.2036\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 227ms/step - loss: 0.2036 - masked_accuracy: 0.9431 - masked_loss: 0.2036 - val_loss: 0.1922 - val_masked_accuracy: 0.9305 - val_masked_loss: 0.1893\n",
      "Epoch 6/20\n",
      " [test]: [cap] i m test [ing] thi [s] model ' s [UNK] how well is it work [ing] .loss: 0.2165 - masked_accuracy: 0.9396 - masked_loss: 0.2165\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 226ms/step - loss: 0.2165 - masked_accuracy: 0.9396 - masked_loss: 0.2165 - val_loss: 0.1977 - val_masked_accuracy: 0.9291 - val_masked_loss: 0.1946\n",
      "Epoch 7/20\n",
      " [test]: [cap] i m ' test [ing] thi [s] model ' s [UNK] how well is it work [ing] .ss: 0.1973 - masked_accuracy: 0.9440 - masked_loss: 0.1973\n",
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would nev [er] be able to talk again : she was gett [ing] so much out of breath , and still , the [cap] queen cri [ed] , [cap] fast [er] , fast [er] , and dragg [ed] her along\n",
      "[test]: [cap] [UNK] is a library , that is used for machine learn [ing] ; it is [UNK] for more language [s] than just [UNK] .\n",
      "[test]: [cap] the [cap] joplin note tak [ing] app can be used to take [UNK] note [s] .\n",
      "[test]: [cap] here are a few word [s] , [UNK] [UNK] [UNK] [UNK] , [cap] rome , interest [ing] loud and [UNK] these word [s] , are all very useful .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 227ms/step - loss: 0.1973 - masked_accuracy: 0.9440 - masked_loss: 0.1973 - val_loss: 0.1872 - val_masked_accuracy: 0.9322 - val_masked_loss: 0.1843\n",
      "Epoch 8/20\n",
      " [test]: [cap] i m ' test [ing] thi [s] model ' s [UNK] . [cap] how well is it work [ing] ?79 - masked_accuracy: 0.9416 - masked_loss: 0.2079\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 226ms/step - loss: 0.2079 - masked_accuracy: 0.9416 - masked_loss: 0.2079 - val_loss: 0.1896 - val_masked_accuracy: 0.9317 - val_masked_loss: 0.1866\n",
      "Epoch 9/20\n",
      " [test]: [cap] i m ' test [ing] thi [s] model ' s [UNK] . [cap] how well is it work [ing] ?32 - masked_accuracy: 0.9452 - masked_loss: 0.1932\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 203ms/step - loss: 0.1932 - masked_accuracy: 0.9452 - masked_loss: 0.1932 - val_loss: 0.1926 - val_masked_accuracy: 0.9305 - val_masked_loss: 0.1896\n",
      "Epoch 10/20\n",
      " [test]: [cap] i m ' test [ing] thi [s] model ' s [UNK] how well is it work [ing] .ss: 0.2076 - masked_accuracy: 0.9414 - masked_loss: 0.2076\n",
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would nev [er] be able to talk again : she was gett [ing] so much out of breath , and still the [cap] queen cri [ed] , [cap] fast [er] ! [cap] fast [er] and dragg [ed] her along !\n",
      "[test]: [cap] [UNK] is a library , that is used for machine , learn [ing] it is [UNK] for more language [s] than just [UNK] .\n",
      "[test]: [cap] the [cap] joplin note , tak [ing] app can be used to take [UNK] note [s] .\n",
      "[test]: [cap] here are a few word [s] , [UNK] [UNK] [UNK] [UNK] , [cap] pilot [ed] loud and [UNK] . [cap] these word [s] are all very useful .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 196ms/step - loss: 0.2076 - masked_accuracy: 0.9414 - masked_loss: 0.2076 - val_loss: 0.1783 - val_masked_accuracy: 0.9340 - val_masked_loss: 0.1756\n",
      "Epoch 11/20\n",
      " [test]: [cap] i m test thi [s] model ' s [UNK] how well is it work [ing] .tep - loss: 0.1846 - masked_accuracy: 0.9472 - masked_loss: 0.1846\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 215ms/step - loss: 0.1846 - masked_accuracy: 0.9472 - masked_loss: 0.1846 - val_loss: 0.1802 - val_masked_accuracy: 0.9340 - val_masked_loss: 0.1774\n",
      "Epoch 12/20\n",
      " [test]: [cap] i m test [ing] thi [s] model ' s [UNK] how well is it work [ing] .loss: 0.1981 - masked_accuracy: 0.9434 - masked_loss: 0.1981\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 232ms/step - loss: 0.1981 - masked_accuracy: 0.9434 - masked_loss: 0.1981 - val_loss: 0.1766 - val_masked_accuracy: 0.9348 - val_masked_loss: 0.1739\n",
      "Epoch 13/20\n",
      " [test]: [cap] i m ' test [ing] thi [s] model ' s [UNK] . [cap] how well is it work [ing] ?13 - masked_accuracy: 0.9479 - masked_loss: 0.1813\n",
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would nev [er] be able to talk again : she was gett [ing] so much out of breath , and still the [cap] queen cri [ed] fast [er] , fast [er] , and dragg [ed] her along .\n",
      "[test]: [cap] [UNK] is a library that is used for machine learn [ing] it is [UNK] for more language [s] than just [UNK] .\n",
      "[test]: [cap] the [cap] joplin note , tak [ing] app can be used to take [UNK] note [s] .\n",
      "[test]: [cap] here are a few word [s] [UNK] [UNK] [UNK] , [cap] joplin interest [ing] loud and [UNK] . [cap] these word [s] are all very useful .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 218ms/step - loss: 0.1813 - masked_accuracy: 0.9479 - masked_loss: 0.1813 - val_loss: 0.1736 - val_masked_accuracy: 0.9354 - val_masked_loss: 0.1709\n",
      "Epoch 14/20\n",
      " [test]: [cap] i m test [ing] thi [s] model ' s [UNK] . [cap] how well is it work [ing] ?1949 - masked_accuracy: 0.9442 - masked_loss: 0.1949\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 224ms/step - loss: 0.1949 - masked_accuracy: 0.9442 - masked_loss: 0.1949 - val_loss: 0.1744 - val_masked_accuracy: 0.9353 - val_masked_loss: 0.1717\n",
      "Epoch 15/20\n",
      " [test]: [cap] i m test [ing] thi [s] model ' s [UNK] how well is it work [ing] .loss: 0.1770 - masked_accuracy: 0.9491 - masked_loss: 0.1770\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 224ms/step - loss: 0.1770 - masked_accuracy: 0.9491 - masked_loss: 0.1770 - val_loss: 0.1691 - val_masked_accuracy: 0.9365 - val_masked_loss: 0.1665\n",
      "Epoch 16/20\n",
      " [test]: [cap] i m ' test [ing] thi [s] model ' s [UNK] . [cap] how well is it work [ing] ?94 - masked_accuracy: 0.9457 - masked_loss: 0.1894\n",
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would nev [er] be able to talk again . [cap] she was gett [ing] so much out of breath , and still ! [cap] the [cap] queen cri [ed] , [cap] fast [er] ! [cap] fast [er] , and dragg\n",
      "[test]: [cap] [UNK] is a library that is used for machine learn [ing] it is [UNK] for more language [s] than just [UNK] .\n",
      "[test]: [cap] the [cap] joplin note , tak [ing] app can be used to take [cap] [UNK] note [s] .\n",
      "[test]: [cap] here are a few word [s] , [UNK] [UNK] [UNK] [cap] [UNK] [cap] pilot [ing] loud and [UNK] . [cap] these word [s] are all very useful .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 221ms/step - loss: 0.1894 - masked_accuracy: 0.9457 - masked_loss: 0.1894 - val_loss: 0.1733 - val_masked_accuracy: 0.9349 - val_masked_loss: 0.1706\n",
      "Epoch 17/20\n",
      " [test]: [cap] i m ' test [ing] thi [s] model ' s [UNK] how well is it work [ing] .ss: 0.1709 - masked_accuracy: 0.9506 - masked_loss: 0.1709\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 192ms/step - loss: 0.1709 - masked_accuracy: 0.9506 - masked_loss: 0.1709 - val_loss: 0.1724 - val_masked_accuracy: 0.9352 - val_masked_loss: 0.1697\n",
      "Epoch 18/20\n",
      " [test]: [cap] i m test [ing] thi [s] model ' s [UNK] . [cap] how well is it work [ing] ?1849 - masked_accuracy: 0.9466 - masked_loss: 0.1849\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 193ms/step - loss: 0.1849 - masked_accuracy: 0.9466 - masked_loss: 0.1849 - val_loss: 0.1737 - val_masked_accuracy: 0.9362 - val_masked_loss: 0.1711\n",
      "Epoch 19/20\n",
      " [test]: [cap] i m ' test [ing] thi [s] model ' s [UNK] how well is it work [ing] .ss: 0.1720 - masked_accuracy: 0.9504 - masked_loss: 0.1720\n",
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would nev [er] be able to talk again : she was gett [ing] so much out of breath , and still the [cap] queen cri [ed] fast [er] , fast [er] , and dragg [ed] her along .\n",
      "[test]: [cap] [UNK] is a library that is used for machine learn [ing] . [cap] it is [UNK] for more language [s] than just [UNK] .\n",
      "[test]: [cap] the [cap] joplin note tak [ing] app can be used to take [UNK] note [s] .\n",
      "[test]: [cap] here are a few word [s] [UNK] [UNK] [UNK] [cap] pilot [s] loud and [UNK] . [cap] these word [s] are all very useful .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 194ms/step - loss: 0.1720 - masked_accuracy: 0.9504 - masked_loss: 0.1720 - val_loss: 0.1625 - val_masked_accuracy: 0.9386 - val_masked_loss: 0.1600\n",
      "Epoch 20/20\n",
      " [test]: [cap] i m ' test [ing] thi [s] model ' s [UNK] . [cap] how well is it work [ing] ?68 - masked_accuracy: 0.9487 - masked_loss: 0.1768\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 192ms/step - loss: 0.1768 - masked_accuracy: 0.9487 - masked_loss: 0.1768 - val_loss: 0.1636 - val_masked_accuracy: 0.9381 - val_masked_loss: 0.1611\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "\tdataset_train,\n",
    "\tepochs = 20,\n",
    "\tsteps_per_epoch = 1200,\n",
    "\tvalidation_data = dataset_validate,\n",
    "\tcallbacks=[DemoCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f68a9fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would nev [er] be able to talk again : she was gett [ing] so much out of breath , and still the [cap] queen cri [ed] [cap] fast [er] , fast [er] , and dragg [ed] her along .\n",
      "[test]: [cap] thi [s] is a test of a the punctuation . [cap] brig , for [cap] i am curiou [s] , how well it work [s] !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_punctuation([\n",
    "\t'not that alice had any idea of doing that she felt as if she would never be able to talk again she was getting so much out of breath and still the queen cried faster faster and dragged her along'\n",
    "]))\n",
    "print(test_punctuation([ 'this is a test of a the punctuation system for i am curious how well it works' ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e9cc9",
   "metadata": {},
   "source": [
    "## Exporting\n",
    "\n",
    "Based on the [Export](https://www.tensorflow.org/text/tutorials/nmt_with_attention#export) section of the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9885c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Export(tf.Module):\n",
    "\tdef __init__(self, model):\n",
    "\t\tself.model = model\n",
    "\t\n",
    "\t@tf.function(input_signature=[tf.RaggedTensorSpec(dtype=tf.int64, shape=[None])])\n",
    "\tdef fix_punctuation(self, input):\n",
    "\t\t# Returns encoded tokens\n",
    "\t\treturn model.fix_punctuation_raw(\n",
    "\t\t\ttf.reshape(input, [1, -1])\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcac14",
   "metadata": {},
   "source": [
    "Run `fix_punctuation` once to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa037d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = Export(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8af27599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'[cap] thi [s] sentence shall be punctuat [ed] for the follow [ing] reason [s] . [cap] first [UNK] make [s] thing [s] easi [er] to read second [UNK] .'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_inputs = context_text_processor('this sentence shall be punctuated for the following reasons first punctatuion makes things easier to read second um')\n",
    "model.decoder.tokens_to_text(export.fix_punctuation(sample_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70247292",
   "metadata": {},
   "source": [
    "Now we save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "badb8d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: punctuator-seq2seq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: punctuator-seq2seq/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(export, 'punctuator-seq2seq', signatures={ 'serving_default': export.fix_punctuation })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197976b4",
   "metadata": {},
   "source": [
    "See [the documentation](https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export) for information about the `signatures` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56d91fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43233"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "web_output_dir = Path('web')\n",
    "vocab_output_file = web_output_dir / 'wordEncodings.ts'\n",
    "\n",
    "vocab_output_file.write_text('''\n",
    "// Auto-generated file!\n",
    "// Created by v2-seq2seq.ipynb\n",
    "export default {};\n",
    "'''.format(json.dumps(target_text_processor.get_vocabulary(), indent = '\\t')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d873f2",
   "metadata": {},
   "source": [
    "### Testing the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c39fbc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported and warmed up!\n"
     ]
    }
   ],
   "source": [
    "reloaded = tf.saved_model.load('punctuator-seq2seq')\n",
    "# Warmup\n",
    "reloaded.fix_punctuation(sample_inputs)\n",
    "print('Imported and warmed up!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca974c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 156 ms, sys: 73 ms, total: 229 ms\n",
      "Wall time: 85.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'[cap] thi [s] sentence shall be punctuat [ed] for the follow [ing] reason [s] . [cap] first [UNK] make [s] thing [s] easi [er] to read second [UNK] .'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.decoder.tokens_to_text(reloaded.fix_punctuation(sample_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2771e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
