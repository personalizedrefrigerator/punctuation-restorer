{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55e2298",
   "metadata": {},
   "source": [
    "# V2: SEQ2SEQ\n",
    "\n",
    "This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention#create_a_tfdata_dataset).\n",
    "\n",
    "Other resources that may be helpful in the future: https://arxiv.org/pdf/2111.10746, https://aclanthology.org/2021.findings-emnlp.393.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71899cc9-86b4-4eda-b525-d818661c70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 18:48:56.663087: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-10 18:48:56.665910: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-10 18:48:56.675072: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-10 18:48:56.689630: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-10 18:48:56.693881: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-10 18:48:56.705545: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-10 18:48:57.568068: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa459db2-5a48-4bac-8cbd-38496888faee",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention) (and [at least one other](https://www.tensorflow.org/text/tutorials/text_generation) of the Tensorflow tutorials).\n",
    "- This [blog post](https://janakiev.com/blog/jupyter-virtual-envs/) was referenced to set up the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b150ba-48c0-4f33-8d08-b9f363d6fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2722afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'It and the other noble gases - helium, neon, krypton, xenon, and radon - will react with other substances only under extreme conditions. The noble gases The noble, or inert, gases are helium, neon, argon, krypton, xenon and radon. The rare gases are helium, neon, argon, krypton or xenon. The noble \"gases\" are helium, neon, argon, krypton and xenon. The noble gases are helium, neon, argon, krypton, xenon and radon. These occur for the noble gases helium, neon, argon, krypton, xenon, radon and ununoctium. Because of their chemical inertness, the elements in this group are called the Nobel Gases : Helium Neon Argon Krypton Xenon Radon Group I Elements The elements in this group have one electron in their outer electronic shell. The inert, or noble, gases (helium, neon, argon, krypton, xenon, and radon) all have completely filled outer shells. The other noble gases, which together make about 1% of the Earth\\'s atmosphere, are neon, argon, krypton, xenon and radon. Most of the noble gases-helium, neon, argon, krypton, xenon, and radon-are rare.', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 20:48:02.785603: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "dataset_raw_train = tfds.load('ai2_arc_with_ir', split='train', shuffle_files=True)\n",
    "dataset_raw_test = tfds.load('ai2_arc_with_ir', split='test', shuffle_files=True)\n",
    "dataset_raw = dataset_raw_train.concatenate(dataset_raw_test)\n",
    "dataset_raw = dataset_raw.map(lambda x: x['paragraph'])\n",
    "for i in dataset_raw.take(1):\n",
    "\tprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "100bc5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_paths = list(Path('data/processed/en/').glob('*.txt'))\n",
    "dataset_raw = dataset_raw.concatenate(\n",
    "\ttf.data.TextLineDataset(data_file_paths)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a92fd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw_mlqa = tfds.load('mlqa/en', split='test', shuffle_files=True) # \"train\" not available\n",
    "dataset_raw_mlqa = dataset_raw_mlqa.map(lambda x: x['context'])\n",
    "dataset_raw = dataset_raw_mlqa.concatenate(dataset_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5faca0eb-c672-4d1c-884f-b5ba38a22b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_unsupported_characters(s):\n",
    "\treturn tf.strings.regex_replace(s, r'[^A-Za-z0-9ùúûüÿàâæçéèêëïîôœÙÚÛÜŸÀÂÆÇÉÈÊËÏÎÔŒ \\t\\n.,?!\\-\\':;&]', '')\n",
    "dataset_raw = dataset_raw.map(remove_unsupported_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9956d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra_sentences(s):\n",
    "\tresult = [s]\n",
    "\tfor sentence in [\n",
    "\t\t'It was interesting. It was all very interesting.',\n",
    "\t\t'I added this.',\n",
    "\t\t'Why would it not?',\n",
    "\t\t'There it was.',\n",
    "\t\t'It happened.',\n",
    "\t\t'However, this part was added.',\n",
    "\t\t'That was a sentence.',\n",
    "\t\t'Here is a word: Test.',\n",
    "\t\t'This might help.',\n",
    "\t\t'How well do these extra words help?',\n",
    "\t]:\n",
    "\t\tresult.append(tf.strings.join([s, sentence]))\n",
    "\tfor sentence in [\n",
    "\t\t'I am hopeful that this prefix sentence will help. ',\n",
    "\t\t'This is a sample. ',\n",
    "\t\t'Here is some text: ',\n",
    "\t\t'Do short added sentences help? ',\n",
    "\t]:\n",
    "\t\tresult.append(tf.strings.join([sentence, s]))\n",
    "\treturn tf.data.Dataset.from_tensor_slices(result)\n",
    "\n",
    "# Append extra sentences to help the model learn to detect sentence breaks\n",
    "# dataset_raw = dataset_raw.flat_map(add_extra_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890fa88",
   "metadata": {},
   "source": [
    "We've now loaded the `.txt` training data files using [`tf.data.TextLineDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset). Each line in the source files is mapped to a new training example. \n",
    "\n",
    "Although some preprocessing has been done by `/data/process_data.py`, paragraphs aren't filtered out based on length/content. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff5c607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item tf.Tensor(b'after completing the journey around south america on 23 february 2006 queen mary 2 met her namesake the original rms queen mary which is permanently docked at long beach california', shape=(), dtype=string) tf.Tensor(b'After completing the journey around South America, on 23 February 2006, Queen Mary 2 met her namesake, the original RMS Queen Mary, which is permanently docked at Long Beach, California. ', shape=(), dtype=string)\n",
      "item tf.Tensor(b'escorted by a flotilla of smaller ships the two queens exchanged a whistle salute which was heard throughout the city of long beach', shape=(), dtype=string) tf.Tensor(b' Escorted by a flotilla of smaller ships, the two Queens exchanged a whistle salute which was heard throughout the city of Long Beach. ', shape=(), dtype=string)\n",
      "item tf.Tensor(b'queen mary 2 met the other serving cunard liners queen victoria and queen elizabeth 2 on 13 january 2008 near the statue of liberty in new york city harbour with a celebratory fireworks display queen elizabeth 2 and queen victoria made a tandem crossing of the atlantic for the meeting', shape=(), dtype=string) tf.Tensor(b' Queen Mary 2 met the other serving Cunard liners Queen Victoria and Queen Elizabeth 2 on 13 January 2008 near the Statue of Liberty in New York City harbour, with a celebratory fireworks display; Queen Elizabeth 2 and Queen Victoria made a tandem crossing of the Atlantic for the meeting. ', shape=(), dtype=string)\n",
      "item tf.Tensor(b'this marked the first time three cunard queens have been present in the same location', shape=(), dtype=string) tf.Tensor(b' This marked the first time three Cunard Queens have been present in the same location. ', shape=(), dtype=string)\n",
      "item tf.Tensor(b'cunard stated this would be the last time these three ships would ever meet due to queen elizabeth 2s impending retirement from service in late 2008', shape=(), dtype=string) tf.Tensor(b\" Cunard stated this would be the last time these three ships would ever meet, due to Queen Elizabeth 2's impending retirement from service in late 2008. \", shape=(), dtype=string)\n",
      "item tf.Tensor(b'however this would prove not to be the case as the three queens met in southampton on 22 april 2008', shape=(), dtype=string) tf.Tensor(b' However this would prove not to be the case, as the three Queens met in Southampton on 22 April 2008. ', shape=(), dtype=string)\n",
      "item tf.Tensor(b'queen mary 2 rendezvoused with queen elizabeth 2 in dubai on saturday 21 march 2009 after the latter ships retirement while both ships were berthed at port rashid', shape=(), dtype=string) tf.Tensor(b\" Queen Mary 2 rendezvoused with Queen Elizabeth 2  in Dubai on Saturday 21 March 2009, after the latter ship's retirement, while both ships were berthed at Port Rashid. \", shape=(), dtype=string)\n",
      "item tf.Tensor(b'with the withdrawal of queen elizabeth 2 from cunards fleet and its docking in dubai queen mary 2 became the only ocean liner left in active passenger service', shape=(), dtype=string) tf.Tensor(b\" With the withdrawal of Queen Elizabeth 2 from Cunard's fleet and its docking in Dubai, Queen Mary 2 became the only ocean liner left in active passenger service. \", shape=(), dtype=string)\n",
      "item tf.Tensor(b'according to the constitution the state language of ukraine is ukrainian', shape=(), dtype=string) tf.Tensor(b'According to the constitution, the state language of Ukraine is Ukrainian. ', shape=(), dtype=string)\n",
      "item tf.Tensor(b'russian is widely spoken especially in eastern and southern ukraine according to the 2001 census 67', shape=(), dtype=string) tf.Tensor(b' Russian is widely spoken, especially in eastern and southern Ukraine. According to the 2001 census, 67. ', shape=(), dtype=string)\n",
      "item tf.Tensor(b'5 percent of the population declared ukrainian as their native language and 29 6 percent declared russian', shape=(), dtype=string) tf.Tensor(b'5 percent of the population declared Ukrainian as their native language and 29.6 percent declared Russian. ', shape=(), dtype=string)\n",
      "item tf.Tensor(b'most native ukrainian speakers know russian as a second language', shape=(), dtype=string) tf.Tensor(b' Most native Ukrainian speakers know Russian as a second language. ', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 20:48:07.692624: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "def remove_incorrect_sentences(paragraph):\n",
    "\t# Some parts of the training data include what seems to be image captions and other content that\n",
    "\t# starts with a lowercase after a sentence break.\n",
    "\treturn tf.strings.regex_replace(paragraph, r'([.?!]) [a-z][^A-Z]+', r'\\1')\n",
    "\n",
    "def break_long_sequences(paragraph):\n",
    "\treturn tf.strings.regex_replace(paragraph, r'.{0,120}[.?]', r'\\0 [SEP]')\n",
    "\n",
    "def split_on_separators(text):\n",
    "\treturn tf.data.Dataset.from_tensor_slices(tf.strings.split(text, '[SEP]'))\n",
    "\n",
    "def filter_paragraphs(context, target):\n",
    "\treturn tf.strings.length(context) > 5 and tf.strings.length(target) > 5\n",
    "\n",
    "punctuation_chars = r'\\?!.,\"\\-\\':;'\n",
    "def add_context(target):\n",
    "\tcontext = tf.strings.regex_replace(target, r'[\\-]{2,}', ' - ')\n",
    "\tcontext = tf.strings.regex_replace(context, r'[\\-\\']', '')\n",
    "\tcontext = tf.strings.regex_replace(context, '[{}]+'.format(punctuation_chars), ' ')\n",
    "\tcontext = tf.strings.strip(\n",
    "\t\ttf.strings.regex_replace(context, '[ ]+', ' ')\n",
    "\t)\n",
    "\tcontext = tf.strings.lower(context)\n",
    "\treturn context, target\n",
    "\n",
    "dataset_raw = dataset_raw.map(remove_incorrect_sentences).map(break_long_sequences).flat_map(split_on_separators).map(add_context).filter(filter_paragraphs)\n",
    "for text, label in dataset_raw.take(12):\n",
    "\tprint('item', text, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b9d94",
   "metadata": {},
   "source": [
    "Now let's inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3646c389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item b'after completing the journey around south america on 23 february 2006 queen mary 2 met her namesake the original rms queen mary which is permanently docked at long beach california' b'After completing the journey around South America, on 23 February 2006, Queen Mary 2 met her namesake, the original RMS Queen Mary, which is permanently docked at Long Beach, California. '\n",
      "item b'escorted by a flotilla of smaller ships the two queens exchanged a whistle salute which was heard throughout the city of long beach' b' Escorted by a flotilla of smaller ships, the two Queens exchanged a whistle salute which was heard throughout the city of Long Beach. '\n",
      "item b'queen mary 2 met the other serving cunard liners queen victoria and queen elizabeth 2 on 13 january 2008 near the statue of liberty in new york city harbour with a celebratory fireworks display queen elizabeth 2 and queen victoria made a tandem crossing of the atlantic for the meeting' b' Queen Mary 2 met the other serving Cunard liners Queen Victoria and Queen Elizabeth 2 on 13 January 2008 near the Statue of Liberty in New York City harbour, with a celebratory fireworks display; Queen Elizabeth 2 and Queen Victoria made a tandem crossing of the Atlantic for the meeting. '\n",
      "item b'this marked the first time three cunard queens have been present in the same location' b' This marked the first time three Cunard Queens have been present in the same location. '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 20:48:09.546335: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for text, label in dataset_raw.take(4).as_numpy_iterator():\n",
    "\tprint('item', text, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24075a",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "197d8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100_000\n",
    "BATCH_SIZE = 16\n",
    "dataset_train = dataset_raw.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Inspired by https://stackoverflow.com/a/74609848.\n",
    "validate_size = 2\n",
    "dataset_validate = dataset_train.take(validate_size)\n",
    "dataset_train = dataset_train.skip(validate_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532db30",
   "metadata": {},
   "source": [
    "### Preparing to process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa159e6",
   "metadata": {},
   "source": [
    "The [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer takes a `standardize` option that preprocesses input data. The default removes punctuation, but we don't want that. Let's redefine it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f74b9fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"[START] [cap] this is a test ! [cap] it ' s working ? ! [END]\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def standardize_tf_text(text):\n",
    "\tpunctuation_regex = '[{p}]'.format(p = punctuation_chars)\n",
    "\n",
    "\t# Clean up &quot artifacts\n",
    "\ttext = tf.strings.regex_replace(text, r'&quot;?', r' \" ')\n",
    "\n",
    "\t# Surround punctuation with spaces for easier tokenization\n",
    "\ttext = tf.strings.regex_replace(text, punctuation_regex, r' \\0 ')\n",
    "\n",
    "\t# Remove repeated spaces\n",
    "\ttext = tf.strings.regex_replace(text, r'\\s+', ' ')\n",
    "\n",
    "\t# Add a special \"capitalize the next letter\" token\n",
    "\ttext = tf.strings.regex_replace(text, r'(\\s|^)([A-Z])', r' [CAP] \\2')\n",
    "\n",
    "\t# Lowercase everything\n",
    "\ttext = tf.strings.lower(text)\n",
    "\n",
    "\t# Remove leading and trailing spaces\n",
    "\ttext = tf.strings.strip(text)\n",
    "\n",
    "\t# Add sequence markings\n",
    "\treturn tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "\n",
    "print(standardize_tf_text('This is a test! It\\'s working?!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59c9ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the 4000 most commonly used tokens\n",
    "max_vocab_size = 4000\n",
    "\n",
    "bert_tokenizer_params = dict(\n",
    "\tvocab_size = max_vocab_size,\n",
    "\treserved_tokens = [ '', '[UNK]', '[MASK]', '[START]', '[END]', '[cap]' ],\n",
    "\tbert_tokenizer_params = {},\n",
    "\tlearn_params = {},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c14c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_target = dataset_train.map(lambda context, target: standardize_tf_text(target))\n",
    "\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "\n",
    "# This can be slow -- avoid recomputing:\n",
    "vocab_file = Path('./data/en_vocab.txt')\n",
    "if not vocab_file.exists():\n",
    "\tvocab = bert_vocab.bert_vocab_from_dataset(\n",
    "\t\tstandardized_target.batch(1000),\n",
    "\t\t**bert_tokenizer_params,\n",
    "\t)\n",
    "\n",
    "\twith open(vocab_file, 'w') as f:\n",
    "\t\tfor token in vocab:\n",
    "\t\t\tif token:\n",
    "\t\t\t\tf.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3cf1b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokens <tf.RaggedTensor [[b'[START]', b'he', b'##ll', b'##o', b'world', b'this', b'is', b'a',\n",
      "  b'test', b'ten', b'##s', b'##or', b'##f', b'##lo', b'##w', b'is',\n",
      "  b'process', b'##ing', b'this', b'[END]']                           ,\n",
      " [b'[START]', b'test', b'[END]']]>\n",
      "Back to text [b'[START] hello world this is a test tensorflow is processing this [END]'\n",
      " b'[START] test [END]']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_text as tf_text\n",
    "\n",
    "en_tokenizer = tf_text.BertTokenizer('data/en_vocab.txt', token_out_type=tf.string)\n",
    "\n",
    "def bert_tokenize(text):\n",
    "\t# The Bert tokenizer outputs a RaggedTensor, where the last dimension groups parts of each word.\n",
    "\t# We want all tokens to be on the same dimension -- merge the last two.\n",
    "\ttokens = en_tokenizer.tokenize(standardize_tf_text(text)).merge_dims(-2, -1)\n",
    "\n",
    "\t# Work around a bug where special tokens like [START] are split into '[', 'START', ']'\n",
    "\tresult = tf.strings.reduce_join(tokens, separator = ' ', axis=-1)\n",
    "\tresult = tf.strings.regex_replace(result, r'\\[ ([A-Za-z]+) \\]', r'[\\1]')\n",
    "\tresult = tf.strings.regex_replace(result, r'\\s+', r' ')\n",
    "\treturn tf.strings.split(result, ' ')\n",
    "\n",
    "\n",
    "example_text = ['hello world this is a test tensorflow is processing this', 'test']\n",
    "example_tokens = bert_tokenize(example_text)\n",
    "print('Example tokens', example_tokens)\n",
    "\n",
    "\n",
    "def bert_detokenize(tokens):\n",
    "\t#return tf.strings.reduce_join(en_tokenizer.detokenize(tokens), separator = ' ')\n",
    "\treturn tf.strings.regex_replace(tf.strings.reduce_join(tokens, separator = ' ', axis=-1), r' ##', '')\n",
    "\n",
    "print('Back to text', bert_detokenize(example_tokens).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018456b9",
   "metadata": {},
   "source": [
    "The text standardization function can now be used to preprocess text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87815e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 14 target words: ['', '[UNK]', '[MASK]', '[START]', '[END]', '[cap]', '!', '\"', '&', \"'\", ',', '-', '.', '0']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = vocab_file.read_text('utf-8').split('\\n')\n",
    "# TextVectorization fails to construct if the vocabulary contains mask or [UNK] tokens.\n",
    "vocabulary = list(filter(lambda s: s and s != '[UNK]', vocabulary))\n",
    "\n",
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "\t# Standardize first with the BERT tokenizer\n",
    "\tstandardize=lambda s: tf.strings.reduce_join(bert_tokenize(s), separator = ' ', axis=-1),\n",
    "\tmax_tokens=max_vocab_size,\n",
    "\t# Allow entries of different lengths\n",
    "\tragged=True,\n",
    "\tvocabulary=vocabulary,\n",
    ")\n",
    "#target_text_processor.adapt(dataset_train.map(lambda context, target: target))\n",
    "\n",
    "print('First 14 target words:', target_text_processor.get_vocabulary()[:14])\n",
    "\n",
    "# The target data should be roughly equivalent to the context data, except have additional (punctuation)\n",
    "# tokens.\n",
    "context_text_processor = target_text_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d66c3",
   "metadata": {},
   "source": [
    "We can use these layers to convert to/from token IDs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4789f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "431d8e67",
   "metadata": {},
   "source": [
    "### Processing the data\n",
    "\n",
    "Now, we'll:\n",
    "1. Map the data through the text processors we just made.\n",
    "2. Shift the target data, so that our network is provided with a history of generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2efef357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "\treturn context_text_processor(context), target_text_processor(target)\n",
    "\n",
    "def add_target_history(context, target):\n",
    "\t# .to_tensor(): Converts from RaggedTensors to Tensors.\n",
    "\t# We give our network the history as target_in\n",
    "\ttarget_in = target[:, :-1].to_tensor()\n",
    "\ttarget_out = target[:, 1:].to_tensor()\n",
    "\treturn (context.to_tensor(), target_in), target_out\n",
    "dataset_train = dataset_train.map(process_text).map(add_target_history).repeat()\n",
    "dataset_validate = dataset_validate.map(process_text).map(add_target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50594897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context [START],v,##i,##ke,##la,##s,was,then,elected,the,first,president,of,the,new,##ly,established,international,olympic,committee,i,##o,##c,[END],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "target_in [START],[cap],v,##i,##ke,##la,##s,was,then,elected,the,first,president,of,the,new,##ly,established,[cap],international,[cap],olympic,[cap],committee,[cap],i,##o,##c,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "target_out [cap],v,##i,##ke,##la,##s,was,then,elected,the,first,president,of,the,new,##ly,established,[cap],international,[cap],olympic,[cap],committee,[cap],i,##o,##c,.,[END],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 20:48:35.824034: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "def inspect_dataset(dataset: tf.data.Dataset):\n",
    "\ttarget_vocab = np.array(target_text_processor.get_vocabulary())\n",
    "\tcontext_vocab = target_vocab\n",
    "\tfor (context, target_in), target_out in dataset.take(1):\n",
    "\t\tcontext_words = context_vocab[context[0]]\n",
    "\t\tprint('context', ','.join(context_words))\n",
    "\t\tprint('target_in', ','.join(target_vocab[target_in[0]]))\n",
    "\t\tprint('target_out', ','.join(target_vocab[target_out[0]]))\n",
    "\n",
    "inspect_dataset(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63bc6c",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### The encoder\n",
    "\n",
    "See https://www.tensorflow.org/text/tutorials/nmt_with_attention#the_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83f026f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Layer):\n",
    "\tdef __init__(self, text_processor, units: int):\n",
    "\t\t\"\"\"\n",
    "\t\tCreates a new Encoder layer. [dimen] is the maxiumum number of elements of the input\n",
    "\t\tthat can be processed by the encoder.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.text_processor = text_processor\n",
    "\t\tself.vocab_size = text_processor.vocabulary_size()\n",
    "\t\tself.units = units\n",
    "\n",
    "\t\t# Converts tokens -> vectors\n",
    "\t\tself.embedding = tf.keras.layers.Embedding(\n",
    "\t\t\t# mask_zero: Treats zero as a padding value that should be ignored\n",
    "\t\t\tself.vocab_size, units, mask_zero = True,\n",
    "\t\t)\n",
    "\t\tgru = tf.keras.layers.GRU(\n",
    "\t\t\tunits, return_sequences = True,\n",
    "\t\t\t# Use the recurrent_initializer suggested by the tutorial (& the default\n",
    "\t\t\t# for kernel_initializer).\n",
    "\t\t\trecurrent_initializer='glorot_uniform'\n",
    "\t\t)\n",
    "\t\tself.rnn = tf.keras.layers.Bidirectional(\n",
    "\t\t\t# merge_mode determines how the forward and backward layers are combined\n",
    "\t\t\t#            'concat' is another option here\n",
    "\t\t\tmerge_mode = 'sum',\n",
    "\t\t\tlayer=gru,\n",
    "\t\t)\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\tx = self.embedding(x)\n",
    "\t\tx = self.rnn(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef prepare_for_input(self, texts):\n",
    "\t\t\"\"\"\n",
    "\t\tUtility method that converts `texts` to a form that can be provided to the `call` method.\n",
    "\t\t\"\"\"\n",
    "\t\ttexts = tf.convert_to_tensor(texts)\n",
    "\t\tif len(texts.shape) == 0:\n",
    "\t\t\ttexts = texts[None]\n",
    "\t\tcontext = self.text_processor(texts).to_tensor()\n",
    "\t\treturn context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea196ff",
   "metadata": {},
   "source": [
    "Try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "331e474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 18:49:18.916428: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens shape (batch, s): (16, 59)\n",
      "Encoder output shape (batch, s, ENCODER_UNITS): (16, 59, 256)\n"
     ]
    }
   ],
   "source": [
    "ENCODER_UNITS = 256\n",
    "encoder = Encoder(context_text_processor, ENCODER_UNITS)\n",
    "\n",
    "for (context, target_history), target_next in dataset_validate.take(1):\n",
    "\tencoder_result = encoder(context)\n",
    "\tprint('Context tokens shape (batch, s):', context.shape)\n",
    "\tprint('Encoder output shape (batch, s, ENCODER_UNITS):', encoder_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd7fb6",
   "metadata": {},
   "source": [
    "### The attention layer\n",
    "\n",
    "Attention can be thought of as training a lookup table with keys and values. The lookup table has inputs `values` and `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "781b2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.Layer):\n",
    "\tdef __init__(self, units, **kwargs):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.attention_layer = tf.keras.layers.MultiHeadAttention(\n",
    "\t\t\tkey_dim=units,\n",
    "\t\t\tnum_heads=1,\n",
    "\t\t\t**kwargs\n",
    "\t\t)\n",
    "\t\t# Keeps \"the mean activation within each example close to 0 and the\n",
    "\t\t# activation standard deviation close to 1\" -- https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization?hl=en\n",
    "\t\tself.norm_layer = tf.keras.layers.LayerNormalization()\n",
    "\t\tself.add_layer = tf.keras.layers.Add()\n",
    "\t\tself.supports_masking = True\n",
    "\n",
    "\tdef call(self, query, value):\n",
    "\t\tattention_output = self.attention_layer(\n",
    "\t\t\tquery = query,\n",
    "\t\t\tvalue = value,\n",
    "\t\t\t#use_causal_mask=True,\n",
    "\t\t\t# Return the attention scores for latter plotting\n",
    "\t\t\t# return_attention_scores = True,\n",
    "\t\t)\n",
    "\n",
    "\t\tx = self.add_layer([ query, attention_output ])\n",
    "\t\tx = self.norm_layer(x)\n",
    "\t\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c8857ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 18:49:27.803434: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded context sequence shape (batch, s, units): (16, 138, 256)\n",
      "Target history sequence shape (batch, t, units): (16, 167, 256)\n",
      "Attention result shape (batch, t, units): (16, 167, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(ENCODER_UNITS)\n",
    "\n",
    "# Test with an example\n",
    "for (context, target_history), target_next in dataset_validate.take(1):\n",
    "\tembed_layer = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(), output_dim=ENCODER_UNITS, mask_zero=True)\n",
    "\ttarget_embed = embed_layer(target_history)\n",
    "\tencoded_context = encoder(context)\n",
    "\tattention_result = attention_layer(target_embed, encoded_context)\n",
    "\n",
    "\tprint('Encoded context sequence shape (batch, s, units):', encoded_context.shape)\n",
    "\tprint('Target history sequence shape (batch, t, units):', target_embed.shape)\n",
    "\tprint('Attention result shape (batch, t, units):', attention_result.shape)\n",
    "\n",
    "\t# Used later \n",
    "\ttest_encoded_context = encoded_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5577a1",
   "metadata": {},
   "source": [
    "### The decoder\n",
    "\n",
    "The decoder produces queries for the attention layer. The decoder operates on `target_history`. At each step during training, it should have no information about future target output (that's what we're trying to determine). As such, we use a unidirectional RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2454259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDense(tf.keras.layers.Dense):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tsuper(CustomDense, self).__init__(*args, **kwargs)\n",
    "\t\n",
    "\tdef compute_mask(self, _inputs, mask=None):\n",
    "\t\treturn mask\n",
    "\n",
    "class Decoder(tf.keras.Layer):\n",
    "\tdef __init__(self, text_processor, units):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.text_processor = text_processor\n",
    "\t\tself.vocab_size = text_processor.vocabulary_size()\n",
    "\t\tself.units = units\n",
    "\n",
    "\t\tself.embedding_layer = tf.keras.layers.Embedding(\n",
    "\t\t\t# mask_zero: Treats zero as a padding value that should be ignored\n",
    "\t\t\tself.vocab_size, units, mask_zero = True,\n",
    "\t\t)\n",
    "\t\tself.rnn_layer = tf.keras.layers.GRU(\n",
    "\t\t\tunits, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform',\n",
    "\t\t)\n",
    "\t\tself.attention_layer = CrossAttention(units)\n",
    "\n",
    "\t\t# Creates logits with the estimated probability of each output token\n",
    "\t\tself.output_layer = CustomDense(self.vocab_size)\n",
    "\n",
    "\t\t# Conversion:\n",
    "\t\tself.word_to_id = tf.keras.layers.StringLookup(\n",
    "\t\t\tvocabulary = text_processor.get_vocabulary(),\n",
    "\t\t\tmask_token = '',\n",
    "\t\t\toov_token = '[UNK]',\n",
    "\t\t)\n",
    "\t\tself.id_to_word = tf.keras.layers.StringLookup(\n",
    "\t\t\tvocabulary = text_processor.get_vocabulary(),\n",
    "\t\t\tmask_token = '',\n",
    "\t\t\toov_token = '[UNK]',\n",
    "\t\t\tinvert = True,\n",
    "\t\t)\n",
    "\t\t# Pre-computing these simplifies exporting\n",
    "\t\tself.start_id = self.word_to_id('[START]')\n",
    "\t\tself.end_id = self.word_to_id('[END]')\n",
    "\n",
    "\t\tself.supports_masking = True\n",
    "\t\n",
    "\tdef build(self, input_shape):\n",
    "\t\t# Nothing tha needs a size allocation based on the input shape\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef call(self, context, target_history, state = None, return_state = False):\n",
    "\t\tx = self.embedding_layer(target_history)\n",
    "\t\tx, state = self.rnn_layer(x, initial_state = state)\n",
    "\t\tx = self.attention_layer(x, context)\n",
    "\n",
    "\t\tlogits = self.output_layer(x)\n",
    "\t\tif return_state:\n",
    "\t\t\treturn logits, state\n",
    "\t\telse:\n",
    "\t\t\treturn logits\n",
    "\t\n",
    "\t## Conversion/testing ##\n",
    "\n",
    "\tdef tokens_to_text(self, tokens):\n",
    "\t\ttext = tf.strings.reduce_join(self.id_to_word(tokens), separator = ' ')\n",
    "\t\ttext = tf.strings.regex_replace(text, r'\\s*\\[START\\]\\s*', '')\n",
    "\t\ttext = tf.strings.regex_replace(text, r'\\s*\\[END\\].*$', '')\n",
    "\t\treturn text\n",
    "\n",
    "\tdef generate_next_token(self, context, target_history, done_vec, state, temperature = 0.0):\n",
    "\t\t# Note: is_done is a vector, indicating whether each item in the batch is done\n",
    "\n",
    "\t\tlogits, state = self(context, target_history, state = state, return_state = True)\n",
    "\n",
    "\t\t# logits has shape (batch, t, target_vocab_size). Only generate the token corresponding\n",
    "\t\t# to the last logits in the sequence (at t - 1)\n",
    "\t\tif temperature > 0:\n",
    "\t\t\tnext_token = tf.where(\n",
    "\t\t\t\tdone_vec,\n",
    "\t\t\t\ttf.constant(0, dtype=tf.int64), # Emit 0 after a sequence is done\n",
    "\t\t\t\ttf.random.categorical(logits[:, -1, :] / temperature, num_samples = 1), # Otherwise, pick the token from a categorical distribution\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tnext_token = tf.math.argmax(logits, axis=-1)\n",
    "\t\tdone_vec = done_vec|(next_token == self.end_id)\n",
    "\t\treturn next_token, done_vec, state\n",
    "\t\n",
    "\tdef get_initial_state(self, context):\n",
    "\t\t# context has shape (batch_size, s, units)\n",
    "\t\tbatch_size = tf.shape(context)[0]\n",
    "\t\tstart_tokens = tf.fill([batch_size, 1], self.start_id)\n",
    "\t\tdone_vec = tf.zeros([batch_size, 1], dtype = tf.bool)\n",
    "\n",
    "\t\t# From the Tensorflow source code:\n",
    "\t\t# > RNN expect the states in a list, even if single state.\n",
    "\t\t# Note: Without the [0] we get a type mismatch while exporting.\n",
    "\t\tinitial_state = self.rnn_layer.get_initial_state(batch_size)[0]\n",
    "\n",
    "\t\treturn start_tokens, done_vec, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d5ccb1",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "219a3522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: b'roman scientists 2016 rock ##ians plan ##ian causedroman scientists 2016 rock ##ians plan ##ian causedroman scientists 2016 rock ##ians plan ##ian caused'\n"
     ]
    }
   ],
   "source": [
    "def test_generation_loop():\n",
    "\tdecoder = Decoder(target_text_processor, ENCODER_UNITS)\n",
    "\tnext_token, done_vec, state = decoder.get_initial_state(test_encoded_context[:3, :, :])\n",
    "\ttokens = [next_token]\n",
    "\n",
    "\tfor i in range(8):\n",
    "\t\tnext_token, done_vec, state = decoder.generate_next_token(test_encoded_context[:3, :, :], next_token, done_vec, state)\n",
    "\t\ttokens.append(next_token)\n",
    "\t\n",
    "\t# Merge all batch outputs into a single dimension\n",
    "\ttokens = tf.concat(tokens, -1) # -1 = last axis\n",
    "\n",
    "\tprint('Output:', decoder.tokens_to_text(tokens).numpy())\n",
    "\n",
    "test_generation_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3411a",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b3a40",
   "metadata": {},
   "source": [
    "We can now build a model for training and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1520bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Punctuator(tf.keras.Model):\n",
    "\tdef __init__(self, units, context_text_processor, target_text_processor):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encoder = Encoder(context_text_processor, units)\n",
    "\t\tself.decoder = Decoder(target_text_processor, units)\n",
    "\t\n",
    "\tdef call(self, inputs):\n",
    "\t\tcontext, target_history = inputs\n",
    "\t\tcontext = self.encoder(context)\n",
    "\t\tlogits = self.decoder(context, target_history)\n",
    "\t\treturn logits\n",
    "\t\n",
    "\tdef fix_punctuation_raw(self, input):\n",
    "\t\t\"\"\"\n",
    "\t\tAdds punctuation to `input`, where `input` is a `Tensor` with shape (batch_size, s) where s is the\n",
    "\t\tcontext length.\n",
    "\t\t\"\"\"\n",
    "\t\tcontext = self.encoder(input)\n",
    "\n",
    "\t\tnext_token, done_vec, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "\t\t# Although a TensorArray would allow more efficient exporting, the ONNX exporter seems to\n",
    "\t\t# have trouble with it. For now, use a Python list.\n",
    "\t\ttokens = []\n",
    "\t\tmax_iterations = 56\n",
    "\n",
    "\t\tfor i in range(max_iterations):\n",
    "\t\t\t# token_history has size: (batch, t, target_vocab_size)\n",
    "\t\t\t# token_history = tf.concat(tokens, 1)\n",
    "\t\t\t# print('history', model.decoder.id_to_word(token_history))\n",
    "\t\t\tnext_token, done_vec, state = self.decoder.generate_next_token(context, next_token, done_vec, state, temperature=0)\n",
    "\t\t\t#tokens = tokens.write(i + 1, next_token)\n",
    "\t\t\ttokens.append(next_token)\n",
    "\n",
    "\t\t\tif tf.executing_eagerly() and tf.reduce_all(done_vec):\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\ttokens = tf.concat(tokens, -1)\n",
    "\t\treturn tokens\n",
    "\n",
    "\tdef fix_punctuation(self, text: list[str]):\n",
    "\t\tinputs = self.encoder.prepare_for_input(text)\n",
    "\t\ttokens = self.fix_punctuation_raw(inputs)\n",
    "\t\treturn self.decoder.tokens_to_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "253c670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 18:49:36.970873: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens shape (batch, s): (16, 136)\n",
      "Target history tokens shape (batch, t): (16, 151)\n",
      "Logits shape (batch, t, vocab_size) (16, 151, 3890)\n"
     ]
    }
   ],
   "source": [
    "model = Punctuator(ENCODER_UNITS, context_text_processor, target_text_processor)\n",
    "\n",
    "for (example_context_tok, example_target_hist), _ in dataset_validate.take(1):\n",
    "\ttest_logits = model((example_context_tok, example_target_hist))\n",
    "\tprint('Context tokens shape (batch, s):', example_context_tok.shape)\n",
    "\tprint('Target history tokens shape (batch, t):', example_target_hist.shape)\n",
    "\tprint('Logits shape (batch, t, vocab_size)', test_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68e81b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"punctuator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"punctuator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,785,344</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,654,002</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m1,785,344\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │     \u001b[38;5;34m2,654,002\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,439,346</span> (16.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,439,346\u001b[0m (16.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,439,346</span> (16.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,439,346\u001b[0m (16.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85551c17",
   "metadata": {},
   "source": [
    "To avoid penalizing masked outputs, we use a custom loss function (see the tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ca4d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss(y_true, y_predict):\n",
    "\tloss = base_loss_fn(y_true, y_predict)\n",
    "\t\n",
    "\tunmasked = y_true != 0\n",
    "\tunmasked = tf.cast(unmasked, loss.dtype)\n",
    "\t# Only consider output with a corresponding label.\n",
    "\tloss *= unmasked\n",
    "\n",
    "\tcount_unmasked = tf.math.reduce_sum(unmasked)\n",
    "\n",
    "\t# reduce_sum: Adds all entries of a vector.\n",
    "\treturn tf.math.reduce_sum(loss)/count_unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ba83b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, predict_logits):\n",
    "\tpredicted_index = tf.math.argmax(predict_logits, axis=-1)\n",
    "\tpredicted_index = tf.cast(predicted_index, y_true.dtype)\n",
    "\n",
    "\tmatch = tf.cast(y_true == predicted_index, tf.float32)\n",
    "\tunmasked = tf.cast(y_true != 0, tf.float32)\n",
    "\tcount_unmasked = tf.math.reduce_sum(unmasked)\n",
    "\n",
    "\treturn tf.math.reduce_sum(match * unmasked) / count_unmasked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d9fde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[masked_accuracy, masked_loss])\n",
    "model.compile(optimizer='adam', loss=masked_loss, metrics=[masked_accuracy, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95ae7f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the tutorial:\n",
      "expected loss 8.266165\n",
      "expected accuracy 0.0002570694087403599\n"
     ]
    }
   ],
   "source": [
    "print('From the tutorial:')\n",
    "vocab_size = float(target_text_processor.vocabulary_size())\n",
    "\n",
    "print('expected loss', tf.math.log(vocab_size).numpy())\n",
    "print('expected accuracy', 1/vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "091b83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 2/20\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - loss: 8.2703 - masked_accuracy: 0.0000e+00 - masked_loss: 8.2703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 18:49:47.637571: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-09-10 18:49:47.814254: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 8.2718 - masked_accuracy: 0.0000e+00 - masked_loss: 5.7902 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 8.27199935913086,\n",
       " 'masked_accuracy': 0.0,\n",
       " 'masked_loss': 5.5146660804748535}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset_validate, steps=20, return_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a179e6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[test]: slow standard merely ##back military cousin reply style charles relief colonists importance piece ##pted suicide per roman ##zer interview fuel ##ca originally ##ground empty wind won villefort seasons boat wine ##owed ##istic ##ap oh atmosphere ##ization abbé 2010 descended nor my probably ##tered throne collection woods paper lost slavery star ##father ##ey inhabitants ##ak board few'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def test_punctuation(text):\n",
    "\treturn '[test]: ' + model.fix_punctuation(text).numpy().decode('utf-8')\n",
    "\n",
    "class DemoCallback(tf.keras.callbacks.Callback):\n",
    "\tdef on_epoch_end(self, epoch_index: int, logs = None):\n",
    "\t\tprint('\\r', test_punctuation([ 'im testing this models performance how well is it working' ]))\n",
    "\t\tprint(test_punctuation([ 'i think its working well but its really hard to tell why are the question marks missing' ]))\n",
    "\t\tif epoch_index % 3 == 0:\n",
    "\t\t\t# From the test data\n",
    "\t\t\tprint(test_punctuation([\n",
    "\t\t\t\t'not that alice had any idea of doing that she felt as if she would never be able to talk again she was getting so much out of breath and still the queen cried faster faster and dragged her along'\n",
    "\t\t\t]))\n",
    "\t\t\tprint(test_punctuation([ 'tensorflow is a library that is used for machine learning it is available for more languages than just python' ]))\n",
    "\t\t\tprint(test_punctuation([ 'the joplin note taking app can be used to take multimedia notes' ]))\n",
    "\t\t\tprint(test_punctuation([ 'here are a few words javascript typescript python joplin interesting loud and sequence these words are all very useful' ]))\n",
    "\n",
    "test_punctuation(tf.constant([ 'this is an example they said' ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a2db06e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " [test]: [cap] i ' m test ##ing this models performance ? [cap] how well is it working ?.1748 - masked_accuracy: 0.9517 - masked_loss: 0.1748\n",
      "[test]: [cap] i think it ' s working well , but it ' s really hard to tell why are the question mark ' s miss ##ing .\n",
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would never be able to talk again she was getting so much out of breath and still , the [cap] queen cried faster faster and dr ##ag ##ged her along .\n",
      "[test]: [cap] ten ##s ##or ##f ##lo ##w is a library that is used for machine learn ##ing it is available for more languages than just p ##y ##th ##on .\n",
      "[test]: [cap] the [cap] j ##op ##lin note taking a ##p ##p can be used to take m ##ult ##imed ##ia notes .\n",
      "[test]: [cap] here are a few words , [cap] j ##a ##va ##s ##cript types ##cript , [cap] p ##y ##th ##on , [cap] j ##op ##lin interest ##ing loud and sequence these words are all very useful .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 233ms/step - loss: 0.1748 - masked_accuracy: 0.9517 - masked_loss: 0.1748 - val_loss: 0.1111 - val_masked_accuracy: 0.6425 - val_masked_loss: 0.0741\n",
      "Epoch 2/30\n",
      " [test]: [cap] i ' m test ##ing this models performance how well is it working . loss: 0.2071 - masked_accuracy: 0.9446 - masked_loss: 0.2071\n",
      "[test]: [cap] i think it ' s working , [cap] well , but it ' s really hard to tell why are the question mark ' s miss ##ing .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 194ms/step - loss: 0.2071 - masked_accuracy: 0.9446 - masked_loss: 0.2071 - val_loss: 0.1734 - val_masked_accuracy: 0.6365 - val_masked_loss: 0.1156\n",
      "Epoch 3/30\n",
      " [test]: [cap] i ' m test ##ing this models ' performance , how well is it working ?s: 0.2291 - masked_accuracy: 0.9389 - masked_loss: 0.2291\n",
      "[test]: [cap] i think it ' s working well , but its really hard to tell . [cap] why are the question mark ' s miss ##ing .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 185ms/step - loss: 0.2291 - masked_accuracy: 0.9389 - masked_loss: 0.2291 - val_loss: 0.1903 - val_masked_accuracy: 0.6301 - val_masked_loss: 0.1269\n",
      "Epoch 4/30\n",
      " [test]: [cap] i ' m test ##ing this models ' performance how well is it working ?oss: 0.2379 - masked_accuracy: 0.9357 - masked_loss: 0.2379\n",
      "[test]: [cap] i think it ' s working well , but it ' s really hard to tell why are the question mark ##s miss ##ing .\n",
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would never be able to talk again she was getting so much out of breath , and still the queen cried faster faster and dr ##ag ##ged her along .\n",
      "[test]: [cap] ten ##s ##or ##f ##lo ##w is a library that is used for machine learn ##ing it is available for more languages than just p ##y ##th ##on .\n",
      "[test]: [cap] the [cap] j ##op ##lin note taking a ##p ##p can be used to take [cap] m ##ult ##imed ##ia notes .\n",
      "[test]: [cap] here are a few words , [cap] j ##a ##va ##s ##cript types ##cript , [cap] p ##y ##th ##on , [cap] j ##op ##lin , [cap] interest ##ing [cap] loud and [cap] sequence [cap] these [cap] words are all very useful .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 186ms/step - loss: 0.2379 - masked_accuracy: 0.9357 - masked_loss: 0.2379 - val_loss: 0.1523 - val_masked_accuracy: 0.6367 - val_masked_loss: 0.1015\n",
      "Epoch 5/30\n",
      " [test]: [cap] i ' m test ##ing this model ' s performance , how well is it working .: 0.2449 - masked_accuracy: 0.9330 - masked_loss: 0.2449\n",
      "[test]: [cap] i think it ' s working well , but it ' s really hard to tell . [cap] why are the question mark ##s miss ##ing .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 186ms/step - loss: 0.2449 - masked_accuracy: 0.9330 - masked_loss: 0.2449 - val_loss: 0.1846 - val_masked_accuracy: 0.6252 - val_masked_loss: 0.1231\n",
      "Epoch 6/30\n",
      " [test]: [cap] i ' m test ##ing this ' s performance . [cap] how well is it working .: 0.2456 - masked_accuracy: 0.9324 - masked_loss: 0.2456\n",
      "[test]: [cap] i think it ' s working . [cap] well , but it ' s really , hard to tell why are the question mark ' s miss ##ing .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 187ms/step - loss: 0.2456 - masked_accuracy: 0.9324 - masked_loss: 0.2456 - val_loss: 0.2547 - val_masked_accuracy: 0.6192 - val_masked_loss: 0.1698\n",
      "Epoch 7/30\n",
      " [test]: [cap] i ' m test ##ing this model ' s performance , how well is it working .: 0.2675 - masked_accuracy: 0.9274 - masked_loss: 0.2675\n",
      "[test]: [cap] i think it ' s working well , but it ' s really hard to tell why are the question mark ##s miss ##ing .\n",
      "[test]: - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "[test]: [cap] ten ##s ##or ##f ##lo ##w is a library that is used for machine learn ##ing it is available for more languages than just p ##y ##th ##on .\n",
      "[test]: [cap] the [cap] j ##op ##lin note taking a ##p ##p can be used to take m ##ult ##imed ##ia notes .\n",
      "[test]: [cap] here are a few words , [cap] j ##a ##va ##s ##cript , [cap] types ##cript , [cap] p ##y ##th ##on , [cap] j ##op ##lin interest ##ing , loud ##ing , loud ##ing , loud very useful .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 191ms/step - loss: 0.2675 - masked_accuracy: 0.9274 - masked_loss: 0.2675 - val_loss: 0.1862 - val_masked_accuracy: 0.6286 - val_masked_loss: 0.1242\n",
      "Epoch 8/30\n",
      " [test]: [cap] i ' m test ##ing this model ' s performance , how well is it working .: 0.2581 - masked_accuracy: 0.9299 - masked_loss: 0.2581\n",
      "[test]: [cap] i think it ' s working well , but it ' s really hard to tell why are the question mark ##s miss ##ing .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 187ms/step - loss: 0.2581 - masked_accuracy: 0.9299 - masked_loss: 0.2581 - val_loss: 0.2472 - val_masked_accuracy: 0.6237 - val_masked_loss: 0.1648\n",
      "Epoch 9/30\n",
      " [test]: [cap] i ' m test ##ing this model ' s performance , how well is it working .: 0.2567 - masked_accuracy: 0.9301 - masked_loss: 0.2567\n",
      "[test]: [cap] i think it ' s working well , but it ' s really hard to tell why are the question mark ##s miss ##ing .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 187ms/step - loss: 0.2567 - masked_accuracy: 0.9301 - masked_loss: 0.2567 - val_loss: 0.1942 - val_masked_accuracy: 0.6315 - val_masked_loss: 0.1295\n",
      "Epoch 10/30\n",
      " [test]: [cap] i ' m test ##ing this models performance how well is it working . loss: 0.2567 - masked_accuracy: 0.9303 - masked_loss: 0.2567\n",
      "[test]: [cap] i think it ' s working well , but it ' s really hard to tell . [cap] why are tell [cap] why are the question mark ' s miss ##ing .\n",
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would never be able to talk again , she was getting so much out of breath , and still the [cap] queen cried , faster , faster and still the [cap] queen cried , faster , faster and dr ##ag\n",
      "[test]: [cap] ten ##s ##or ##f ##lo ##w is a library that is used for machine learn ##ing it is available for more languages than just p ##y ##th ##on .\n",
      "[test]: [cap] the [cap] j ##op ##lin note taking [cap] a ##p ##p can be used to take [cap] m ##ult ##imed ##ia notes .\n",
      "[test]: [cap] here are a few words , j ##a ##va ##s ##cript types ##cript , [cap] p ##y ##th ##on , [cap] j ##op ##lin , interest ##ing loud , and sequence these words are all very useful .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 191ms/step - loss: 0.2567 - masked_accuracy: 0.9303 - masked_loss: 0.2567 - val_loss: 0.2511 - val_masked_accuracy: 0.6250 - val_masked_loss: 0.1674\n",
      "Epoch 11/30\n",
      " [test]: [cap] i ' m test ##ing this model ' s performance how well is it working .ss: 0.2574 - masked_accuracy: 0.9303 - masked_loss: 0.2574\n",
      "[test]: [cap] i think its working well , but its really hard to tell why are the question mark ##s miss ##ing .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 204ms/step - loss: 0.2574 - masked_accuracy: 0.9303 - masked_loss: 0.2574 - val_loss: 0.2510 - val_masked_accuracy: 0.6220 - val_masked_loss: 0.1673\n",
      "Epoch 12/30\n",
      " [test]: [cap] i ' m test ##ing this models ' performance how well is it working .oss: 0.2326 - masked_accuracy: 0.9369 - masked_loss: 0.2326\n",
      "[test]: [cap] i think it ' s working well , but its really hard to tell why are the question mark ##s miss ##ing .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 240ms/step - loss: 0.2325 - masked_accuracy: 0.9369 - masked_loss: 0.2325 - val_loss: 0.2209 - val_masked_accuracy: 0.6268 - val_masked_loss: 0.1473\n",
      "Epoch 13/30\n",
      " [test]: [cap] i ' m test ##ing this models performance how well is it working . loss: 0.1993 - masked_accuracy: 0.9455 - masked_loss: 0.1993\n",
      "[test]: [cap] i think it ' s working well , but it ' s really hard to tell why are the question mark ##s miss ##ing .\n",
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would never be able to talk again , she was getting so much out of breath , and still the [cap] queen cried faster faster and dr ##ag ##ged her along .\n",
      "[test]: [cap] ten ##s ##or ##f ##lo ##w is a library that is used for machine learn ##ing it is available for more languages than just p ##y ##th ##on .\n",
      "[test]: [cap] the [cap] j ##op ##lin note taking a ##p ##p can be used to take m ##ult ##imed ##ia notes .\n",
      "[test]: [cap] here are a few words , [cap] j ##a - [cap] b ##s ##cript , types ##cript p ##y ##th ##on , j ##op ##lin , interest ##ing loud and sequence these words are all very useful .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 234ms/step - loss: 0.1994 - masked_accuracy: 0.9455 - masked_loss: 0.1994 - val_loss: 0.2066 - val_masked_accuracy: 0.6356 - val_masked_loss: 0.1377\n",
      "Epoch 14/30\n",
      " [test]: [cap] i ' m test ##ing this models performance how well is it working . loss: 0.2159 - masked_accuracy: 0.9415 - masked_loss: 0.2159\n",
      "[test]: [cap] i think it ' s working well , but it ' s really hard to tell why are the question mark ##s miss ##ing .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 231ms/step - loss: 0.2159 - masked_accuracy: 0.9415 - masked_loss: 0.2159 - val_loss: 0.1887 - val_masked_accuracy: 0.6345 - val_masked_loss: 0.1258\n",
      "Epoch 15/30\n",
      " [test]: [cap] i ' m test ##ing this models performance how well is it working . loss: 0.2253 - masked_accuracy: 0.9394 - masked_loss: 0.2253\n",
      "[test]: [cap] i think it ' s working well , but it ' s really hard to tell why are the question mark ##s miss ##ing .\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 215ms/step - loss: 0.2253 - masked_accuracy: 0.9394 - masked_loss: 0.2253 - val_loss: 0.1860 - val_masked_accuracy: 0.6328 - val_masked_loss: 0.1240\n",
      "Epoch 16/30\n",
      "\u001b[1m 469/1200\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 197ms/step - loss: 0.2356 - masked_accuracy: 0.9367 - masked_loss: 0.2356"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m\t\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_validate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mDemoCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:322\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    320\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    321\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m--> 322\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/callbacks/callback_list.py:106\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    104\u001b[0m logs \u001b[38;5;241m=\u001b[39m logs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/callbacks/progbar_logger.py:58\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/callbacks/progbar_logger.py:95\u001b[0m, in \u001b[0;36mProgbarLogger._update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m=\u001b[39m batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# One-indexed.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/utils/progbar.py:182\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m finalize:\n\u001b[1;32m    180\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 182\u001b[0m \u001b[43mio_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_total_width \u001b[38;5;241m=\u001b[39m total_width\n\u001b[1;32m    184\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/utils/io_utils.py:99\u001b[0m, in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m         sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(message)\n\u001b[0;32m---> 99\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(message)\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/ipykernel/iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib64/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib64/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "\tdataset_train,\n",
    "\tepochs = 30,\n",
    "\tsteps_per_epoch = 1200,\n",
    "\tvalidation_data = dataset_validate,\n",
    "\tcallbacks=[DemoCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f68a9fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would never be able to talk again , she was getting so much out of breath , and still the [cap] queen cried , faster , faster and dr ##ag ##ged her along .\n",
      "[test]: [cap] this is a test of the p ##un ##ct ##uation system , for [cap] i am c ##urious how well it works will it work .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_punctuation([\n",
    "\t'not that alice had any idea of doing that she felt as if she would never be able to talk again she was getting so much out of breath and still the queen cried faster faster and dragged her along'\n",
    "]))\n",
    "print(test_punctuation([ 'this is a test of the punctuation system for i am curious how well it works will it work' ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e9cc9",
   "metadata": {},
   "source": [
    "## Exporting\n",
    "\n",
    "Based on the [Export](https://www.tensorflow.org/text/tutorials/nmt_with_attention#export) section of the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9885c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Export(tf.Module):\n",
    "\tdef __init__(self, model):\n",
    "\t\tself.model = model\n",
    "\t\n",
    "\t@tf.function(input_signature=[tf.RaggedTensorSpec(dtype=tf.int64, shape=[None])])\n",
    "\tdef fix_punctuation(self, input):\n",
    "\t\t# Returns encoded tokens\n",
    "\t\treturn model.fix_punctuation_raw(\n",
    "\t\t\ttf.reshape(input, [1, -1])\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcac14",
   "metadata": {},
   "source": [
    "Run `fix_punctuation` once to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa037d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = Export(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8af27599",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Binding inputs to tf.function failed due to `The two structures don't have the same nested structure.\n\nFirst structure: type=list str=[TensorSpec(shape=(None,), dtype=tf.int64, name=None)]\n\nSecond structure: type=list str=[<tf.Tensor: shape=(32,), dtype=int64, numpy=\narray([   3,  122,  826,  969,  285,  116,   50,  984,  426, 1432,  104,\n         89,  413, 3044,  161,   50,  984,  426,  396,  276,  214, 1429,\n        689,   39,  469,  758,   94,  836,  342,   55,  194,    4])>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 0, 32])>]\n\nMore specifically: The two structures don't have the same number of elements. First structure: type=list str=[TensorSpec(shape=(None,), dtype=tf.int64, name=None)]. Second structure: type=list str=[<tf.Tensor: shape=(32,), dtype=int64, numpy=\narray([   3,  122,  826,  969,  285,  116,   50,  984,  426, 1432,  104,\n         89,  413, 3044,  161,   50,  984,  426,  396,  276,  214, 1429,\n        689,   39,  469,  758,   94,  836,  342,   55,  194,    4])>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 0, 32])>]\nEntire first structure:\n[.]\nEntire second structure:\n[., .]`. Received args: (<tf.RaggedTensor [[3, 122, 826, 969, 285, 116, 50, 984, 426, 1432, 104, 89, 413, 3044, 161,\n  50, 984, 426, 396, 276, 214, 1429, 689, 39, 469, 758, 94, 836, 342, 55,\n  194, 4]]>,) and kwargs: {} for signature: (input: RaggedTensorSpec(TensorShape([None]), tf.int64, 0, tf.int64)).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_inputs \u001b[38;5;241m=\u001b[39m context_text_processor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis sentence shall be punctuated for the following reasons first punctatuion makes things easier to read second um\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mtokens_to_text(\u001b[43mexport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfix_punctuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_inputs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:446\u001b[0m, in \u001b[0;36mbind_function_inputs\u001b[0;34m(args, kwargs, function_type, default_values)\u001b[0m\n\u001b[1;32m    442\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mbind_with_defaults(\n\u001b[1;32m    443\u001b[0m       args, sanitized_kwargs, default_values\n\u001b[1;32m    444\u001b[0m   )\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 446\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    447\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBinding inputs to tf.function failed due to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for signature:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m   ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bound_arguments\n",
      "\u001b[0;31mTypeError\u001b[0m: Binding inputs to tf.function failed due to `The two structures don't have the same nested structure.\n\nFirst structure: type=list str=[TensorSpec(shape=(None,), dtype=tf.int64, name=None)]\n\nSecond structure: type=list str=[<tf.Tensor: shape=(32,), dtype=int64, numpy=\narray([   3,  122,  826,  969,  285,  116,   50,  984,  426, 1432,  104,\n         89,  413, 3044,  161,   50,  984,  426,  396,  276,  214, 1429,\n        689,   39,  469,  758,   94,  836,  342,   55,  194,    4])>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 0, 32])>]\n\nMore specifically: The two structures don't have the same number of elements. First structure: type=list str=[TensorSpec(shape=(None,), dtype=tf.int64, name=None)]. Second structure: type=list str=[<tf.Tensor: shape=(32,), dtype=int64, numpy=\narray([   3,  122,  826,  969,  285,  116,   50,  984,  426, 1432,  104,\n         89,  413, 3044,  161,   50,  984,  426,  396,  276,  214, 1429,\n        689,   39,  469,  758,   94,  836,  342,   55,  194,    4])>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 0, 32])>]\nEntire first structure:\n[.]\nEntire second structure:\n[., .]`. Received args: (<tf.RaggedTensor [[3, 122, 826, 969, 285, 116, 50, 984, 426, 1432, 104, 89, 413, 3044, 161,\n  50, 984, 426, 396, 276, 214, 1429, 689, 39, 469, 758, 94, 836, 342, 55,\n  194, 4]]>,) and kwargs: {} for signature: (input: RaggedTensorSpec(TensorShape([None]), tf.int64, 0, tf.int64))."
     ]
    }
   ],
   "source": [
    "sample_inputs = context_text_processor('this sentence shall be punctuated for the following reasons first punctatuion makes things easier to read second um')\n",
    "model.decoder.tokens_to_text(export.fix_punctuation(sample_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70247292",
   "metadata": {},
   "source": [
    "Now we save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "badb8d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: punctuator-seq2seq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: punctuator-seq2seq/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(export, 'punctuator-seq2seq', signatures={ 'serving_default': export.fix_punctuation })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197976b4",
   "metadata": {},
   "source": [
    "See [the documentation](https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export) for information about the `signatures` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56d91fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43430"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "web_output_dir = Path('web')\n",
    "vocab_output_file = web_output_dir / 'wordEncodings.ts'\n",
    "\n",
    "vocab_output_file.write_text('''\n",
    "// Auto-generated file!\n",
    "// Created by v2-seq2seq.ipynb\n",
    "export default {};\n",
    "'''.format(json.dumps(target_text_processor.get_vocabulary(), indent = '\\t')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d873f2",
   "metadata": {},
   "source": [
    "### Testing the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c39fbc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported and warmed up!\n"
     ]
    }
   ],
   "source": [
    "reloaded = tf.saved_model.load('punctuator-seq2seq')\n",
    "# Warmup\n",
    "reloaded.fix_punctuation(sample_inputs)\n",
    "print('Imported and warmed up!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca974c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 152 ms, sys: 64.7 ms, total: 216 ms\n",
      "Wall time: 65.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'[cap] thi [s] sentence shall be [UNK] [ed] for the follow [ing] reason [s] . [cap] first [UNK] make [s] thing [s] easi [er] to read second [UNK] .'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.decoder.tokens_to_text(reloaded.fix_punctuation(sample_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afb2771e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(27,), dtype=int64, numpy=\n",
       "array([   8,   45,    3, 2553,  193,   40,    1,    7,   28,    5,  139,\n",
       "         14,  556,    3,   78,    1,  146,    3,  253,    3, 1003,   12,\n",
       "         13,  384,  213,    1,    9])>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text_processor('this sentence shall be punctuated for the following reasons first punctatuion makes things easier to read second um')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa1bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
