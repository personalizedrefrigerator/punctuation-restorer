{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55e2298",
   "metadata": {},
   "source": [
    "# V2: SEQ2SEQ\n",
    "\n",
    "This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention#create_a_tfdata_dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71899cc9-86b4-4eda-b525-d818661c70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 14:20:20.263898: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-05 14:20:20.266879: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-05 14:20:20.276312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-05 14:20:20.291136: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-05 14:20:20.295456: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-05 14:20:20.307899: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-05 14:20:21.158235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa459db2-5a48-4bac-8cbd-38496888faee",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention) (and [at least one other](https://www.tensorflow.org/text/tutorials/text_generation) of the Tensorflow tutorials).\n",
    "- This [blog post](https://janakiev.com/blog/jupyter-virtual-envs/) was referenced to set up the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b150ba-48c0-4f33-8d08-b9f363d6fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Tuple\n",
    "from IPython.display import display, Markdown\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5faca0eb-c672-4d1c-884f-b5ba38a22b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_paths = list(Path('data/processed/en/').glob('*.txt'))\n",
    "dataset_raw = tf.data.TextLineDataset(\n",
    "\tdata_file_paths,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890fa88",
   "metadata": {},
   "source": [
    "We've now loaded the `.txt` training data files using [`tf.data.TextLineDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset). Each line in the source files is mapped to a new training example. \n",
    "\n",
    "Although some preprocessing has been done by `/data/process_data.py`, paragraphs aren't filtered out based on length/content. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5c607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_paragraphs(context, target):\n",
    "\treturn tf.strings.length(context) > 5\n",
    "\n",
    "punctuation_chars = r'\\?!.,\"\\-\\':'\n",
    "def add_context(target):\n",
    "\tcontext = tf.strings.regex_replace(target, '[{}]+'.format(punctuation_chars), '')\n",
    "\tcontext = tf.strings.strip(\n",
    "\t\ttf.strings.regex_replace(context, '[ ]+', ' ')\n",
    "\t)\n",
    "\tcontext = tf.strings.lower(context)\n",
    "\treturn context, target\n",
    "\n",
    "dataset_raw = dataset_raw.map(add_context).filter(filter_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b9d94",
   "metadata": {},
   "source": [
    "Now let's inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3646c389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'illustration' b'Illustration '\n",
      "b'alices adventures in wonderland' b\"Alice's Adventures in Wonderland\"\n",
      "b'by lewis carroll' b'by Lewis Carroll'\n",
      "b'the millennium fulcrum edition 30' b'THE MILLENNIUM FULCRUM EDITION 3.0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 14:20:21.826408: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for text, label in dataset_raw.take(4).as_numpy_iterator():\n",
    "\tprint(text, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24075a",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197d8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100_000\n",
    "BATCH_SIZE = 16\n",
    "dataset_train = dataset_raw.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Inspired by https://stackoverflow.com/a/74609848.\n",
    "validate_size = 64\n",
    "dataset_validate = dataset_train.take(validate_size)\n",
    "dataset_train = dataset_train.skip(validate_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532db30",
   "metadata": {},
   "source": [
    "### Preparing to process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa159e6",
   "metadata": {},
   "source": [
    "The [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer takes a `standardize` option that preprocesses input data. The default removes punctuation, but we don't want that. Let's redefine it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f74b9fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"[START] [cap] this is a test ! [cap] it ' s working ? ! [END]\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def standardize_tf_text(text):\n",
    "\tpunctuation_regex = '[{p}]'.format(p = punctuation_chars)\n",
    "\n",
    "\t# Surround punctuation with spaces for easier tokenization\n",
    "\ttext = tf.strings.regex_replace(text, punctuation_regex, r' \\0 ')\n",
    "\n",
    "\t# Remove repeated spaces\n",
    "\ttext = tf.strings.regex_replace(text, r'\\s+', ' ')\n",
    "\n",
    "\t# Add a special \"capitalize the next letter\" token\n",
    "\ttext = tf.strings.regex_replace(text, r'(\\s|^)([A-Z])', r' [CAP] \\2')\n",
    "\n",
    "\t# Lowercase everything\n",
    "\ttext = tf.strings.lower(text)\n",
    "\n",
    "\ttext = tf.strings.strip(text)\n",
    "\n",
    "\t# Add sequence markings\n",
    "\treturn tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "\n",
    "print(standardize_tf_text('This is a test! It\\'s working?!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018456b9",
   "metadata": {},
   "source": [
    "The text standardization function can now be used to preprocess text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87815e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 14 target words: ['', '[UNK]', '[cap]', ',', 'the', '.', '[START]', '[END]', 'and', 'to', 'of', 'i', 'a', 'you']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 14:20:30.293864: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Keep only the 3000 most commonly used tokens\n",
    "max_vocab_size = 3000\n",
    "\n",
    "# context_text_processor = tf.keras.layers.TextVectorization(\n",
    "# \tstandardize=standardize_tf_text,\n",
    "# \tmax_tokens=max_vocab_size,\n",
    "# \t# Allow entries of different lengths\n",
    "# \tragged=True,\n",
    "# )\n",
    "# context_text_processor.adapt(dataset_train.map(lambda context, target: context))\n",
    "\n",
    "# print('First 14 context words:', context_text_processor.get_vocabulary()[:14])\n",
    "# print('Context vocab length', len(context_text_processor.get_vocabulary()))\n",
    "\n",
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "\tstandardize=standardize_tf_text,\n",
    "\tmax_tokens=max_vocab_size,\n",
    "\t# Allow entries of different lengths\n",
    "\tragged=True,\n",
    ")\n",
    "target_text_processor.adapt(dataset_train.map(lambda context, target: target))\n",
    "\n",
    "print('First 14 target words:', target_text_processor.get_vocabulary()[:14])\n",
    "\n",
    "# The target data should be roughly equivalent to the context data, except have additional (punctuation)\n",
    "# tokens.\n",
    "context_text_processor = target_text_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d66c3",
   "metadata": {},
   "source": [
    "We can use these layers to convert to/from token IDs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b4789f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokens tf.Tensor([  6   1 266  41  23  12   1   1  23   1  41   7], shape=(12,), dtype=int64)\n",
      "Back to text [START] [UNK] world this is a [UNK] [UNK] is [UNK] this [END]\n"
     ]
    }
   ],
   "source": [
    "example_text = 'hello world this is a test tensorflow is processing this'\n",
    "example_tokens = context_text_processor(example_text)\n",
    "print('Example tokens', example_tokens)\n",
    "\n",
    "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
    "tokens = context_vocab[example_tokens.numpy()]\n",
    "print('Back to text', ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d8e67",
   "metadata": {},
   "source": [
    "### Processing the data\n",
    "\n",
    "Now, we'll:\n",
    "1. Map the data through the text processors we just made.\n",
    "2. Shift the target data, so that our network is provided with a history of generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2efef357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "\treturn context_text_processor(context), target_text_processor(target)\n",
    "\n",
    "def add_target_history(context, target):\n",
    "\t# .to_tensor(): Converts from RaggedTensors to Tensors.\n",
    "\t# We give our network the history as target_in\n",
    "\ttarget_in = target[:, :-1].to_tensor()\n",
    "\ttarget_out = target[:, 1:].to_tensor()\n",
    "\treturn (context.to_tensor(), target_in), target_out\n",
    "dataset_train = dataset_train.map(process_text).map(add_target_history).repeat()\n",
    "dataset_validate = dataset_validate.map(process_text).map(add_target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50594897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context [START],excited,beyond,his,usual,calm,[UNK],franz,rose,with,the,[UNK],and,was,about,to,join,the,loud,[UNK],[UNK],that,followed,but,suddenly,his,purpose,was,arrested,his,hands,fell,by,his,sides,and,the,[UNK],[UNK],[UNK],on,his,lips,[END],,,,,,,,,,,,,,,,,,,,,,\n",
      "target_in [START],[cap],excited,beyond,his,usual,calm,[UNK],,,[cap],franz,rose,with,the,[UNK],,,and,was,about,to,join,the,loud,,,[UNK],[UNK],that,followed,but,suddenly,his,purpose,was,arrested,,,his,hands,fell,by,his,sides,,,and,the,half,-,uttered,[UNK],[UNK],on,his,lips,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "target_out [cap],excited,beyond,his,usual,calm,[UNK],,,[cap],franz,rose,with,the,[UNK],,,and,was,about,to,join,the,loud,,,[UNK],[UNK],that,followed,but,suddenly,his,purpose,was,arrested,,,his,hands,fell,by,his,sides,,,and,the,half,-,uttered,[UNK],[UNK],on,his,lips,.,[END],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def inspect_dataset(dataset: tf.data.Dataset):\n",
    "\ttarget_vocab = np.array(target_text_processor.get_vocabulary())\n",
    "\tfor (context, target_in), target_out in dataset.take(1):\n",
    "\t\tcontext_words = context_vocab[context[0]]\n",
    "\t\tprint('context', ','.join(context_words))\n",
    "\t\tprint('target_in', ','.join(target_vocab[target_in[0]]))\n",
    "\t\tprint('target_out', ','.join(target_vocab[target_out[0]]))\n",
    "\n",
    "inspect_dataset(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63bc6c",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### The encoder\n",
    "\n",
    "See https://www.tensorflow.org/text/tutorials/nmt_with_attention#the_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83f026f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Layer):\n",
    "\tdef __init__(self, text_processor, units: int):\n",
    "\t\t\"\"\"\n",
    "\t\tCreates a new Encoder layer. [dimen] is the maxiumum number of elements of the input\n",
    "\t\tthat can be processed by the encoder.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.text_processor = text_processor\n",
    "\t\tself.vocab_size = text_processor.vocabulary_size()\n",
    "\t\tself.units = units\n",
    "\n",
    "\t\t# Converts tokens -> vectors\n",
    "\t\tself.embedding = tf.keras.layers.Embedding(\n",
    "\t\t\t# mask_zero: Treats zero as a padding value that should be ignored\n",
    "\t\t\tself.vocab_size, units, mask_zero = True,\n",
    "\t\t)\n",
    "\t\tgru = tf.keras.layers.GRU(\n",
    "\t\t\tunits, return_sequences = True,\n",
    "\t\t\t# Use the recurrent_initializer suggested by the tutorial (& the default\n",
    "\t\t\t# for kernel_initializer).\n",
    "\t\t\trecurrent_initializer='glorot_uniform'\n",
    "\t\t)\n",
    "\t\tself.rnn = tf.keras.layers.Bidirectional(\n",
    "\t\t\t# merge_mode determines how the forward and backward layers are combined\n",
    "\t\t\t#            'concat' is another option here\n",
    "\t\t\tmerge_mode = 'sum',\n",
    "\t\t\tlayer=gru,\n",
    "\t\t)\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\tx = self.embedding(x)\n",
    "\t\tx = self.rnn(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef convert_input(self, texts):\n",
    "\t\ttexts = tf.convert_to_tensor(texts)\n",
    "\t\tif len(texts.shape) == 0:\n",
    "\t\t\ttexts = texts[None]\n",
    "\t\tcontext = self.text_processor(texts).to_tensor()\n",
    "\t\tcontext = self(context)\n",
    "\t\treturn context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea196ff",
   "metadata": {},
   "source": [
    "Try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331e474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens shape (batch, s): (16, 66)\n",
      "Encoder output shape (batch, s, ENCODER_UNITS): (16, 66, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 14:20:37.085797: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "ENCODER_UNITS = 64\n",
    "encoder = Encoder(context_text_processor, ENCODER_UNITS)\n",
    "\n",
    "for (context, target_history), target_next in dataset_validate.take(1):\n",
    "\tencoder_result = encoder(context)\n",
    "\tprint('Context tokens shape (batch, s):', context.shape)\n",
    "\tprint('Encoder output shape (batch, s, ENCODER_UNITS):', encoder_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd7fb6",
   "metadata": {},
   "source": [
    "### The attention layer\n",
    "\n",
    "Attention can be thought of as training a lookup table with keys and values. The lookup table has inputs `values` and `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "781b2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.Layer):\n",
    "\tdef __init__(self, units, **kwargs):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.attention_layer = tf.keras.layers.MultiHeadAttention(\n",
    "\t\t\tkey_dim=units,\n",
    "\t\t\tnum_heads=1,\n",
    "\t\t\t**kwargs\n",
    "\t\t)\n",
    "\t\t# Keeps \"the mean activation within each example close to 0 and the\n",
    "\t\t# activation standard deviation close to 1\" -- https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization?hl=en\n",
    "\t\tself.norm_layer = tf.keras.layers.LayerNormalization()\n",
    "\t\tself.add_layer = tf.keras.layers.Add()\n",
    "\t\tself.supports_masking = True\n",
    "\n",
    "\tdef call(self, query, value):\n",
    "\t\tattention_output = self.attention_layer(\n",
    "\t\t\tquery = query,\n",
    "\t\t\tvalue = value,\n",
    "\t\t\t#use_causal_mask=True,\n",
    "\t\t\t# Return the attention scores for latter plotting\n",
    "\t\t\t# return_attention_scores = True,\n",
    "\t\t)\n",
    "\n",
    "\t\tx = self.add_layer([ query, attention_output ])\n",
    "\t\tx = self.norm_layer(x)\n",
    "\t\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8857ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded context sequence shape (batch, s, units): (16, 58, 64)\n",
      "Target history sequence shape (batch, t, units): (16, 69, 64)\n",
      "Attention result shape (batch, t, units): (16, 69, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(ENCODER_UNITS)\n",
    "\n",
    "# Test with an example\n",
    "for (context, target_history), target_next in dataset_validate.take(1):\n",
    "\tembed_layer = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(), output_dim=ENCODER_UNITS, mask_zero=True)\n",
    "\ttarget_embed = embed_layer(target_history)\n",
    "\tencoded_context = encoder(context)\n",
    "\tattention_result = attention_layer(target_embed, encoded_context)\n",
    "\n",
    "\tprint('Encoded context sequence shape (batch, s, units):', encoded_context.shape)\n",
    "\tprint('Target history sequence shape (batch, t, units):', target_embed.shape)\n",
    "\tprint('Attention result shape (batch, t, units):', attention_result.shape)\n",
    "\n",
    "\t# Used later \n",
    "\ttest_encoded_context = encoded_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5577a1",
   "metadata": {},
   "source": [
    "### The decoder\n",
    "\n",
    "The decoder produces queries for the attention layer. The decoder operates on `target_history`. At each step during training, it should have no information about future target output (that's what we're trying to determine). As such, we use a unidirectional RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2454259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDense(tf.keras.layers.Dense):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tsuper(CustomDense, self).__init__(*args, **kwargs)\n",
    "\t\n",
    "\tdef compute_mask(self, _inputs, mask=None):\n",
    "\t\treturn mask\n",
    "\n",
    "class Decoder(tf.keras.Layer):\n",
    "\tdef __init__(self, text_processor, units):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.text_processor = text_processor\n",
    "\t\tself.vocab_size = text_processor.vocabulary_size()\n",
    "\t\tself.units = units\n",
    "\n",
    "\t\tself.embedding_layer = tf.keras.layers.Embedding(\n",
    "\t\t\t# mask_zero: Treats zero as a padding value that should be ignored\n",
    "\t\t\tself.vocab_size, units, mask_zero = True,\n",
    "\t\t)\n",
    "\t\tself.rnn_layer = tf.keras.layers.GRU(\n",
    "\t\t\tunits, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform',\n",
    "\t\t)\n",
    "\t\tself.attention_layer = CrossAttention(units)\n",
    "\n",
    "\t\t# Creates logits with the estimated probability of each output token\n",
    "\t\tself.output_layer = CustomDense(self.vocab_size)\n",
    "\n",
    "\t\t# Conversion:\n",
    "\t\tself.word_to_id = tf.keras.layers.StringLookup(\n",
    "\t\t\tvocabulary = text_processor.get_vocabulary(),\n",
    "\t\t\tmask_token = '',\n",
    "\t\t\toov_token = '[UNK]',\n",
    "\t\t)\n",
    "\t\tself.id_to_word = tf.keras.layers.StringLookup(\n",
    "\t\t\tvocabulary = text_processor.get_vocabulary(),\n",
    "\t\t\tmask_token = '',\n",
    "\t\t\toov_token = '[UNK]',\n",
    "\t\t\tinvert = True,\n",
    "\t\t)\n",
    "\t\t# Pre-computing these simplifies exporting\n",
    "\t\tself.start_id = self.word_to_id('[START]')\n",
    "\t\tself.end_id = self.word_to_id('[END]')\n",
    "\n",
    "\t\tself.supports_masking = True\n",
    "\t\n",
    "\tdef build(self, input_shape):\n",
    "\t\t# Nothing tha needs a size allocation based on the input shape\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef call(self, context, target_history, state = None, return_state = False):\n",
    "\t\tx = self.embedding_layer(target_history)\n",
    "\t\tx, state = self.rnn_layer(x, initial_state = state)\n",
    "\t\tx = self.attention_layer(x, context)\n",
    "\n",
    "\t\tlogits = self.output_layer(x)\n",
    "\t\tif return_state:\n",
    "\t\t\treturn logits, state\n",
    "\t\telse:\n",
    "\t\t\treturn logits\n",
    "\t\n",
    "\t## Conversion/testing ##\n",
    "\n",
    "\tdef tokens_to_text(self, tokens):\n",
    "\t\treturn tf.strings.reduce_join(self.id_to_word(tokens), separator = ' ')\n",
    "\n",
    "\tdef generate_next_token(self, context, target_history, done_vec, state, temperature = 0.0):\n",
    "\t\t# Note: is_done is a vector, indicating whether each item in the batch is done\n",
    "\n",
    "\t\tlogits, state = self(context, target_history, state = state, return_state = True)\n",
    "\n",
    "\t\t# logits has shape (batch, t, target_vocab_size). Only generate the token corresponding\n",
    "\t\t# to the last logits in the sequence (at t - 1)\n",
    "\t\tif temperature > 0:\n",
    "\t\t\tnext_token = tf.where(\n",
    "\t\t\t\tdone_vec,\n",
    "\t\t\t\ttf.constant(0, dtype=tf.int64), # Emit 0 after a sequence is done\n",
    "\t\t\t\ttf.random.categorical(logits[:, -1, :] / temperature, num_samples = 1), # Otherwise, pick the token from a categorical distribution\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tnext_token = tf.math.argmax(logits, axis=-1)\n",
    "\t\tdone_vec = done_vec|(next_token == self.end_id)\n",
    "\t\treturn next_token, done_vec, state\n",
    "\t\n",
    "\tdef get_initial_state(self, context):\n",
    "\t\t# context has shape (batch_size, s, units)\n",
    "\t\tbatch_size = tf.shape(context)[0]\n",
    "\t\tstart_tokens = tf.fill([batch_size, 1], self.start_id)\n",
    "\t\tdone_vec = tf.zeros([batch_size, 1], dtype = tf.bool)\n",
    "\n",
    "\t\t# From the Tensorflow source code:\n",
    "\t\t# > RNN expect the states in a list, even if single state.\n",
    "\t\t# Note: Without the [0] we get a type mismatch while exporting.\n",
    "\t\tinitial_state = self.rnn_layer.get_initial_state(batch_size)[0]\n",
    "\n",
    "\t\treturn start_tokens, done_vec, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d5ccb1",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "219a3522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: b'[START] lucien touched couldn god sunk gazed reward relief [START] lucien perfectly divine exercise john stock europe command [START] solitude tenderness finger spare see devil did bosom'\n"
     ]
    }
   ],
   "source": [
    "def test_generation_loop():\n",
    "\tdecoder = Decoder(target_text_processor, ENCODER_UNITS)\n",
    "\tnext_token, done_vec, state = decoder.get_initial_state(test_encoded_context[:3, :, :])\n",
    "\ttokens = [next_token]\n",
    "\n",
    "\tfor i in range(8):\n",
    "\t\tnext_token, done_vec, state = decoder.generate_next_token(test_encoded_context[:3, :, :], next_token, done_vec, state)\n",
    "\t\ttokens.append(next_token)\n",
    "\t\n",
    "\t# Merge all batch outputs into a single dimension\n",
    "\ttokens = tf.concat(tokens, -1) # -1 = last axis\n",
    "\n",
    "\tprint('Output:', decoder.tokens_to_text(tokens).numpy())\n",
    "\n",
    "test_generation_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3411a",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b3a40",
   "metadata": {},
   "source": [
    "We can now build a model for training and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1520bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Punctuator(tf.keras.Model):\n",
    "\tdef __init__(self, units, context_text_processor, target_text_processor):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encoder = Encoder(context_text_processor, units)\n",
    "\t\tself.decoder = Decoder(target_text_processor, units)\n",
    "\t\n",
    "\tdef call(self, inputs):\n",
    "\t\tcontext, target_history = inputs\n",
    "\t\tcontext = self.encoder(context)\n",
    "\t\tlogits = self.decoder(context, target_history)\n",
    "\t\treturn logits\n",
    "\t\n",
    "\tdef fix_punctuation(self, text):\n",
    "\t\tcontext = self.encoder.convert_input(text)\n",
    "\n",
    "\t\tnext_token, done_vec, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "\t\t# A TensorArray allows more efficient exporting\n",
    "\t\ttokens = tf.TensorArray(tf.int64, size=0, dynamic_size=True)\n",
    "\t\ttokens = tokens.write(0, next_token)\n",
    "\n",
    "\t\t# Use a tf.range to allow dynamic loop size while exporting\n",
    "\t\tfor i in tf.range(60):\n",
    "\t\t\t# token_history has size: (batch, t, target_vocab_size)\n",
    "\t\t\t# token_history = tf.concat(tokens, 1)\n",
    "\t\t\t# print('history', model.decoder.id_to_word(token_history))\n",
    "\t\t\tnext_token, done_vec, state = self.decoder.generate_next_token(context, next_token, done_vec, state, temperature=0)\n",
    "\t\t\ttokens = tokens.write(i + 1, next_token)\n",
    "\n",
    "\t\t\t# executing_eagerly() is false if tracing execution (e.g. in a tf.function being prepared for\n",
    "\t\t\t# export)\n",
    "\t\t\tif tf.reduce_all(done_vec):\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\ttokens = tokens.stack()\n",
    "\t\tbatch_size = tf.shape(context)[0]\n",
    "\t\ttokens = tf.reshape(tokens, [batch_size, -1])\n",
    "\t\treturn self.decoder.tokens_to_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "253c670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens shape (batch, s): (16, 63)\n",
      "Target history tokens shape (batch, t): (16, 94)\n",
      "Logits shape (batch, t, vocab_size) (16, 94, 3000)\n"
     ]
    }
   ],
   "source": [
    "model = Punctuator(ENCODER_UNITS, context_text_processor, target_text_processor)\n",
    "\n",
    "for (example_context_tok, example_target_hist), _ in dataset_validate.take(1):\n",
    "\ttest_logits = model((example_context_tok, example_target_hist))\n",
    "\tprint('Context tokens shape (batch, s):', example_context_tok.shape)\n",
    "\tprint('Target history tokens shape (batch, t):', example_target_hist.shape)\n",
    "\tprint('Logits shape (batch, t, vocab_size)', test_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68e81b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"punctuator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"punctuator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">241,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">428,728</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │       \u001b[38;5;34m241,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │       \u001b[38;5;34m428,728\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">670,648</span> (2.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m670,648\u001b[0m (2.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">670,648</span> (2.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m670,648\u001b[0m (2.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85551c17",
   "metadata": {},
   "source": [
    "To avoid penalizing masked outputs, we use a custom loss function (see the tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ca4d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss(y_true, y_predict):\n",
    "\tloss = base_loss_fn(y_true, y_predict)\n",
    "\t\n",
    "\tunmasked = y_true != 0\n",
    "\tunmasked = tf.cast(unmasked, loss.dtype)\n",
    "\t# Only consider output with a corresponding label.\n",
    "\tloss *= unmasked\n",
    "\n",
    "\tcount_unmasked = tf.math.reduce_sum(unmasked)\n",
    "\n",
    "\t# reduce_sum: Adds all entries of a vector.\n",
    "\treturn tf.math.reduce_sum(loss)/count_unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ba83b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, predict_logits):\n",
    "\tpredicted_index = tf.math.argmax(predict_logits, axis=-1)\n",
    "\tpredicted_index = tf.cast(predicted_index, y_true.dtype)\n",
    "\n",
    "\tmatch = tf.cast(y_true == predicted_index, tf.float32)\n",
    "\tunmasked = tf.cast(y_true != 0, tf.float32)\n",
    "\tcount_unmasked = tf.math.reduce_sum(unmasked)\n",
    "\n",
    "\treturn tf.math.reduce_sum(match * unmasked) / count_unmasked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d9fde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[masked_accuracy, masked_loss])\n",
    "model.compile(optimizer='adam', loss=masked_loss, metrics=[masked_accuracy, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95ae7f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the tutorial:\n",
      "expected loss 8.006368\n",
      "expected accuracy 0.0003333333333333333\n"
     ]
    }
   ],
   "source": [
    "print('From the tutorial:')\n",
    "vocab_size = float(target_text_processor.vocabulary_size())\n",
    "\n",
    "print('expected loss', tf.math.log(vocab_size).numpy())\n",
    "print('expected accuracy', 1/vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "091b83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 8.0115 - masked_accuracy: 4.4516e-04 - masked_loss: 8.0115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 8.012113571166992,\n",
       " 'masked_accuracy': 0.0003151628188788891,\n",
       " 'masked_loss': 8.012113571166992}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset_validate, steps=20, return_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a179e6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[test]: [START] usual william tail long date take mice date take political barrymore common supposed understood past why obey ease after task awakened china perfect giant dearest faria real informed dreadful lower feast accomplished advantage real sound was date them lock ceremony insisted am revenge merchant taught pick left villefort laugh resumed surprise ask peculiar madness off stayed unfortunate tears rays uncle'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def test_punctuation(text):\n",
    "\treturn '[test]: ' + model.fix_punctuation(text).numpy().decode('utf-8')\n",
    "\n",
    "class DemoCallback(tf.keras.callbacks.Callback):\n",
    "\tdef on_epoch_end(self, epoch_index: int, logs = None):\n",
    "\t\tprint('\\r', test_punctuation([ 'test this is a sample will it work if it does then how well' ]))\n",
    "\t\tif epoch_index % 10 == 0:\n",
    "\t\t\t# From the test data\n",
    "\t\t\tprint(test_punctuation([\n",
    "\t\t\t\t'not that alice had any idea of doing that she felt as if she would never be able to talk again, she was getting so much out of breath and still the queen cried faster faster and dragged her along'\n",
    "\t\t\t]))\n",
    "\t\t\tprint(test_punctuation([ 'tensorflow is a library that is used for machine learning it is available for more languages than just python' ]))\n",
    "\t\t\tprint(test_punctuation([ 'the joplin note taking app can be used to take multimedia notes' ]))\n",
    "\t\t\tprint(test_punctuation([ 'here are a few words javascript typescript python joplin interesting loud and sequence these words are all very useful' ]))\n",
    "\n",
    "test_punctuation(tf.constant([ 'this is an example they said' ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2db06e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 5.8100 - masked_accuracy: 0.1244 - masked_loss: 5.8100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [test]: [START] [cap] i have not not be [UNK] , and [cap] i have not not be [UNK] , and [cap] i have not not be [UNK] , and [cap] i have not not be [UNK] , and [cap] i have not not be [UNK] , and [cap] i have not not be [UNK] , and [cap] i have not not be\n",
      "[test]: [START] [cap] i was a [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of the [UNK] of\n",
      "[test]: [START] [cap] i have not [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and\n",
      "[test]: [START] [cap] i have not [UNK] , and [UNK] , and [cap] i have not [UNK] , and [UNK] , and [cap] i have not [UNK] , and [UNK] , and [cap] i have not [UNK] , and [UNK] , and [cap] i have not [UNK] , and [UNK] , and [cap] i have not [UNK] , and [UNK] , and\n",
      "[test]: [START] [cap] i was [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK] [UNK] , and [UNK]\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 123ms/step - loss: 5.8076 - masked_accuracy: 0.1245 - masked_loss: 5.8076 - val_loss: 4.3229 - val_masked_accuracy: 0.2425 - val_masked_loss: 4.2564\n",
      "Epoch 2/40\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 4.2348 - masked_accuracy: 0.2589 - masked_loss: 4.2348"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 14:22:06.185728: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [test]: [START] [cap] [UNK] , [cap] [UNK] , [cap] [UNK] , [cap] i have not a [UNK] ? [END]\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 108ms/step - loss: 4.2345 - masked_accuracy: 0.2590 - masked_loss: 4.2345 - val_loss: 3.9079 - val_masked_accuracy: 0.2953 - val_masked_loss: 3.8478\n",
      "Epoch 3/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will be it , if it is a good [UNK] . [END]203 - masked_accuracy: 0.3356 - masked_loss: 3.7203\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 109ms/step - loss: 3.7194 - masked_accuracy: 0.3357 - masked_loss: 3.7194 - val_loss: 2.8522 - val_masked_accuracy: 0.4811 - val_masked_loss: 2.8083\n",
      "Epoch 4/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it , if if it does , then , said [cap] well . [END]ccuracy: 0.5242 - masked_loss: 2.6333\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 111ms/step - loss: 2.6325 - masked_accuracy: 0.5243 - masked_loss: 2.6325 - val_loss: 1.8455 - val_masked_accuracy: 0.6403 - val_masked_loss: 1.8171\n",
      "Epoch 5/40\n",
      " [test]: [START] [cap] [UNK] this is , a [UNK] will it work if it does , then , but well . [END]ked_accuracy: 0.6592 - masked_loss: 1.7613\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 108ms/step - loss: 1.7608 - masked_accuracy: 0.6593 - masked_loss: 1.7608 - val_loss: 1.3487 - val_masked_accuracy: 0.7147 - val_masked_loss: 1.3279\n",
      "Epoch 6/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work if it does then how well . [END] - masked_accuracy: 0.7309 - masked_loss: 1.3336\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 108ms/step - loss: 1.3331 - masked_accuracy: 0.7310 - masked_loss: 1.3331 - val_loss: 0.9071 - val_masked_accuracy: 0.7923 - val_masked_loss: 0.8931\n",
      "Epoch 7/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work if it does then how well . [END] - masked_accuracy: 0.8067 - masked_loss: 0.9008\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 109ms/step - loss: 0.9006 - masked_accuracy: 0.8067 - masked_loss: 0.9006 - val_loss: 0.7192 - val_masked_accuracy: 0.8255 - val_masked_loss: 0.7082\n",
      "Epoch 8/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work if it does then how well . [END] - masked_accuracy: 0.8321 - masked_loss: 0.7406\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 110ms/step - loss: 0.7405 - masked_accuracy: 0.8321 - masked_loss: 0.7405 - val_loss: 0.6022 - val_masked_accuracy: 0.8459 - val_masked_loss: 0.5929\n",
      "Epoch 9/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work if it does then how well . [END] - masked_accuracy: 0.8553 - masked_loss: 0.6047\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 114ms/step - loss: 0.6045 - masked_accuracy: 0.8553 - masked_loss: 0.6046 - val_loss: 0.5259 - val_masked_accuracy: 0.8569 - val_masked_loss: 0.5178\n",
      "Epoch 10/40\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.6174 - masked_accuracy: 0.8517 - masked_loss: 0.6174"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 14:26:29.963612: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work , if it does then how well ? [END]\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 110ms/step - loss: 0.6174 - masked_accuracy: 0.8517 - masked_loss: 0.6174 - val_loss: 0.5377 - val_masked_accuracy: 0.8536 - val_masked_loss: 0.5294\n",
      "Epoch 11/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work if it does then , how well . [END] masked_accuracy: 0.8612 - masked_loss: 0.5448\n",
      "[test]: [START] [cap] not that [cap] alice had any idea of doing that she felt as if she would never be able to talk again , she was getting so much out of breath , and still the queen , cried [cap] [UNK] , [UNK] , and dragged her along . [END]\n",
      "[test]: [START] [cap] [UNK] is a library , that is used for [UNK] learning it is [UNK] for more [UNK] than just [UNK] . [END]\n",
      "[test]: [START] [cap] the [cap] [UNK] [cap] douglass , taking [cap] [UNK] , can be used to take [UNK] notes . [END]\n",
      "[test]: [START] [cap] here are a few words [cap] [UNK] [cap] [UNK] [cap] [UNK] [cap] [UNK] [cap] mr , loud , and [UNK] these words are all very useful . [END]\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 117ms/step - loss: 0.5448 - masked_accuracy: 0.8612 - masked_loss: 0.5448 - val_loss: 0.5039 - val_masked_accuracy: 0.8543 - val_masked_loss: 0.4962\n",
      "Epoch 12/40\n",
      " [test]: [START] [cap] [UNK] , this is a [UNK] will it work if it does then how well . [END] masked_accuracy: 0.8615 - masked_loss: 0.5484\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 104ms/step - loss: 0.5484 - masked_accuracy: 0.8616 - masked_loss: 0.5484 - val_loss: 0.4667 - val_masked_accuracy: 0.8632 - val_masked_loss: 0.4596\n",
      "Epoch 13/40\n",
      " [test]: [START] [cap] [UNK] , this is a [UNK] will it work if it does then , how well . [END]asked_accuracy: 0.8657 - masked_loss: 0.5263\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 107ms/step - loss: 0.5262 - masked_accuracy: 0.8657 - masked_loss: 0.5262 - val_loss: 0.4496 - val_masked_accuracy: 0.8653 - val_masked_loss: 0.4427\n",
      "Epoch 14/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work if it does then how well ? [END] - masked_accuracy: 0.8673 - masked_loss: 0.5127\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 104ms/step - loss: 0.5127 - masked_accuracy: 0.8673 - masked_loss: 0.5127 - val_loss: 0.4580 - val_masked_accuracy: 0.8628 - val_masked_loss: 0.4509\n",
      "Epoch 15/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work if it does then ? [cap] how well . [END]d_accuracy: 0.8725 - masked_loss: 0.4799\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 108ms/step - loss: 0.4799 - masked_accuracy: 0.8725 - masked_loss: 0.4799 - val_loss: 0.5015 - val_masked_accuracy: 0.8578 - val_masked_loss: 0.4938\n",
      "Epoch 16/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work if it does then how well ? [END] - masked_accuracy: 0.8688 - masked_loss: 0.4954\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 104ms/step - loss: 0.4955 - masked_accuracy: 0.8688 - masked_loss: 0.4955 - val_loss: 0.4880 - val_masked_accuracy: 0.8589 - val_masked_loss: 0.4805\n",
      "Epoch 17/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work if it does then how well . [END] - masked_accuracy: 0.8741 - masked_loss: 0.4743\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 116ms/step - loss: 0.4742 - masked_accuracy: 0.8741 - masked_loss: 0.4743 - val_loss: 0.6173 - val_masked_accuracy: 0.8325 - val_masked_loss: 0.6078\n",
      "Epoch 18/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work , if it does then ? [cap] how well ? [END]accuracy: 0.8707 - masked_loss: 0.4911\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 105ms/step - loss: 0.4910 - masked_accuracy: 0.8707 - masked_loss: 0.4910 - val_loss: 0.4325 - val_masked_accuracy: 0.8668 - val_masked_loss: 0.4259\n",
      "Epoch 19/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does then ? [cap] she then ? [cap] how well ? [END]ed_loss: 0.4367\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - loss: 0.4367 - masked_accuracy: 0.8793 - masked_loss: 0.4367 - val_loss: 0.4558 - val_masked_accuracy: 0.8609 - val_masked_loss: 0.4488\n",
      "Epoch 20/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does then how well ? [END]d_accuracy: 0.8775 - masked_loss: 0.4484\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 105ms/step - loss: 0.4484 - masked_accuracy: 0.8775 - masked_loss: 0.4484 - val_loss: 0.4036 - val_masked_accuracy: 0.8725 - val_masked_loss: 0.3974\n",
      "Epoch 21/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does . [cap] then , how well ? [END]: 0.8815 - masked_loss: 0.4341\n",
      "[test]: [START] [cap] not that [cap] alice had any idea of doing that she felt as if she would never be able to talk again , she was getting so much out of breath and still , the [cap] queen cried [cap] [UNK] [cap] [UNK] , and dragged her along along . [END]\n",
      "[test]: [START] [cap] [UNK] is a library , that is used for [UNK] learning it is [UNK] for more [UNK] than just [UNK] . [END]\n",
      "[test]: [START] [cap] the [cap] [UNK] note , taking [UNK] can be used to take [UNK] notes . [END]\n",
      "[test]: [START] [cap] here are a few words [UNK] [UNK] [UNK] [UNK] interesting loud and [UNK] these words are all very useful . [END]\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 112ms/step - loss: 0.4340 - masked_accuracy: 0.8815 - masked_loss: 0.4340 - val_loss: 0.4016 - val_masked_accuracy: 0.8736 - val_masked_loss: 0.3954\n",
      "Epoch 22/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does then ? [cap] how well ? [END]cy: 0.8810 - masked_loss: 0.4304\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 105ms/step - loss: 0.4304 - masked_accuracy: 0.8810 - masked_loss: 0.4304 - val_loss: 0.4249 - val_masked_accuracy: 0.8672 - val_masked_loss: 0.4184\n",
      "Epoch 23/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does then how well ? [END]d_accuracy: 0.8579 - masked_loss: 0.5868\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - loss: 0.5865 - masked_accuracy: 0.8580 - masked_loss: 0.5865 - val_loss: 0.3882 - val_masked_accuracy: 0.8752 - val_masked_loss: 0.3822\n",
      "Epoch 24/40\n",
      " [test]: [START] [cap] [UNK] ' s this is a [UNK] will it work if it does then how well ? [END]asked_accuracy: 0.8838 - masked_loss: 0.4138\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 105ms/step - loss: 0.4138 - masked_accuracy: 0.8838 - masked_loss: 0.4138 - val_loss: 0.3827 - val_masked_accuracy: 0.8781 - val_masked_loss: 0.3768\n",
      "Epoch 25/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it ' s work if it does then how well ? [END]asked_accuracy: 0.8865 - masked_loss: 0.4053\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 112ms/step - loss: 0.4053 - masked_accuracy: 0.8865 - masked_loss: 0.4053 - val_loss: 0.3830 - val_masked_accuracy: 0.8769 - val_masked_loss: 0.3771\n",
      "Epoch 26/40\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.4255 - masked_accuracy: 0.8822 - masked_loss: 0.4255"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 14:35:06.667711: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does then how well ? [END]\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - loss: 0.4256 - masked_accuracy: 0.8821 - masked_loss: 0.4256 - val_loss: 0.4554 - val_masked_accuracy: 0.8626 - val_masked_loss: 0.4484\n",
      "Epoch 27/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does then how well ? [END]d_accuracy: 0.8870 - masked_loss: 0.4050\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 107ms/step - loss: 0.4050 - masked_accuracy: 0.8870 - masked_loss: 0.4050 - val_loss: 0.3608 - val_masked_accuracy: 0.8834 - val_masked_loss: 0.3552\n",
      "Epoch 28/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does then how well ? [END]d_accuracy: 0.8863 - masked_loss: 0.4047\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 110ms/step - loss: 0.4048 - masked_accuracy: 0.8863 - masked_loss: 0.4048 - val_loss: 0.3790 - val_masked_accuracy: 0.8772 - val_masked_loss: 0.3732\n",
      "Epoch 29/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does then how well ? [END]d_accuracy: 0.8889 - masked_loss: 0.3964\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 105ms/step - loss: 0.3964 - masked_accuracy: 0.8889 - masked_loss: 0.3964 - val_loss: 0.3778 - val_masked_accuracy: 0.8778 - val_masked_loss: 0.3720\n",
      "Epoch 30/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work if it does then ? [cap] how well ? [END]d_accuracy: 0.8912 - masked_loss: 0.3831\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - loss: 0.3831 - masked_accuracy: 0.8911 - masked_loss: 0.3831 - val_loss: 0.3708 - val_masked_accuracy: 0.8810 - val_masked_loss: 0.3651\n",
      "Epoch 31/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does . [cap] then how well ? [END]cy: 0.8895 - masked_loss: 0.3859\n",
      "[test]: [START] [cap] not that [cap] alice had any idea of doing that she felt as if she would never be able to talk again . [cap] i was getting so much out of breath , and still , the queen cried , [cap] i ' [UNK] [UNK] , and dragged her along . [END]\n",
      "[test]: [START] [cap] [UNK] is a library , that is used for [UNK] learning it is [UNK] for more [UNK] than just [UNK] . [END]\n",
      "[test]: [START] [cap] the [cap] [UNK] [cap] note [cap] taking [cap] [UNK] . [cap] can be used to take [UNK] notes . [END]\n",
      "[test]: [START] [cap] here are a few words [cap] [UNK] [cap] [UNK] [cap] [UNK] [cap] [UNK] [cap] linton , loud loud and [UNK] these words are all very useful . [END]\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 115ms/step - loss: 0.3859 - masked_accuracy: 0.8895 - masked_loss: 0.3859 - val_loss: 0.3599 - val_masked_accuracy: 0.8815 - val_masked_loss: 0.3544\n",
      "Epoch 32/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work if it does then how well ? [END] - masked_accuracy: 0.8830 - masked_loss: 0.4379\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 107ms/step - loss: 0.4378 - masked_accuracy: 0.8830 - masked_loss: 0.4378 - val_loss: 0.3675 - val_masked_accuracy: 0.8808 - val_masked_loss: 0.3618\n",
      "Epoch 33/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does then how well ? [END]d_accuracy: 0.8911 - masked_loss: 0.3833\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 114ms/step - loss: 0.3833 - masked_accuracy: 0.8911 - masked_loss: 0.3833 - val_loss: 0.3760 - val_masked_accuracy: 0.8784 - val_masked_loss: 0.3702\n",
      "Epoch 34/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does then how well ? [END]d_accuracy: 0.8928 - masked_loss: 0.3753\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - loss: 0.3753 - masked_accuracy: 0.8928 - masked_loss: 0.3753 - val_loss: 0.3525 - val_masked_accuracy: 0.8830 - val_masked_loss: 0.3470\n",
      "Epoch 35/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does . [cap] then , how well ? [END]: 0.8918 - masked_loss: 0.3957\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 104ms/step - loss: 0.3957 - masked_accuracy: 0.8918 - masked_loss: 0.3957 - val_loss: 0.3599 - val_masked_accuracy: 0.8840 - val_masked_loss: 0.3544\n",
      "Epoch 36/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does . [cap] then how well ? [END]cy: 0.8926 - masked_loss: 0.3773\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 105ms/step - loss: 0.3773 - masked_accuracy: 0.8926 - masked_loss: 0.3773 - val_loss: 0.3884 - val_masked_accuracy: 0.8776 - val_masked_loss: 0.3824\n",
      "Epoch 37/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does , then , how well ? [END]curacy: 0.8908 - masked_loss: 0.3792\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 104ms/step - loss: 0.3792 - masked_accuracy: 0.8908 - masked_loss: 0.3792 - val_loss: 0.3546 - val_masked_accuracy: 0.8829 - val_masked_loss: 0.3492\n",
      "Epoch 38/40\n",
      " [test]: [START] [cap] [UNK] this is a [UNK] will it work , if it does . [cap] then , how well ? [END]curacy: 0.8923 - masked_loss: 0.3802\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 107ms/step - loss: 0.3802 - masked_accuracy: 0.8923 - masked_loss: 0.3802 - val_loss: 0.3732 - val_masked_accuracy: 0.8779 - val_masked_loss: 0.3675\n",
      "Epoch 39/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] , will it work if it does . [cap] then how well ? [END]: 0.8931 - masked_loss: 0.3719\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - loss: 0.3719 - masked_accuracy: 0.8931 - masked_loss: 0.3719 - val_loss: 0.3448 - val_masked_accuracy: 0.8849 - val_masked_loss: 0.3395\n",
      "Epoch 40/40\n",
      " [test]: [START] [cap] [UNK] . [cap] this is a [UNK] will it work if it does then how well ? [END]d_accuracy: 0.8924 - masked_loss: 0.3787\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 104ms/step - loss: 0.3787 - masked_accuracy: 0.8924 - masked_loss: 0.3787 - val_loss: 0.3394 - val_masked_accuracy: 0.8862 - val_masked_loss: 0.3341\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "\tdataset_train,\n",
    "\tepochs = 40,\n",
    "\tsteps_per_epoch = 300,\n",
    "\tvalidation_data = dataset_validate,\n",
    "\tcallbacks=[DemoCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f68a9fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test]: [START] [cap] not that [cap] alice had any idea of doing that she felt as if she would never be able to talk again . [cap] she was getting so much out of breath , and still the queen cried , [cap] [UNK] [UNK] and dragged her along . [END]\n",
      "[test]: [START] [cap] this is a [UNK] of a [UNK] system , for [cap] i am curious how well it works . [END]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_punctuation([\n",
    "\t'not that alice had any idea of doing that she felt as if she would never be able to talk again she was getting so much out of breath and still the queen cried faster faster and dragged her along'\n",
    "]))\n",
    "print(test_punctuation([ 'this is a test of a the punctuation system for i am curious how well it works' ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e9cc9",
   "metadata": {},
   "source": [
    "## Exporting\n",
    "\n",
    "Based on the [Export](https://www.tensorflow.org/text/tutorials/nmt_with_attention#export) section of the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9885c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Export(tf.Module):\n",
    "\tdef __init__(self, model):\n",
    "\t\tself.model = model\n",
    "\t\n",
    "\t@tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "\tdef fix_punctuation(self, inputs):\n",
    "\t\treturn model.fix_punctuation(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcac14",
   "metadata": {},
   "source": [
    "Run `fix_punctuation` once to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa037d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = Export(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8af27599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1725572559.330611   25103 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'[START] [cap] this sentence shall be [UNK] , for the following reasons first [UNK] makes things [UNK] to read second [UNK] . [END]'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_inputs = tf.constant([ 'this sentence shall be punctuated for the following reasons first punctatuion makes things easier to read second um' ])\n",
    "export.fix_punctuation(sample_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70247292",
   "metadata": {},
   "source": [
    "Now we save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "badb8d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: punctuator-seq2seq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: punctuator-seq2seq/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(export, 'punctuator-seq2seq', signatures={ 'serving_default': export.fix_punctuation })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197976b4",
   "metadata": {},
   "source": [
    "See [the documentation](https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export) for information about the `signatures` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c39fbc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported and warmed up!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1725572963.812083   25103 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    }
   ],
   "source": [
    "reloaded = tf.saved_model.load('punctuator-seq2seq')\n",
    "# Warmup\n",
    "reloaded.fix_punctuation(tf.constant(['this is a test is it not']))\n",
    "print('Imported and warmed up!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca974c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.1 ms, sys: 8.51 ms, total: 25.6 ms\n",
      "Wall time: 10.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'[START] [cap] this sentence should end with a full stop ? [END]'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "reloaded.fix_punctuation(tf.constant(['this sentence should end with a full stop']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2771e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
