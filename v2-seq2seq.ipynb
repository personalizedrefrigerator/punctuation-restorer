{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55e2298",
   "metadata": {},
   "source": [
    "# V2: SEQ2SEQ\n",
    "\n",
    "This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention#create_a_tfdata_dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71899cc9-86b4-4eda-b525-d818661c70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 16:47:54.329279: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-05 16:47:54.332587: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-05 16:47:54.341908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-05 16:47:54.356937: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-05 16:47:54.361081: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-05 16:47:54.372956: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-05 16:47:55.220823: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa459db2-5a48-4bac-8cbd-38496888faee",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention) (and [at least one other](https://www.tensorflow.org/text/tutorials/text_generation) of the Tensorflow tutorials).\n",
    "- This [blog post](https://janakiev.com/blog/jupyter-virtual-envs/) was referenced to set up the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b150ba-48c0-4f33-8d08-b9f363d6fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Tuple\n",
    "from IPython.display import display, Markdown\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5faca0eb-c672-4d1c-884f-b5ba38a22b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_paths = list(Path('data/processed/en/').glob('*.txt'))\n",
    "dataset_raw = tf.data.TextLineDataset(\n",
    "\tdata_file_paths,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890fa88",
   "metadata": {},
   "source": [
    "We've now loaded the `.txt` training data files using [`tf.data.TextLineDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset). Each line in the source files is mapped to a new training example. \n",
    "\n",
    "Although some preprocessing has been done by `/data/process_data.py`, paragraphs aren't filtered out based on length/content. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5c607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_paragraphs(context, target):\n",
    "\treturn tf.strings.length(context) > 5\n",
    "\n",
    "punctuation_chars = r'\\?!.,\"\\-\\':'\n",
    "def add_context(target):\n",
    "\tcontext = tf.strings.regex_replace(target, '[{}]+'.format(punctuation_chars), '')\n",
    "\tcontext = tf.strings.strip(\n",
    "\t\ttf.strings.regex_replace(context, '[ ]+', ' ')\n",
    "\t)\n",
    "\tcontext = tf.strings.lower(context)\n",
    "\treturn context, target\n",
    "\n",
    "dataset_raw = dataset_raw.map(add_context).filter(filter_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b9d94",
   "metadata": {},
   "source": [
    "Now let's inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3646c389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'illustration' b'Illustration '\n",
      "b'alices adventures in wonderland' b\"Alice's Adventures in Wonderland\"\n",
      "b'by lewis carroll' b'by Lewis Carroll'\n",
      "b'the millennium fulcrum edition 30' b'THE MILLENNIUM FULCRUM EDITION 3.0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 16:47:55.892466: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for text, label in dataset_raw.take(4).as_numpy_iterator():\n",
    "\tprint(text, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24075a",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197d8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100_000\n",
    "BATCH_SIZE = 16\n",
    "dataset_train = dataset_raw.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Inspired by https://stackoverflow.com/a/74609848.\n",
    "validate_size = 64\n",
    "dataset_validate = dataset_train.take(validate_size)\n",
    "dataset_train = dataset_train.skip(validate_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532db30",
   "metadata": {},
   "source": [
    "### Preparing to process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa159e6",
   "metadata": {},
   "source": [
    "The [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer takes a `standardize` option that preprocesses input data. The default removes punctuation, but we don't want that. Let's redefine it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f74b9fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"[START] [cap] this is a test ! [cap] it ' s working ? ! [END]\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def standardize_tf_text(text):\n",
    "\tpunctuation_regex = '[{p}]'.format(p = punctuation_chars)\n",
    "\n",
    "\t# Surround punctuation with spaces for easier tokenization\n",
    "\ttext = tf.strings.regex_replace(text, punctuation_regex, r' \\0 ')\n",
    "\n",
    "\t# Remove repeated spaces\n",
    "\ttext = tf.strings.regex_replace(text, r'\\s+', ' ')\n",
    "\n",
    "\t# Add a special \"capitalize the next letter\" token\n",
    "\ttext = tf.strings.regex_replace(text, r'(\\s|^)([A-Z])', r' [CAP] \\2')\n",
    "\n",
    "\t# Lowercase everything\n",
    "\ttext = tf.strings.lower(text)\n",
    "\n",
    "\t# Remove leading and trailing spaces (avoid .strip -- not supported in ONNX)\n",
    "\ttext = tf.strings.regex_replace(text, r'^\\s+', r'')\n",
    "\ttext = tf.strings.regex_replace(text, r'\\s+$', r'')\n",
    "\n",
    "\t# Add sequence markings\n",
    "\treturn tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "\n",
    "print(standardize_tf_text('This is a test! It\\'s working?!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018456b9",
   "metadata": {},
   "source": [
    "The text standardization function can now be used to preprocess text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87815e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 14 target words: ['', '[UNK]', '[cap]', ',', 'the', '.', '[START]', '[END]', 'and', 'to', 'of', 'i', 'a', 'in']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 16:48:06.200979: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Keep only the 2000 most commonly used tokens\n",
    "max_vocab_size = 2000\n",
    "\n",
    "# context_text_processor = tf.keras.layers.TextVectorization(\n",
    "# \tstandardize=standardize_tf_text,\n",
    "# \tmax_tokens=max_vocab_size,\n",
    "# \t# Allow entries of different lengths\n",
    "# \tragged=True,\n",
    "# )\n",
    "# context_text_processor.adapt(dataset_train.map(lambda context, target: context))\n",
    "\n",
    "# print('First 14 context words:', context_text_processor.get_vocabulary()[:14])\n",
    "# print('Context vocab length', len(context_text_processor.get_vocabulary()))\n",
    "\n",
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "\tstandardize=standardize_tf_text,\n",
    "\tmax_tokens=max_vocab_size,\n",
    "\t# Allow entries of different lengths\n",
    "\tragged=True,\n",
    ")\n",
    "target_text_processor.adapt(dataset_train.map(lambda context, target: target))\n",
    "\n",
    "print('First 14 target words:', target_text_processor.get_vocabulary()[:14])\n",
    "\n",
    "# The target data should be roughly equivalent to the context data, except have additional (punctuation)\n",
    "# tokens.\n",
    "context_text_processor = target_text_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d66c3",
   "metadata": {},
   "source": [
    "We can use these layers to convert to/from token IDs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b4789f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokens tf.Tensor([  6   1 261  42  24  12   1   1  24   1  42   7], shape=(12,), dtype=int64)\n",
      "Back to text [START] [UNK] world this is a [UNK] [UNK] is [UNK] this [END]\n"
     ]
    }
   ],
   "source": [
    "example_text = 'hello world this is a test tensorflow is processing this'\n",
    "example_tokens = context_text_processor(example_text)\n",
    "print('Example tokens', example_tokens)\n",
    "\n",
    "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
    "tokens = context_vocab[example_tokens.numpy()]\n",
    "print('Back to text', ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d8e67",
   "metadata": {},
   "source": [
    "### Processing the data\n",
    "\n",
    "Now, we'll:\n",
    "1. Map the data through the text processors we just made.\n",
    "2. Shift the target data, so that our network is provided with a history of generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2efef357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "\treturn context_text_processor(context), target_text_processor(target)\n",
    "\n",
    "def add_target_history(context, target):\n",
    "\t# .to_tensor(): Converts from RaggedTensors to Tensors.\n",
    "\t# We give our network the history as target_in\n",
    "\ttarget_in = target[:, :-1].to_tensor()\n",
    "\ttarget_out = target[:, 1:].to_tensor()\n",
    "\treturn (context.to_tensor(), target_in), target_out\n",
    "dataset_train = dataset_train.map(process_text).map(add_target_history).repeat()\n",
    "dataset_validate = dataset_validate.map(process_text).map(add_target_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50594897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context [START],at,the,house,of,m,de,morcerf,[END],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "target_in [START],[cap],at,the,house,of,[cap],m,.,de,[cap],morcerf,.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "target_out [cap],at,the,house,of,[cap],m,.,de,[cap],morcerf,.,[END],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def inspect_dataset(dataset: tf.data.Dataset):\n",
    "\ttarget_vocab = np.array(target_text_processor.get_vocabulary())\n",
    "\tfor (context, target_in), target_out in dataset.take(1):\n",
    "\t\tcontext_words = context_vocab[context[0]]\n",
    "\t\tprint('context', ','.join(context_words))\n",
    "\t\tprint('target_in', ','.join(target_vocab[target_in[0]]))\n",
    "\t\tprint('target_out', ','.join(target_vocab[target_out[0]]))\n",
    "\n",
    "inspect_dataset(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63bc6c",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### The encoder\n",
    "\n",
    "See https://www.tensorflow.org/text/tutorials/nmt_with_attention#the_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83f026f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Layer):\n",
    "\tdef __init__(self, text_processor, units: int):\n",
    "\t\t\"\"\"\n",
    "\t\tCreates a new Encoder layer. [dimen] is the maxiumum number of elements of the input\n",
    "\t\tthat can be processed by the encoder.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.text_processor = text_processor\n",
    "\t\tself.vocab_size = text_processor.vocabulary_size()\n",
    "\t\tself.units = units\n",
    "\n",
    "\t\t# Converts tokens -> vectors\n",
    "\t\tself.embedding = tf.keras.layers.Embedding(\n",
    "\t\t\t# mask_zero: Treats zero as a padding value that should be ignored\n",
    "\t\t\tself.vocab_size, units, mask_zero = True,\n",
    "\t\t)\n",
    "\t\tgru = tf.keras.layers.GRU(\n",
    "\t\t\tunits, return_sequences = True,\n",
    "\t\t\t# Use the recurrent_initializer suggested by the tutorial (& the default\n",
    "\t\t\t# for kernel_initializer).\n",
    "\t\t\trecurrent_initializer='glorot_uniform'\n",
    "\t\t)\n",
    "\t\tself.rnn = tf.keras.layers.Bidirectional(\n",
    "\t\t\t# merge_mode determines how the forward and backward layers are combined\n",
    "\t\t\t#            'concat' is another option here\n",
    "\t\t\tmerge_mode = 'sum',\n",
    "\t\t\tlayer=gru,\n",
    "\t\t)\n",
    "\n",
    "\tdef call(self, x):\n",
    "\t\tx = self.embedding(x)\n",
    "\t\tx = self.rnn(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef convert_input(self, texts):\n",
    "\t\ttexts = tf.convert_to_tensor(texts)\n",
    "\t\tif len(texts.shape) == 0:\n",
    "\t\t\ttexts = texts[None]\n",
    "\t\tcontext = self.text_processor(texts).to_tensor()\n",
    "\t\tcontext = self(context)\n",
    "\t\treturn context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea196ff",
   "metadata": {},
   "source": [
    "Try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331e474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens shape (batch, s): (16, 49)\n",
      "Encoder output shape (batch, s, ENCODER_UNITS): (16, 49, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 16:48:12.781839: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "ENCODER_UNITS = 48\n",
    "encoder = Encoder(context_text_processor, ENCODER_UNITS)\n",
    "\n",
    "for (context, target_history), target_next in dataset_validate.take(1):\n",
    "\tencoder_result = encoder(context)\n",
    "\tprint('Context tokens shape (batch, s):', context.shape)\n",
    "\tprint('Encoder output shape (batch, s, ENCODER_UNITS):', encoder_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd7fb6",
   "metadata": {},
   "source": [
    "### The attention layer\n",
    "\n",
    "Attention can be thought of as training a lookup table with keys and values. The lookup table has inputs `values` and `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "781b2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.Layer):\n",
    "\tdef __init__(self, units, **kwargs):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.attention_layer = tf.keras.layers.MultiHeadAttention(\n",
    "\t\t\tkey_dim=units,\n",
    "\t\t\tnum_heads=1,\n",
    "\t\t\t**kwargs\n",
    "\t\t)\n",
    "\t\t# Keeps \"the mean activation within each example close to 0 and the\n",
    "\t\t# activation standard deviation close to 1\" -- https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization?hl=en\n",
    "\t\tself.norm_layer = tf.keras.layers.LayerNormalization()\n",
    "\t\tself.add_layer = tf.keras.layers.Add()\n",
    "\t\tself.supports_masking = True\n",
    "\n",
    "\tdef call(self, query, value):\n",
    "\t\tattention_output = self.attention_layer(\n",
    "\t\t\tquery = query,\n",
    "\t\t\tvalue = value,\n",
    "\t\t\t#use_causal_mask=True,\n",
    "\t\t\t# Return the attention scores for latter plotting\n",
    "\t\t\t# return_attention_scores = True,\n",
    "\t\t)\n",
    "\n",
    "\t\tx = self.add_layer([ query, attention_output ])\n",
    "\t\tx = self.norm_layer(x)\n",
    "\t\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8857ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded context sequence shape (batch, s, units): (16, 52, 48)\n",
      "Target history sequence shape (batch, t, units): (16, 60, 48)\n",
      "Attention result shape (batch, t, units): (16, 60, 48)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(ENCODER_UNITS)\n",
    "\n",
    "# Test with an example\n",
    "for (context, target_history), target_next in dataset_validate.take(1):\n",
    "\tembed_layer = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(), output_dim=ENCODER_UNITS, mask_zero=True)\n",
    "\ttarget_embed = embed_layer(target_history)\n",
    "\tencoded_context = encoder(context)\n",
    "\tattention_result = attention_layer(target_embed, encoded_context)\n",
    "\n",
    "\tprint('Encoded context sequence shape (batch, s, units):', encoded_context.shape)\n",
    "\tprint('Target history sequence shape (batch, t, units):', target_embed.shape)\n",
    "\tprint('Attention result shape (batch, t, units):', attention_result.shape)\n",
    "\n",
    "\t# Used later \n",
    "\ttest_encoded_context = encoded_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5577a1",
   "metadata": {},
   "source": [
    "### The decoder\n",
    "\n",
    "The decoder produces queries for the attention layer. The decoder operates on `target_history`. At each step during training, it should have no information about future target output (that's what we're trying to determine). As such, we use a unidirectional RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2454259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDense(tf.keras.layers.Dense):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tsuper(CustomDense, self).__init__(*args, **kwargs)\n",
    "\t\n",
    "\tdef compute_mask(self, _inputs, mask=None):\n",
    "\t\treturn mask\n",
    "\n",
    "class Decoder(tf.keras.Layer):\n",
    "\tdef __init__(self, text_processor, units):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.text_processor = text_processor\n",
    "\t\tself.vocab_size = text_processor.vocabulary_size()\n",
    "\t\tself.units = units\n",
    "\n",
    "\t\tself.embedding_layer = tf.keras.layers.Embedding(\n",
    "\t\t\t# mask_zero: Treats zero as a padding value that should be ignored\n",
    "\t\t\tself.vocab_size, units, mask_zero = True,\n",
    "\t\t)\n",
    "\t\tself.rnn_layer = tf.keras.layers.GRU(\n",
    "\t\t\tunits, return_sequences = True, return_state = True, recurrent_initializer='glorot_uniform',\n",
    "\t\t)\n",
    "\t\tself.attention_layer = CrossAttention(units)\n",
    "\n",
    "\t\t# Creates logits with the estimated probability of each output token\n",
    "\t\tself.output_layer = CustomDense(self.vocab_size)\n",
    "\n",
    "\t\t# Conversion:\n",
    "\t\tself.word_to_id = tf.keras.layers.StringLookup(\n",
    "\t\t\tvocabulary = text_processor.get_vocabulary(),\n",
    "\t\t\tmask_token = '',\n",
    "\t\t\toov_token = '[UNK]',\n",
    "\t\t)\n",
    "\t\tself.id_to_word = tf.keras.layers.StringLookup(\n",
    "\t\t\tvocabulary = text_processor.get_vocabulary(),\n",
    "\t\t\tmask_token = '',\n",
    "\t\t\toov_token = '[UNK]',\n",
    "\t\t\tinvert = True,\n",
    "\t\t)\n",
    "\t\t# Pre-computing these simplifies exporting\n",
    "\t\tself.start_id = self.word_to_id('[START]')\n",
    "\t\tself.end_id = self.word_to_id('[END]')\n",
    "\n",
    "\t\tself.supports_masking = True\n",
    "\t\n",
    "\tdef build(self, input_shape):\n",
    "\t\t# Nothing tha needs a size allocation based on the input shape\n",
    "\t\tpass\n",
    "\t\n",
    "\tdef call(self, context, target_history, state = None, return_state = False):\n",
    "\t\tx = self.embedding_layer(target_history)\n",
    "\t\tx, state = self.rnn_layer(x, initial_state = state)\n",
    "\t\tx = self.attention_layer(x, context)\n",
    "\n",
    "\t\tlogits = self.output_layer(x)\n",
    "\t\tif return_state:\n",
    "\t\t\treturn logits, state\n",
    "\t\telse:\n",
    "\t\t\treturn logits\n",
    "\t\n",
    "\t## Conversion/testing ##\n",
    "\n",
    "\tdef tokens_to_text(self, tokens):\n",
    "\t\ttext = tf.strings.reduce_join(self.id_to_word(tokens), separator = ' ')\n",
    "\t\ttext = tf.strings.regex_replace(text, r'\\s*\\[START\\]\\s*', '')\n",
    "\t\ttext = tf.strings.regex_replace(text, r'\\s*\\[END\\].*$', '')\n",
    "\t\treturn text\n",
    "\n",
    "\tdef generate_next_token(self, context, target_history, done_vec, state, temperature = 0.0):\n",
    "\t\t# Note: is_done is a vector, indicating whether each item in the batch is done\n",
    "\n",
    "\t\tlogits, state = self(context, target_history, state = state, return_state = True)\n",
    "\n",
    "\t\t# logits has shape (batch, t, target_vocab_size). Only generate the token corresponding\n",
    "\t\t# to the last logits in the sequence (at t - 1)\n",
    "\t\tif temperature > 0:\n",
    "\t\t\tnext_token = tf.where(\n",
    "\t\t\t\tdone_vec,\n",
    "\t\t\t\ttf.constant(0, dtype=tf.int64), # Emit 0 after a sequence is done\n",
    "\t\t\t\ttf.random.categorical(logits[:, -1, :] / temperature, num_samples = 1), # Otherwise, pick the token from a categorical distribution\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tnext_token = tf.math.argmax(logits, axis=-1)\n",
    "\t\tdone_vec = done_vec|(next_token == self.end_id)\n",
    "\t\treturn next_token, done_vec, state\n",
    "\t\n",
    "\tdef get_initial_state(self, context):\n",
    "\t\t# context has shape (batch_size, s, units)\n",
    "\t\tbatch_size = tf.shape(context)[0]\n",
    "\t\tstart_tokens = tf.fill([batch_size, 1], self.start_id)\n",
    "\t\tdone_vec = tf.zeros([batch_size, 1], dtype = tf.bool)\n",
    "\n",
    "\t\t# From the Tensorflow source code:\n",
    "\t\t# > RNN expect the states in a list, even if single state.\n",
    "\t\t# Note: Without the [0] we get a type mismatch while exporting.\n",
    "\t\tinitial_state = self.rnn_layer.get_initial_state(batch_size)[0]\n",
    "\n",
    "\t\treturn start_tokens, done_vec, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d5ccb1",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "219a3522",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Missing required positional argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \ttokens \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(tokens, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# -1 = last axis\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m'\u001b[39m, decoder\u001b[38;5;241m.\u001b[39mtokens_to_text(tokens)\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtest_generation_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m, in \u001b[0;36mtest_generation_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Merge all batch outputs into a single dimension\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(tokens, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# -1 = last axis\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[0;32mIn[16], line 64\u001b[0m, in \u001b[0;36mDecoder.tokens_to_text\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokens_to_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[1;32m     63\u001b[0m \ttext \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstrings\u001b[38;5;241m.\u001b[39mreduce_join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_to_word(tokens), separator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m \ttext \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregex_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms*\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m[START\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \ttext \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstrings\u001b[38;5;241m.\u001b[39mregex_replace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[END\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m].*$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     66\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/tensorflow/python/util/dispatch.py:1254\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iterable_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m replace_iterable_params(args, kwargs, iterable_params)\n\u001b[0;32m-> 1254\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mapi_dispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1256\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: Missing required positional argument"
     ]
    }
   ],
   "source": [
    "def test_generation_loop():\n",
    "\tdecoder = Decoder(target_text_processor, ENCODER_UNITS)\n",
    "\tnext_token, done_vec, state = decoder.get_initial_state(test_encoded_context[:3, :, :])\n",
    "\ttokens = [next_token]\n",
    "\n",
    "\tfor i in range(8):\n",
    "\t\tnext_token, done_vec, state = decoder.generate_next_token(test_encoded_context[:3, :, :], next_token, done_vec, state)\n",
    "\t\ttokens.append(next_token)\n",
    "\t\n",
    "\t# Merge all batch outputs into a single dimension\n",
    "\ttokens = tf.concat(tokens, -1) # -1 = last axis\n",
    "\n",
    "\tprint('Output:', decoder.tokens_to_text(tokens).numpy())\n",
    "\n",
    "test_generation_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3411a",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b3a40",
   "metadata": {},
   "source": [
    "We can now build a model for training and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Punctuator(tf.keras.Model):\n",
    "\tdef __init__(self, units, context_text_processor, target_text_processor):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encoder = Encoder(context_text_processor, units)\n",
    "\t\tself.decoder = Decoder(target_text_processor, units)\n",
    "\t\n",
    "\tdef call(self, inputs):\n",
    "\t\tcontext, target_history = inputs\n",
    "\t\tcontext = self.encoder(context)\n",
    "\t\tlogits = self.decoder(context, target_history)\n",
    "\t\treturn logits\n",
    "\t\n",
    "\tdef fix_punctuation(self, text):\n",
    "\t\tcontext = self.encoder.convert_input(text)\n",
    "\n",
    "\t\tnext_token, done_vec, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "\t\t# A TensorArray allows more efficient exporting\n",
    "\t\t#tokens = tf.TensorArray(tf.int64, size=0, dynamic_size=True)\n",
    "\t\t#tokens = tokens.write(0, next_token)\n",
    "\t\ttokens = []\n",
    "\n",
    "\t\t# Use a tf.constant to allow exporting\n",
    "\t\t#i = tf.constant(0)\n",
    "\t\t#max_iterations = tf.constant(60)\n",
    "\n",
    "\t\tfor i in range(34):\n",
    "\t\t\t# token_history has size: (batch, t, target_vocab_size)\n",
    "\t\t\t# token_history = tf.concat(tokens, 1)\n",
    "\t\t\t# print('history', model.decoder.id_to_word(token_history))\n",
    "\t\t\tnext_token, done_vec, state = self.decoder.generate_next_token(context, next_token, done_vec, state, temperature=0)\n",
    "\t\t\t#tokens = tokens.write(i + 1, next_token)\n",
    "\t\t\ttokens.append(next_token)\n",
    "\n",
    "\t\t\tif tf.executing_eagerly() and tf.reduce_all(done_vec):\n",
    "\t\t\t\tbreak\n",
    "\t\t\t# while not tf.reduce_all(done_vec):\n",
    "\t\t\t# \n",
    "\t\t\n",
    "\t\t#tokens = tokens.stack()\n",
    "\t\ttokens = tf.concat(tokens, -1)\n",
    "\t\t# batch_size = tf.shape(context)[0]\n",
    "\t\t# When exporting to ONNX, tokens_to_text can only operate on a single dimension. As such,\n",
    "\t\t# all inputs are collapsed:\n",
    "\t\ttokens = tf.reshape(tokens, [ -1 ])\n",
    "\t\treturn self.decoder.tokens_to_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens shape (batch, s): (16, 50)\n",
      "Target history tokens shape (batch, t): (16, 59)\n",
      "Logits shape (batch, t, vocab_size) (16, 59, 2000)\n"
     ]
    }
   ],
   "source": [
    "model = Punctuator(ENCODER_UNITS, context_text_processor, target_text_processor)\n",
    "\n",
    "for (example_context_tok, example_target_hist), _ in dataset_validate.take(1):\n",
    "\ttest_logits = model((example_context_tok, example_target_hist))\n",
    "\tprint('Context tokens shape (batch, s):', example_context_tok.shape)\n",
    "\tprint('Target history tokens shape (batch, t):', example_target_hist.shape)\n",
    "\tprint('Logits shape (batch, t, vocab_size)', test_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e81b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"punctuator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"punctuator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">124,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">217,616</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ encoder_1 (\u001b[38;5;33mEncoder\u001b[0m)             │ ?                      │       \u001b[38;5;34m124,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │       \u001b[38;5;34m217,616\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">341,840</span> (1.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m341,840\u001b[0m (1.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">341,840</span> (1.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m341,840\u001b[0m (1.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85551c17",
   "metadata": {},
   "source": [
    "To avoid penalizing masked outputs, we use a custom loss function (see the tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss(y_true, y_predict):\n",
    "\tloss = base_loss_fn(y_true, y_predict)\n",
    "\t\n",
    "\tunmasked = y_true != 0\n",
    "\tunmasked = tf.cast(unmasked, loss.dtype)\n",
    "\t# Only consider output with a corresponding label.\n",
    "\tloss *= unmasked\n",
    "\n",
    "\tcount_unmasked = tf.math.reduce_sum(unmasked)\n",
    "\n",
    "\t# reduce_sum: Adds all entries of a vector.\n",
    "\treturn tf.math.reduce_sum(loss)/count_unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba83b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, predict_logits):\n",
    "\tpredicted_index = tf.math.argmax(predict_logits, axis=-1)\n",
    "\tpredicted_index = tf.cast(predicted_index, y_true.dtype)\n",
    "\n",
    "\tmatch = tf.cast(y_true == predicted_index, tf.float32)\n",
    "\tunmasked = tf.cast(y_true != 0, tf.float32)\n",
    "\tcount_unmasked = tf.math.reduce_sum(unmasked)\n",
    "\n",
    "\treturn tf.math.reduce_sum(match * unmasked) / count_unmasked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[masked_accuracy, masked_loss])\n",
    "model.compile(optimizer='adam', loss=masked_loss, metrics=[masked_accuracy, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae7f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the tutorial:\n",
      "expected loss 7.6009026\n",
      "expected accuracy 0.0005\n"
     ]
    }
   ],
   "source": [
    "print('From the tutorial:')\n",
    "vocab_size = float(target_text_processor.vocabulary_size())\n",
    "\n",
    "print('expected loss', tf.math.log(vocab_size).numpy())\n",
    "print('expected accuracy', 1/vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - loss: 7.5944 - masked_accuracy: 8.6438e-04 - masked_loss: 7.5944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 7.595437526702881,\n",
       " 'masked_accuracy': 0.0006910106167197227,\n",
       " 'masked_loss': 7.595437526702881}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(dataset_validate, steps=20, return_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179e6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[test]: vessel blow eyes hungry years followed seat meet approached evident emmanuel sorry top matter demanded demanded muttered game maximilian evening lifted lifted marks curtain possible or dull shining haste chain replied don slept fit'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def test_punctuation(text):\n",
    "\treturn '[test]: ' + model.fix_punctuation(text).numpy().decode('utf-8')\n",
    "\n",
    "class DemoCallback(tf.keras.callbacks.Callback):\n",
    "\tdef on_epoch_end(self, epoch_index: int, logs = None):\n",
    "\t\tprint('\\r', test_punctuation([ 'test this is a sample will it work if it does then how well' ]))\n",
    "\t\tif epoch_index % 10 == 0:\n",
    "\t\t\t# From the test data\n",
    "\t\t\tprint(test_punctuation([\n",
    "\t\t\t\t'not that alice had any idea of doing that she felt as if she would never be able to talk again, she was getting so much out of breath and still the queen cried faster faster and dragged her along'\n",
    "\t\t\t]))\n",
    "\t\t\tprint(test_punctuation([ 'tensorflow is a library that is used for machine learning it is available for more languages than just python' ]))\n",
    "\t\t\tprint(test_punctuation([ 'the joplin note taking app can be used to take multimedia notes' ]))\n",
    "\t\t\tprint(test_punctuation([ 'here are a few words javascript typescript python joplin interesting loud and sequence these words are all very useful' ]))\n",
    "\n",
    "test_punctuation(tf.constant([ 'this is an example they said' ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db06e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 5.2618 - masked_accuracy: 0.1740 - masked_loss: 5.2618"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [test]: [cap] it is a [UNK] [UNK] . [END]\n",
      "[test]: [cap] the [cap] [UNK] , [cap] i had been [UNK] , and the [UNK] of the [UNK] of the [cap] [UNK] , [cap] i had been [UNK] , and the [UNK] of the [UNK]\n",
      "[test]: [cap] [UNK] , [UNK] [UNK] , [UNK] [UNK] , [UNK] [UNK] , [UNK] [UNK] . [END]\n",
      "[test]: [cap] [UNK] , [cap] i have been [UNK] to [UNK] . [END]\n",
      "[test]: [cap] i have been [UNK] [UNK] [UNK] , and [UNK] [UNK] [UNK] , and [UNK] [UNK] [UNK] , and [UNK] [UNK] [UNK] , and [UNK] [UNK] [UNK] , and [UNK] [UNK] [UNK] , and\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 80ms/step - loss: 5.2604 - masked_accuracy: 0.1741 - masked_loss: 5.2604 - val_loss: 3.8750 - val_masked_accuracy: 0.2889 - val_masked_loss: 3.8154\n",
      "Epoch 2/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 3.6518 - masked_accuracy: 0.3323 - masked_loss: 3.6518"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 16:20:57.766915: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [test]: [cap] [UNK] this is a [UNK] will it your own , it is then , how ? [END]\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 78ms/step - loss: 3.6511 - masked_accuracy: 0.3325 - masked_loss: 3.6511 - val_loss: 2.4330 - val_masked_accuracy: 0.5380 - val_masked_loss: 2.3956\n",
      "Epoch 3/20\n",
      " [test]: [cap] [UNK] this is a [UNK] will it work , if it does then , how well . [END]2067 - masked_accuracy: 0.5801 - masked_loss: 2.2067\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 78ms/step - loss: 2.2062 - masked_accuracy: 0.5801 - masked_loss: 2.2062 - val_loss: 1.5380 - val_masked_accuracy: 0.6716 - val_masked_loss: 1.5144\n",
      "Epoch 4/20\n",
      " [test]: [cap] [UNK] this is a [UNK] will it work , if it does then how well . [END]1.3861 - masked_accuracy: 0.7122 - masked_loss: 1.3861\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 77ms/step - loss: 1.3859 - masked_accuracy: 0.7122 - masked_loss: 1.3859 - val_loss: 0.9245 - val_masked_accuracy: 0.7842 - val_masked_loss: 0.9103\n",
      "Epoch 5/20\n",
      " [test]: [cap] [UNK] this is a [UNK] will it work , if it does then , how well . [END]9688 - masked_accuracy: 0.7885 - masked_loss: 0.9688\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 77ms/step - loss: 0.9688 - masked_accuracy: 0.7885 - masked_loss: 0.9688 - val_loss: 0.7569 - val_masked_accuracy: 0.8164 - val_masked_loss: 0.7452\n",
      "Epoch 6/20\n",
      " [test]: [cap] [UNK] this is a [UNK] will it work , if it does then , then well . [END]611 - masked_accuracy: 0.8245 - masked_loss: 0.7611\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 79ms/step - loss: 0.7610 - masked_accuracy: 0.8245 - masked_loss: 0.7610 - val_loss: 0.6343 - val_masked_accuracy: 0.8321 - val_masked_loss: 0.6245\n",
      "Epoch 7/20\n",
      " [test]: [cap] [UNK] , this is a [UNK] will it work if it does , then how well . [END]6541 - masked_accuracy: 0.8411 - masked_loss: 0.6541\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 77ms/step - loss: 0.6541 - masked_accuracy: 0.8411 - masked_loss: 0.6541 - val_loss: 0.6323 - val_masked_accuracy: 0.8310 - val_masked_loss: 0.6225\n",
      "Epoch 8/20\n",
      " [test]: [cap] [UNK] this is a [UNK] will it work if it does then , how well . [END]0.6124 - masked_accuracy: 0.8491 - masked_loss: 0.6124\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 77ms/step - loss: 0.6124 - masked_accuracy: 0.8491 - masked_loss: 0.6124 - val_loss: 0.5137 - val_masked_accuracy: 0.8531 - val_masked_loss: 0.5058\n",
      "Epoch 9/20\n",
      " [test]: [cap] [UNK] this is a [UNK] will it work if it does then how well . [END]: 0.5502 - masked_accuracy: 0.8589 - masked_loss: 0.5502\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 79ms/step - loss: 0.5503 - masked_accuracy: 0.8589 - masked_loss: 0.5503 - val_loss: 0.5265 - val_masked_accuracy: 0.8493 - val_masked_loss: 0.5184\n",
      "Epoch 10/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.5583 - masked_accuracy: 0.8572 - masked_loss: 0.5583"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 16:26:09.167080: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [test]: [cap] [UNK] this is a [UNK] will it work if it does then , then , how well . [END]\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 78ms/step - loss: 0.5583 - masked_accuracy: 0.8572 - masked_loss: 0.5583 - val_loss: 0.5850 - val_masked_accuracy: 0.8355 - val_masked_loss: 0.5760\n",
      "Epoch 11/20\n",
      " [test]: [cap] [UNK] this is a [UNK] will it work , if it does then how well . [END]0.5405 - masked_accuracy: 0.8594 - masked_loss: 0.5405\n",
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would never be able to talk again . [cap] she was getting getting so much out of breath\n",
      "[test]: [cap] [UNK] is a library , that is used for [UNK] [UNK] it is [UNK] for more [UNK] than just [UNK] . [END]\n",
      "[test]: [cap] the [UNK] note , taking [UNK] can be used to take [UNK] notes . [END]\n",
      "[test]: [cap] here are a few words [UNK] [UNK] [UNK] , [UNK] interesting loud and [UNK] these words are all very [UNK] . [END]\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 83ms/step - loss: 0.5406 - masked_accuracy: 0.8594 - masked_loss: 0.5406 - val_loss: 0.5016 - val_masked_accuracy: 0.8525 - val_masked_loss: 0.4938\n",
      "Epoch 12/20\n",
      " [test]: [cap] [UNK] , this is a [UNK] will it work , if it does then ? [cap] how well . [END]asked_accuracy: 0.8606 - masked_loss: 0.5325\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 81ms/step - loss: 0.5325 - masked_accuracy: 0.8606 - masked_loss: 0.5324 - val_loss: 0.4766 - val_masked_accuracy: 0.8566 - val_masked_loss: 0.4693\n",
      "Epoch 13/20\n",
      " [test]: [cap] [UNK] . [cap] this is a [UNK] will it work , if it does then how well . [END] masked_accuracy: 0.8713 - masked_loss: 0.4763\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 70ms/step - loss: 0.4764 - masked_accuracy: 0.8713 - masked_loss: 0.4764 - val_loss: 0.4479 - val_masked_accuracy: 0.8623 - val_masked_loss: 0.4410\n",
      "Epoch 14/20\n",
      " [test]: [cap] [UNK] this is a [UNK] will it work , if it does then ? [cap] how well . [END] masked_accuracy: 0.8694 - masked_loss: 0.4867\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 74ms/step - loss: 0.4868 - masked_accuracy: 0.8694 - masked_loss: 0.4868 - val_loss: 0.8783 - val_masked_accuracy: 0.7886 - val_masked_loss: 0.8648\n",
      "Epoch 15/20\n",
      " [test]: [cap] [UNK] , this is a [UNK] will it work , if it does then ? [cap] how well . [END]asked_accuracy: 0.8591 - masked_loss: 0.5519\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 71ms/step - loss: 0.5517 - masked_accuracy: 0.8592 - masked_loss: 0.5517 - val_loss: 0.4350 - val_masked_accuracy: 0.8653 - val_masked_loss: 0.4283\n",
      "Epoch 16/20\n",
      " [test]: [cap] [UNK] this is a [UNK] will it work if it does then how well . [END]: 0.4554 - masked_accuracy: 0.8750 - masked_loss: 0.4554\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 75ms/step - loss: 0.4554 - masked_accuracy: 0.8750 - masked_loss: 0.4554 - val_loss: 0.4541 - val_masked_accuracy: 0.8624 - val_masked_loss: 0.4471\n",
      "Epoch 17/20\n",
      " [test]: [cap] [UNK] . [cap] this is a [UNK] will it work , if it does then how well . [END] masked_accuracy: 0.8767 - masked_loss: 0.4524\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 76ms/step - loss: 0.4524 - masked_accuracy: 0.8767 - masked_loss: 0.4524 - val_loss: 0.4158 - val_masked_accuracy: 0.8704 - val_masked_loss: 0.4094\n",
      "Epoch 18/20\n",
      " [test]: [cap] [UNK] . [cap] this is a [UNK] will it work if it does then how well . [END] - masked_accuracy: 0.8777 - masked_loss: 0.4422\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 81ms/step - loss: 0.4422 - masked_accuracy: 0.8777 - masked_loss: 0.4422 - val_loss: 0.4289 - val_masked_accuracy: 0.8659 - val_masked_loss: 0.4223\n",
      "Epoch 19/20\n",
      " [test]: [cap] [UNK] this is a [UNK] will it work if it does , then how well . [END]0.4502 - masked_accuracy: 0.8773 - masked_loss: 0.4502\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 74ms/step - loss: 0.4502 - masked_accuracy: 0.8773 - masked_loss: 0.4502 - val_loss: 0.5549 - val_masked_accuracy: 0.8471 - val_masked_loss: 0.5463\n",
      "Epoch 20/20\n",
      " [test]: [cap] [UNK] this is a [UNK] will it work , if it does , then how well . [END]4373 - masked_accuracy: 0.8803 - masked_loss: 0.4373\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 76ms/step - loss: 0.4373 - masked_accuracy: 0.8803 - masked_loss: 0.4373 - val_loss: 0.4292 - val_masked_accuracy: 0.8675 - val_masked_loss: 0.4226\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "\tdataset_train,\n",
    "\tepochs = 30,\n",
    "\tsteps_per_epoch = 500,\n",
    "\tvalidation_data = dataset_validate,\n",
    "\tcallbacks=[DemoCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a9fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test]: [cap] not that [cap] alice had any idea of doing that she felt as if she would never be able to talk again she was getting so much out of breath and still the\n",
      "[test]: [cap] this is a [UNK] of a the [UNK] [UNK] for [cap] i am curious how well it [UNK] . [END]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_punctuation([\n",
    "\t'not that alice had any idea of doing that she felt as if she would never be able to talk again she was getting so much out of breath and still the queen cried faster faster and dragged her along'\n",
    "]))\n",
    "print(test_punctuation([ 'this is a test of a the punctuation system for i am curious how well it works' ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1e9cc9",
   "metadata": {},
   "source": [
    "## Exporting\n",
    "\n",
    "Based on the [Export](https://www.tensorflow.org/text/tutorials/nmt_with_attention#export) section of the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9885c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Export(tf.Module):\n",
    "\tdef __init__(self, model):\n",
    "\t\tself.model = model\n",
    "\t\n",
    "\t@tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "\tdef fix_punctuation(self, inputs):\n",
    "\t\treturn model.fix_punctuation(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bcac14",
   "metadata": {},
   "source": [
    "Run `fix_punctuation` once to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa037d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "export = Export(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af27599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1725579156.092879   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.094410   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.095418   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.096418   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.097391   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.098292   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.098739   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.099194   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.099651   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.100125   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.100580   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.101095   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.101634   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.102197   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.102784   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.103479   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.104122   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.104692   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.105243   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.105789   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.106371   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.106914   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.107422   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.107955   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.108471   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.108984   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.109485   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.109993   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.110494   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.111014   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.111510   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.112029   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.112527   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579156.113040   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b\"[cap] this sentence shall be [UNK] for the following reasons first [UNK] makes things [UNK] to read second [UNK] . [END] . [END] - [UNK] . [END] . [END] . [END] ' s .\">"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_inputs = tf.constant([ 'this sentence shall be punctuated for the following reasons first punctatuion makes things easier to read second um' ])\n",
    "export.fix_punctuation(sample_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70247292",
   "metadata": {},
   "source": [
    "Now we save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb8d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/self/Documents/punctuation-fixer/env/lib64/python3.11/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: punctuator-seq2seq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: punctuator-seq2seq/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(export, 'punctuator-seq2seq', signatures={ 'serving_default': export.fix_punctuation })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197976b4",
   "metadata": {},
   "source": [
    "See [the documentation](https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export) for information about the `signatures` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39fbc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1725579164.622979   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.624267   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.625886   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.626974   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.628011   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.628919   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.629445   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.630004   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.630457   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.630936   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.631394   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.631867   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.632337   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.632836   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.633277   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.633740   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.634210   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.634669   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.635138   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.635627   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.636112   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.636622   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.637226   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.637859   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.638381   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.638951   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.639500   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.640040   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.640569   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.641169   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.641729   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.642247   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.642772   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "W0000 00:00:1725579164.643306   10088 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"106\" frequency: 2611 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 18874368 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported and warmed up!\n"
     ]
    }
   ],
   "source": [
    "reloaded = tf.saved_model.load('punctuator-seq2seq')\n",
    "# Warmup\n",
    "reloaded.fix_punctuation(tf.constant(['this is a test is it not']))\n",
    "print('Imported and warmed up!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca974c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 74.1 ms, sys: 27.8 ms, total: 102 ms\n",
      "Wall time: 40.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b\"[cap] this sentence should end with a full stop . [END] . [END] . [END] . [END] . [END] ' s . [END] . [END] ' . [END] . [END] ' . [END] .\">"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "reloaded.fix_punctuation(tf.constant(['this sentence should end with a full stop']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2771e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
