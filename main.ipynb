{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71899cc9-86b4-4eda-b525-d818661c70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 15:15:53.913380: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-31 15:15:53.916932: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-31 15:15:53.926520: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-31 15:15:53.942232: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-31 15:15:53.946733: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-31 15:15:53.958760: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-31 15:15:54.781552: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa459db2-5a48-4bac-8cbd-38496888faee",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention) (and [at least one other](https://www.tensorflow.org/text/tutorials/text_generation) of the Tensorflow tutorials).\n",
    "- This [blog post](https://janakiev.com/blog/jupyter-virtual-envs/) was referenced to set up the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b150ba-48c0-4f33-8d08-b9f363d6fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "from prepare_data import load_data, reconstruct_from_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5faca0eb-c672-4d1c-884f-b5ba38a22b63",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m context_raw, target_raw \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/en/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/prepare_data.py:135\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(dirPath)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_file() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mas_posix()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m src, dst \u001b[38;5;241m=\u001b[39m \u001b[43mload_file_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m source\u001b[38;5;241m.\u001b[39mextend(src)\n\u001b[1;32m    137\u001b[0m dest\u001b[38;5;241m.\u001b[39mextend(dst)\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/prepare_data.py:119\u001b[0m, in \u001b[0;36mload_file_data\u001b[0;34m(dataPath)\u001b[0m\n\u001b[1;32m    115\u001b[0m paragraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m paragraph\n\u001b[1;32m    117\u001b[0m source\u001b[38;5;241m.\u001b[39mextend(normalized)\n\u001b[0;32m--> 119\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(normalized)\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/prepare_data.py:75\u001b[0m, in \u001b[0;36mcreate_labels\u001b[0;34m(original, normalized)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(character) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([ operation\u001b[38;5;241m.\u001b[39mvalue, character ])\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(to_np, output)))\n",
      "File \u001b[0;32m~/Documents/punctuation-fixer/prepare_data.py:73\u001b[0m, in \u001b[0;36mcreate_labels.<locals>.to_np\u001b[0;34m(entry)\u001b[0m\n\u001b[1;32m     71\u001b[0m operation, character \u001b[38;5;241m=\u001b[39m entry\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(character) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([ \u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m, character ])\n",
      "File \u001b[0;32m/usr/lib64/python3.11/enum.py:211\u001b[0m, in \u001b[0;36mproperty.__get__\u001b[0;34m(self, instance, ownerclass)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (ownerclass, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    209\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/python3.11/enum.py:1257\u001b[0m, in \u001b[0;36mEnum.value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The name of the Enum member.\"\"\"\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_\n\u001b[0;32m-> 1257\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The value of the Enum member.\"\"\"\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value_\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "context_raw, target_raw = load_data('./data/en/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890fa88",
   "metadata": {},
   "source": [
    "We store the **expected** output in `target_raw` and the input to our model in `context_raw`. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbcb7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', 'x'],\n",
       "       ['0', 'x'],\n",
       "       ['0', 'x'],\n",
       "       ...,\n",
       "       ['1', 'x'],\n",
       "       ['0', 'x'],\n",
       "       ['0', 'x']], dtype='<U21')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab821739",
   "metadata": {},
   "source": [
    "Each element in `target_raw` is an operation (e.g. 0 = copy) followed by a character code. For example, `1` is \"capitalize\" and `0` is \"copy\". Note that `120` (`ord(x)`) is used for operations that take no arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea3dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['0', 'x'], dtype='<U21'), 'n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_raw[24], context_raw[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f129de",
   "metadata": {},
   "source": [
    "\n",
    "## Creating a dataset\n",
    "\n",
    "We begin by vectorizing our data. `target_raw` and `context_raw` are already tokenized by characters/operations.\n",
    "\n",
    "We start by creating a vectorization for the `target_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2805cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size in chars: 53\n"
     ]
    }
   ],
   "source": [
    "input_vocab = sorted(set(context_raw))\n",
    "print('Input vocab size in chars:', len(input_vocab))\n",
    "\n",
    "chars_to_ids_in = tf.keras.layers.StringLookup(vocabulary=input_vocab)\n",
    "# Invert: Map chars to IDs instead of IDs to chars\n",
    "ids_to_chars_in = tf.keras.layers.StringLookup(vocabulary=chars_to_ids_in.get_vocabulary(), invert=True)\n",
    "\n",
    "# in: \"Input\"\n",
    "def text_from_ids_in(ids: list[int]):\n",
    "\treturn tf.strings.reduce_join(ids_to_chars_in(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc264b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6185399,), dtype=int64, numpy=array([22, 25, 25, ..., 31, 13,  1])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids_input = chars_to_ids_in(context_raw)\n",
    "all_ids_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b3a40",
   "metadata": {},
   "source": [
    "Now, we do the same for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520bd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vocab size in chars: 8\n"
     ]
    }
   ],
   "source": [
    "output_vocab = sorted(set(target_raw[:, 1]))\n",
    "print('Output vocab size in chars:', len(output_vocab))\n",
    "\n",
    "chars_to_ids_out = tf.keras.layers.StringLookup(vocabulary=output_vocab)\n",
    "# Invert: Map chars to IDs instead of IDs to chars\n",
    "ids_to_chars_out = tf.keras.layers.StringLookup(vocabulary=chars_to_ids_out.get_vocabulary(), invert=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba83b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', \"'\", ',', '-', '.', '/', '?', 'x']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(target_raw[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fde0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6185399,), dtype=int64, numpy=array([8, 8, 8, ..., 8, 8, 8])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids_output = chars_to_ids_out(target_raw[:, 1])\n",
    "all_ids_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7b5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_commands_output = np.array(list(map(int, target_raw[:, 0])))\n",
    "all_commands_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af035816",
   "metadata": {},
   "source": [
    "Now that we have vectorized inputs and outputs, let's create a `Dataset` we can feed to the model.\n",
    "\n",
    "First, combine the expected inputs and outputs into a single vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e72752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6185399, 3), dtype=int64, numpy=\n",
       "array([[22,  1,  8],\n",
       "       [25,  0,  8],\n",
       "       [25,  0,  8],\n",
       "       ...,\n",
       "       [31,  1,  8],\n",
       "       [13,  0,  8],\n",
       "       [ 1,  0,  8]])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def column(v):\n",
    "\treturn tf.reshape(v, [-1, 1])\n",
    "\n",
    "ids_and_outputs = tf.concat([\n",
    "\tcolumn(all_ids_input), column(all_commands_output), column(all_ids_output)\n",
    "], 1)\n",
    "ids_and_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45b3dd",
   "metadata": {},
   "source": [
    "Next, create a `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1626f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i c(1, x), l c(0, x), l c(0, x), u c(0, x), s c(0, x), t c(0, x), r c(0, x), a c(0, x), t c(0, x), i c(0, x), o c(0, x), n c(0, x),   c(0, x), a c(1, x), l c(0, x), i c(0, x), c c(0, x), e c(0, x), s c(2, '),   c(0, x), a c(1, x), d c(0, x), v c(0, x), e c(0, x), n c(0, x), t c(0, x), u c(0, x), r c(0, x), e c(0, x), s c(0, x),   c(0, x), i c(0, x), "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 14:50:46.883326: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Separates ids_and_outputs along its first dimension into different items in the dataset.\n",
    "ids_input_dataset = tf.data.Dataset.from_tensor_slices(all_ids_input)\n",
    "# Tuples: from_tensor_slices pairs entries of each tuple item to produce the dataset.\n",
    "output_dataset = tf.data.Dataset.from_tensor_slices((all_commands_output, all_ids_output))\n",
    "dataset = tf.data.Dataset.zip(ids_input_dataset, output_dataset)\n",
    "\n",
    "# Preview the dataset -- demonstrates converting Tensors to numpy to text\n",
    "for input, expected_sample_outputs in dataset.take(32):\n",
    "\tcmd_output, arg_output = expected_sample_outputs\n",
    "\tinput_char = ids_to_chars_in(input).numpy().decode('utf-8')\n",
    "\tcommand = cmd_output.numpy()\n",
    "\tcommand_arg = ids_to_chars_out(arg_output).numpy().decode('utf-8')\n",
    "\n",
    "\tprint('{} c({}, {})'.format(input_char, command, command_arg), end = ', ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe60be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tf.Tensor(b'illustration alices adventures in wonderland by lewis carroll the millennium fulc', shape=(), dtype=string)\n",
      "Inputs: tf.Tensor(b'rum edition 30 contents chapter i down the rabbithole chapter ii the pool of tear', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 14:50:46.960135: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "seq_length = 80\n",
    "\n",
    "# batch: Convert the dataset to sequences of the target size.\n",
    "# drop_remainder: Drop the last batch if it has fewer than 80 elements\n",
    "sequences = dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for sample_inputs, expected_sample_outputs in sequences.take(2):\n",
    "\tprint('Inputs:', text_from_ids_in(sample_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c9b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7917f",
   "metadata": {},
   "source": [
    "Our dataset now pairs inputs and labels!\n",
    "\n",
    "**Note**: This [StackOverflow](https://stackoverflow.com/questions/53171885/how-to-use-tf-data-dataset-and-tf-keras-do-multi-inputs-and-multi-outpus) question, the documentation on [Dataset.zip](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=en#zip), and documentation on [Dataset.from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=en#from_tensor_slices) were helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854f4b3",
   "metadata": {},
   "source": [
    "## Final preprocessing\n",
    "\n",
    "We now shuffle the data, then do final batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True).prefetch(tf.data.AUTOTUNE)\n",
    "# Break into test and training data (no validation data for now).\n",
    "# Inspired by https://stackoverflow.com/a/74609848.\n",
    "# test_size = dataset.cardinality() * 1 // 4\n",
    "# dataset_test = dataset.take(test_size)\n",
    "# dataset = dataset.skip(test_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3f818",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b9620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_in 54\n",
      "vocab_size_out 9\n",
      "command_count 3\n",
      "EMBEDDING_DIM 32\n",
      "RNN_UNITS 64\n"
     ]
    }
   ],
   "source": [
    "# .get_vocabulary: Returns a list of the characters in use.\n",
    "vocab_size_in = len(chars_to_ids_in.get_vocabulary())\n",
    "\n",
    "vocab_size_out = len(chars_to_ids_out.get_vocabulary())\n",
    "command_count = len(set(all_commands_output))\n",
    "size_out = command_count + vocab_size_out # Output includes both commands and the command arg\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "RNN_UNITS = 64 # Dimensionality of GRU output\n",
    "\n",
    "print('vocab_size_in', vocab_size_in)\n",
    "print('vocab_size_out', vocab_size_out)\n",
    "print('command_count', command_count)\n",
    "print('EMBEDDING_DIM', EMBEDDING_DIM)\n",
    "print('RNN_UNITS', RNN_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(tf.keras.Model):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.embedding_layer = tf.keras.layers.Embedding(vocab_size_in, EMBEDDING_DIM)\n",
    "\t\t# return_sequences: Return the full sequence of outputs, rather than just the last.\n",
    "\t\t# return_state: Returns the last state in addition to the output\n",
    "\t\tself.gru_layer = tf.keras.layers.GRU(RNN_UNITS, return_sequences=True, return_state=True)\n",
    "\t\tself.dense_command_layer = tf.keras.layers.Dense(command_count, activation=tf.keras.activations.log_softmax)\n",
    "\t\tself.dense_arg_layer = tf.keras.layers.Dense(vocab_size_out, activation=tf.keras.activations.log_softmax)\n",
    "\t\n",
    "\tdef call(self, inputs, states = None, return_state = False, training = False):\n",
    "\t\tx = self.embedding_layer(inputs, training = training)\n",
    "\t\tif states is None:\n",
    "\t\t\tbatch_size, _ = inputs.shape\n",
    "\t\t\tstates = self.gru_layer.get_initial_state(batch_size)\n",
    "\n",
    "\t\tx, states = self.gru_layer(x, initial_state = states, training = training)\n",
    "\t\ty_command = self.dense_command_layer(x, training = training)\n",
    "\t\ty_arg = self.dense_arg_layer(x, training = training)\n",
    "\n",
    "\t\tif return_state:\n",
    "\t\t\treturn y_command, y_arg, states\n",
    "\t\telse:\n",
    "\t\t\treturn y_command, y_arg\n",
    "\n",
    "# We override tf.keras.Model to allow extracting the state later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d58d5",
   "metadata": {},
   "source": [
    "## Trying the (untrained) model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dbe598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TakeDataset element_spec=(TensorSpec(shape=(64, 81), dtype=tf.int64, name=None), (TensorSpec(shape=(64, 81), dtype=tf.int64, name=None), TensorSpec(shape=(64, 81), dtype=tf.int64, name=None)))>\n",
      "(64, 81, 3) :: (batch_size, seq_length, num_commands)\n",
      "(64, 81, 9) :: (batch_size, seq_length, num_command_args)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"language_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"language_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,728</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,816</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>))                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">585</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m81\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │         \u001b[38;5;34m1,728\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m81\u001b[0m, \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;34m64\u001b[0m,    │        \u001b[38;5;34m18,816\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m64\u001b[0m))                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m81\u001b[0m, \u001b[38;5;34m3\u001b[0m)            │           \u001b[38;5;34m195\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m81\u001b[0m, \u001b[38;5;34m9\u001b[0m)            │           \u001b[38;5;34m585\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,324</span> (83.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,324\u001b[0m (83.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,324</span> (83.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,324\u001b[0m (83.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(dataset.take(1))\n",
    "\n",
    "for sample_inputs, expected_sample_outputs in dataset.take(1):\n",
    "\tsample_cmd_predictions, sample_arg_predictions = model(sample_inputs)\n",
    "\tprint(sample_cmd_predictions.shape, ':: (batch_size, seq_length, num_commands)')\n",
    "\tprint(sample_arg_predictions.shape, ':: (batch_size, seq_length, num_command_args)')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa561ba",
   "metadata": {},
   "source": [
    "Now let's inspect `sample_predictions`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793ee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 1)\n",
      "(81,)\n"
     ]
    }
   ],
   "source": [
    "# Take one sample of the data, where sample_cmd_predictions[0] contains log probability\n",
    "sampled_cmd_indices = tf.random.categorical(sample_cmd_predictions[0], num_samples = 1)\n",
    "print(sampled_cmd_indices.shape)\n",
    "\n",
    "# tf.squeeze: Removes dimensions of size 1.\n",
    "sampled_cmd_indices = tf.squeeze(sampled_cmd_indices).numpy()\n",
    "print(sampled_cmd_indices.shape)\n",
    "\n",
    "# Do the same for the arg predictions\n",
    "sampled_arg_indices = tf.squeeze(tf.random.categorical(sample_arg_predictions[0], num_samples = 1)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad136ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_from_ids_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[43mtext_from_ids_in\u001b[49m(sample_inputs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput:\u001b[39m\u001b[38;5;124m'\u001b[39m, input_text)\n\u001b[1;32m      4\u001b[0m sampled_commands \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack((sampled_cmd_indices, ids_to_chars_out(sampled_arg_indices)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_from_ids_in' is not defined"
     ]
    }
   ],
   "source": [
    "input_text = text_from_ids_in(sample_inputs[0]).numpy().decode('utf-8')\n",
    "print('Input:', input_text)\n",
    "\n",
    "sampled_commands = np.stack((sampled_cmd_indices, ids_to_chars_out(sampled_arg_indices)), axis=1)\n",
    "reconstructed = reconstruct_from_labels(input_text, sampled_commands)\n",
    "print('Next predictions:', reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d276d",
   "metadata": {},
   "source": [
    "Seemingly random output, as expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8707f29",
   "metadata": {},
   "source": [
    "## Training!\n",
    "\n",
    "We can train it now! It's a standard classification problem -- given the previous RNN state and the current character, predict the next character.\n",
    "\n",
    "We're using the `SparseCategoricalCrossentropy` loss. See https://datascience.stackexchange.com/a/41923 and perhaps https://stats.stackexchange.com/a/420730 for commentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e4b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss pre-training (cmd): 2.978065013885498\n",
      "loss pre-training (arg): 8.961331367492676\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "sample_batch_cmd_mean_loss = loss_fn(expected_sample_outputs[0], sample_cmd_predictions)\n",
    "sample_batch_arg_mean_loss = loss_fn(expected_sample_outputs[1], sample_arg_predictions)\n",
    "print('loss pre-training (cmd):', float(tf.exp(sample_batch_cmd_mean_loss)))\n",
    "print('loss pre-training (arg):', float(tf.exp(sample_batch_arg_mean_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0a3a3",
   "metadata": {},
   "source": [
    "As expected, the initial loss is large.\n",
    "\n",
    "Now we attach the loss function and an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a42f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=[loss_fn, loss_fn])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222cd99",
   "metadata": {},
   "source": [
    "We're just about ready to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f36be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpoints\n",
    "\n",
    "checkpoint_path = './tf_model_checkpoints/checkpoint.weights.h5'\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "\tfilepath=checkpoint_path, monitor='loss', mode='min', save_weights_only=True, save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e9c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad20dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 55ms/step - loss: 0.2410 - sparse_categorical_crossentropy_loss: 0.1008\n",
      "Epoch 2/5\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 57ms/step - loss: 0.2388 - sparse_categorical_crossentropy_loss: 0.0998\n",
      "Epoch 3/5\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 53ms/step - loss: 0.2364 - sparse_categorical_crossentropy_loss: 0.0990\n",
      "Epoch 4/5\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 54ms/step - loss: 0.2349 - sparse_categorical_crossentropy_loss: 0.0984\n",
      "Epoch 5/5\n",
      "\u001b[1m1193/1193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 60ms/step - loss: 0.2323 - sparse_categorical_crossentropy_loss: 0.0974\n"
     ]
    }
   ],
   "source": [
    "# Note: This block was run >1 times (twice)\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34e041",
   "metadata": {},
   "source": [
    "## Add punctuation\n",
    "\n",
    "Let's try it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a23d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Punctuator:\n",
    "\tdef __init__(self, model: LanguageModel, temperature: float = 1.0):\n",
    "\t\tself.temperature = temperature\n",
    "\t\tself.model = model\n",
    "\t\tself.last_states = None\n",
    "\n",
    "\t\t# See https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor?hl=en\n",
    "\t\tskip_ids = chars_to_ids_out(['[UNK]'])\n",
    "\t\tout_vocab_size = len(chars_to_ids_out.get_vocabulary())\n",
    "\t\tprint(out_vocab_size, skip_ids)\n",
    "\t\tself.prediction_mask = tf.sparse.to_dense(tf.SparseTensor(\n",
    "\t\t\tindices=[skip_ids], # shape [N, ndims]. This specifies the nonzero elements' indices.\n",
    "\t\t\tvalues=[float('-inf')] * len(skip_ids),\n",
    "\t\t\tdense_shape=[out_vocab_size],\n",
    "\t\t))\n",
    "\t\n",
    "\tdef step(self, input: str|Any):\n",
    "\t\t# Data conversion\n",
    "\t\tinput_chars = tf.strings.unicode_split(input, 'UTF-8')\n",
    "\t\tinput_ids = chars_to_ids_in(input_chars)\n",
    "\t\tinput_ids = tf.reshape(input_ids, [1, -1]) # Convert to column vec\n",
    "\n",
    "\t\t# Run it!\n",
    "\t\t# predicted.shape is [batch, char, next_char_logits]\n",
    "\t\tpredicted_cmd, predicted_arg, states = self.model(inputs=input_ids, states=self.last_states, return_state=True)\n",
    "\t\tself.last_states = states\n",
    "\n",
    "\t\tprint(predicted_cmd.shape, ':: (batch_size, seq_len, num_cmds)')\n",
    "\n",
    "\t\tpredicted_next_cmd_logits = (predicted_cmd[-1, :, :]) / self.temperature\n",
    "\t\tpredicted_next_arg_logits = predicted_arg[-1, :, :] / self.temperature\n",
    "\t\tpredicted_next_arg_logits += self.prediction_mask # Sets some weights to -inf\n",
    "\n",
    "\t\tpredicted_cmd = tf.random.categorical(predicted_next_cmd_logits, num_samples=1)\n",
    "\t\tpredicted_arg_id = tf.random.categorical(predicted_next_arg_logits, num_samples=1)\n",
    "\t\tpredicted_arg = ids_to_chars_out(predicted_arg_id)\n",
    "\n",
    "\t\tpredicted_commands = np.stack((tf.squeeze(predicted_cmd), tf.squeeze(predicted_arg)), axis=1)\n",
    "\t\treturn reconstruct_from_labels(input, predicted_commands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d749297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "(1, 56, 3) :: (batch_size, seq_len, num_cmds)\n",
      "howeverx This test must fail alicesx testx the first thing \n",
      "(1, 5, 3) :: (batch_size, seq_len, num_cmds)\n",
      "once \n",
      "CPU times: user 170 ms, sys: 6.6 ms, total: 177 ms\n",
      "Wall time: 181 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "punctuator = Punctuator(model)\n",
    "\n",
    "print(punctuator.step('~however this test must fail alices test the first thing '))\n",
    "\n",
    "print(punctuator.step('~once upon a time '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee3931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
