{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71899cc9-86b4-4eda-b525-d818661c70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 19:34:41.842096: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-31 19:34:41.845902: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-31 19:34:41.862187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-31 19:34:41.885677: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-31 19:34:41.891301: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-31 19:34:41.905810: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-31 19:34:42.770257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa459db2-5a48-4bac-8cbd-38496888faee",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention) (and [at least one other](https://www.tensorflow.org/text/tutorials/text_generation) of the Tensorflow tutorials).\n",
    "- This [blog post](https://janakiev.com/blog/jupyter-virtual-envs/) was referenced to set up the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "92b150ba-48c0-4f33-8d08-b9f363d6fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "from prepare_data import load_data, reconstruct_from_labels\n",
    "from IPython.display import display, Markdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5faca0eb-c672-4d1c-884f-b5ba38a22b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_raw, target_raw = load_data('./data/en/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890fa88",
   "metadata": {},
   "source": [
    "We store the **expected** output in `target_raw` and the input to our model in `context_raw`. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bbcb7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '2', '1', ..., '2', '1', '1'], dtype='<U1')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab821739",
   "metadata": {},
   "source": [
    "Each element in `target_raw` is an operation (e.g. 0 = copy) followed by a character code. For example, `1` is \"capitalize\" and `0` is \"copy\". Note that `120` (`ord(x)`) is used for operations that take no arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ea3dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1', 'v')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_raw[24], context_raw[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f129de",
   "metadata": {},
   "source": [
    "\n",
    "## Creating a dataset\n",
    "\n",
    "We begin by vectorizing our data. `target_raw` and `context_raw` are already tokenized by characters/operations.\n",
    "\n",
    "We start by creating a vectorization for the `target_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e2805cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size in chars: 54\n"
     ]
    }
   ],
   "source": [
    "input_vocab = sorted(set(context_raw))\n",
    "print('Input vocab size in chars:', len(input_vocab))\n",
    "\n",
    "chars_to_ids_in = tf.keras.layers.StringLookup(vocabulary=input_vocab)\n",
    "# Invert: Map chars to IDs instead of IDs to chars\n",
    "ids_to_chars_in = tf.keras.layers.StringLookup(vocabulary=chars_to_ids_in.get_vocabulary(), invert=True)\n",
    "\n",
    "# in: \"Input\"\n",
    "def text_from_ids_in(ids: list[int]):\n",
    "\treturn tf.strings.reduce_join(ids_to_chars_in(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fc264b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6754252,), dtype=int64, numpy=array([40, 22, 25, ..., 31, 13,  1])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids_input = chars_to_ids_in(context_raw)\n",
    "all_ids_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b3a40",
   "metadata": {},
   "source": [
    "Now, we do the same for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1520bd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vocab size in chars: 11\n"
     ]
    }
   ],
   "source": [
    "output_vocab = sorted(set(target_raw))\n",
    "print('Output vocab size in chars:', len(output_vocab))\n",
    "\n",
    "chars_to_ids_out = tf.keras.layers.StringLookup(vocabulary=output_vocab)\n",
    "# Invert: Map chars to IDs instead of IDs to chars\n",
    "ids_to_chars_out = tf.keras.layers.StringLookup(vocabulary=chars_to_ids_out.get_vocabulary(), invert=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba83b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', \"'\", ',', '-', '.', '/', '1', '2', ':', ';', '?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d9fde0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6754252,), dtype=int64, numpy=array([7, 8, 7, ..., 8, 7, 7])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids_output = chars_to_ids_out(target_raw)\n",
    "all_ids_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af035816",
   "metadata": {},
   "source": [
    "Now that we have vectorized inputs and outputs, let's create a `Dataset` we can feed to the model.\n",
    "\n",
    "First, combine the expected inputs and outputs into a single vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e72752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6754252, 2), dtype=int64, numpy=\n",
       "array([[40,  7],\n",
       "       [22,  8],\n",
       "       [25,  7],\n",
       "       ...,\n",
       "       [31,  8],\n",
       "       [13,  7],\n",
       "       [ 1,  7]])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def column(v):\n",
    "\treturn tf.reshape(v, [-1, 1])\n",
    "\n",
    "ids_and_outputs = tf.concat([\n",
    "\tcolumn(all_ids_input), column(all_ids_output)\n",
    "], 1)\n",
    "ids_and_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45b3dd",
   "metadata": {},
   "source": [
    "Next, create a `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f1626f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~ c(1), i c(2), l c(1), l c(1), u c(1), s c(1), t c(1), r c(1), a c(1), t c(1), i c(1), o c(1), n c(1),   c(1), ~ c(1), a c(2), l c(1), i c(1), c c(1), e c(1), s c('),   c(1), a c(2), d c(1), v c(1), e c(1), n c(1), t c(1), u c(1), r c(1), e c(1), s c(1), "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 19:34:50.214256: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "def shift_inputs(inputs: list):\n",
    "\treturn tf.concat([inputs[4:], [0, 0, 0, 0]], 0)\n",
    "\n",
    "# Separates ids_and_outputs along its first dimension into different items in the dataset.\n",
    "ids_input_main = all_ids_input\n",
    "ids_input_shifted = shift_inputs(all_ids_input)\n",
    "ids_input_shifted_twice = shift_inputs(ids_input_shifted)\n",
    "\n",
    "# Tuples: from_tensor_slices pairs entries of each tuple item to produce the dataset.\n",
    "input_dataset = tf.data.Dataset.from_tensor_slices((ids_input_main, ids_input_shifted, ids_input_shifted_twice))\n",
    "output_dataset = tf.data.Dataset.from_tensor_slices(all_ids_output)\n",
    "dataset = tf.data.Dataset.zip(input_dataset, output_dataset)\n",
    "\n",
    "# Preview the dataset -- demonstrates converting Tensors to numpy to text\n",
    "for input, expected_sample_outputs in dataset.take(32):\n",
    "\tinput_char = ids_to_chars_in(input[0]).numpy().decode('utf-8')\n",
    "\toutput_char = ids_to_chars_out(expected_sample_outputs).numpy().decode('utf-8')\n",
    "\n",
    "\tprint('{} c({})'.format(input_char, output_char), end = ', ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfe60be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tf.Tensor(\n",
      "[b'~illustration ~alices adventures in wonderland ~by lewis carroll ~the millennium fulcrum edition 30 ~conte'\n",
      " b'ustration ~alices adventures in wonderland ~by lewis carroll ~the millennium fulcrum edition 30 ~contents '\n",
      " b'ation ~alices adventures in wonderland ~by lewis carroll ~the millennium fulcrum edition 30 ~contents ~cha'], shape=(3,), dtype=string)\n",
      "Inputs: tf.Tensor(\n",
      "[b'nts ~chapter i down the rabbithole chapter ii the pool of tears chapter iii a caucusrace and a long tale c'\n",
      " b'~chapter i down the rabbithole chapter ii the pool of tears chapter iii a caucusrace and a long tale chapt'\n",
      " b'pter i down the rabbithole chapter ii the pool of tears chapter iii a caucusrace and a long tale chapter i'], shape=(3,), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 19:34:50.350828: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "seq_length = 105\n",
    "\n",
    "# batch: Convert the dataset to sequences of the target size.\n",
    "# drop_remainder: Drop the last batch if it has fewer than seq_length elements\n",
    "sequences = dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for sample_inputs, expected_sample_outputs in sequences.take(2):\n",
    "\tprint('Inputs:', text_from_ids_in(sample_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30c9b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7917f",
   "metadata": {},
   "source": [
    "Our dataset now pairs inputs and labels!\n",
    "\n",
    "**Note**: This [StackOverflow](https://stackoverflow.com/questions/53171885/how-to-use-tf-data-dataset-and-tf-keras-do-multi-inputs-and-multi-outpus) question, the documentation on [Dataset.zip](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=en#zip), and documentation on [Dataset.from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=en#from_tensor_slices) were helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854f4b3",
   "metadata": {},
   "source": [
    "## Final preprocessing\n",
    "\n",
    "We now shuffle the data, then do final batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a50a19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True).prefetch(tf.data.AUTOTUNE)\n",
    "# Break into test and training data (no validation data for now).\n",
    "# Inspired by https://stackoverflow.com/a/74609848.\n",
    "validate_size = dataset.cardinality() * 1 // 8\n",
    "dataset_validate = dataset.take(validate_size)\n",
    "dataset = dataset.skip(validate_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3f818",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b47b9620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_in 55\n",
      "vocab_size_out 12\n",
      "EMBEDDING_DIM 64\n",
      "RNN_UNITS 64\n"
     ]
    }
   ],
   "source": [
    "# .get_vocabulary: Returns a list of the characters in use.\n",
    "vocab_size_in = len(chars_to_ids_in.get_vocabulary())\n",
    "\n",
    "vocab_size_out = len(chars_to_ids_out.get_vocabulary())\n",
    "size_out = vocab_size_out # Output includes both commands and the command arg\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "RNN_UNITS = 64 # Dimensionality of GRU output\n",
    "\n",
    "print('vocab_size_in', vocab_size_in)\n",
    "print('vocab_size_out', vocab_size_out)\n",
    "print('EMBEDDING_DIM', EMBEDDING_DIM)\n",
    "print('RNN_UNITS', RNN_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6287f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(tf.keras.Model):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.embedding_layer = tf.keras.layers.Embedding(vocab_size_in, EMBEDDING_DIM)\n",
    "\t\tself.merge_layer = tf.keras.layers.Concatenate()\n",
    "\t\t# return_sequences: Return the full sequence of outputs, rather than just the last.\n",
    "\t\t# return_state: Returns the last state in addition to the output\n",
    "\t\tself.gru_layer = tf.keras.layers.GRU(RNN_UNITS, return_sequences=True, return_state=True)\n",
    "\t\tself.dense_layer = tf.keras.layers.Dense(size_out, activation=tf.keras.activations.log_softmax)\n",
    "\t\n",
    "\tdef call(self, inputs, states = None, return_state = False, training = False):\n",
    "\t\tinputs_orig, inputs_ahead, inputs_ahead_x2 = inputs\n",
    "\t\t\n",
    "\t\tx = self.merge_layer([\n",
    "\t\t\tself.embedding_layer(inputs_orig, training=training),\n",
    "\t\t\tself.embedding_layer(inputs_ahead, training=training),\n",
    "\t\t\tself.embedding_layer(inputs_ahead_x2, training=training),\n",
    "\t\t])\n",
    "\t\tif states is None:\n",
    "\t\t\tbatch_size, _ = inputs_orig.shape\n",
    "\t\t\tstates = self.gru_layer.get_initial_state(batch_size)\n",
    "\n",
    "\t\tx, states = self.gru_layer(x, initial_state = states, training = training)\n",
    "\t\tx = self.dense_layer(x, training = training)\n",
    "\n",
    "\t\tif return_state:\n",
    "\t\t\treturn x, states\n",
    "\t\telse:\n",
    "\t\t\treturn x\n",
    "\n",
    "# We override tf.keras.Model to allow extracting the state later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5188bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d58d5",
   "metadata": {},
   "source": [
    "## Trying the (untrained) model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00dbe598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TakeDataset element_spec=((TensorSpec(shape=(64, 106), dtype=tf.int64, name=None), TensorSpec(shape=(64, 106), dtype=tf.int64, name=None), TensorSpec(shape=(64, 106), dtype=tf.int64, name=None)), TensorSpec(shape=(64, 106), dtype=tf.int64, name=None))>\n",
      "(64, 106, 12) :: (batch_size, seq_length, num_commands)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"language_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"language_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">106</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,520</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">106</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">106</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,536</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>))                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">106</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">780</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m106\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m3,520\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)       │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m106\u001b[0m, \u001b[38;5;34m192\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m106\u001b[0m, \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;34m64\u001b[0m,   │        \u001b[38;5;34m49,536\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m64\u001b[0m))                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m106\u001b[0m, \u001b[38;5;34m12\u001b[0m)          │           \u001b[38;5;34m780\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,836</span> (210.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m53,836\u001b[0m (210.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,836</span> (210.30 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m53,836\u001b[0m (210.30 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(dataset.take(1))\n",
    "\n",
    "for sample_inputs, expected_sample_outputs in dataset.take(1):\n",
    "\tsample_predictions = model(sample_inputs)\n",
    "\tprint(sample_predictions.shape, ':: (batch_size, seq_length, num_commands)')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa561ba",
   "metadata": {},
   "source": [
    "Now let's inspect `sample_predictions`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1793ee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106, 1)\n",
      "(106,)\n"
     ]
    }
   ],
   "source": [
    "# Take one sample of the data, where sample_cmd_predictions[0] contains log probability\n",
    "sampled_indices = tf.random.categorical(sample_predictions[0], num_samples = 1)\n",
    "print(sampled_indices.shape)\n",
    "\n",
    "# tf.squeeze: Removes dimensions of size 1.\n",
    "sampled_indices = tf.squeeze(sampled_indices).numpy()\n",
    "print(sampled_indices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ad136ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: covering a means of extrication but he himself looked so composed and so grave also i became ashamed of fe\n",
      "Next predictions: .c;oV?e/r,i,n-g 'a. [UNK]m,e/a-n;s /o?f, ;e'x?tr[UNK]i,c[UNK]a/t/i'on[UNK] 'b[UNK]u,t ;he' H.i!m's!e!l[UNK]f! /l/o:o.k.e'd; s/o .c'oMP[UNK]o.s/e?d, ,anD- ;s.o? .g?ra!v.e/ !a;lso- i- 'b-eca:m/e !a/s-h:a.m[UNK]e'd .of' F.e\n"
     ]
    }
   ],
   "source": [
    "input_text = text_from_ids_in(sample_inputs[0][0]).numpy().decode('utf-8')\n",
    "print('Input:', input_text)\n",
    "\n",
    "sampled_commands = ids_to_chars_out(sampled_indices).numpy()\n",
    "reconstructed = reconstruct_from_labels(input_text, sampled_commands)\n",
    "print('Next predictions:', reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d276d",
   "metadata": {},
   "source": [
    "Seemingly random output, as expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8707f29",
   "metadata": {},
   "source": [
    "## Training!\n",
    "\n",
    "We can train it now! It's a standard classification problem -- given the previous RNN state and the current character, predict the next character.\n",
    "\n",
    "We're using the `SparseCategoricalCrossentropy` loss. See https://datascience.stackexchange.com/a/41923 and perhaps https://stats.stackexchange.com/a/420730 for commentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b5e4b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss pre-training: 12.003934860229492\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "sample_batch_mean_loss = loss_fn(expected_sample_outputs, sample_predictions)\n",
    "print('loss pre-training:', float(tf.exp(sample_batch_mean_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0a3a3",
   "metadata": {},
   "source": [
    "As expected, the initial loss is large.\n",
    "\n",
    "Now we attach the loss function and an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7a42f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222cd99",
   "metadata": {},
   "source": [
    "We're just about ready to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00f36be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpoints\n",
    "\n",
    "checkpoint_path = './tf_model_checkpoints/checkpoint.weights.h5'\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "\tfilepath=checkpoint_path, monitor='val_loss', mode='min', save_weights_only=True, save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41e9c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ad20dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 112ms/step - loss: 0.3242 - val_loss: 0.2103\n",
      "Epoch 2/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 111ms/step - loss: 0.1665 - val_loss: 0.1948\n",
      "Epoch 3/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 113ms/step - loss: 0.1507 - val_loss: 0.1844\n",
      "Epoch 4/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - loss: 0.1406 - val_loss: 0.1754\n",
      "Epoch 5/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 87ms/step - loss: 0.1358 - val_loss: 0.1717\n",
      "Epoch 6/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 87ms/step - loss: 0.1312 - val_loss: 0.1723\n",
      "Epoch 7/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 89ms/step - loss: 0.1281 - val_loss: 0.1683\n",
      "Epoch 8/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 108ms/step - loss: 0.1260 - val_loss: 0.1661\n",
      "Epoch 9/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 110ms/step - loss: 0.1238 - val_loss: 0.1629\n",
      "Epoch 10/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 108ms/step - loss: 0.1221 - val_loss: 0.1615\n",
      "Epoch 11/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 110ms/step - loss: 0.1213 - val_loss: 0.1629\n",
      "Epoch 12/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 111ms/step - loss: 0.1197 - val_loss: 0.1638\n",
      "Epoch 13/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 110ms/step - loss: 0.1183 - val_loss: 0.1589\n",
      "Epoch 14/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 109ms/step - loss: 0.1182 - val_loss: 0.1599\n",
      "Epoch 15/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 110ms/step - loss: 0.1178 - val_loss: 0.1570\n",
      "Epoch 16/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 106ms/step - loss: 0.1170 - val_loss: 0.1600\n",
      "Epoch 17/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 115ms/step - loss: 0.1170 - val_loss: 0.1557\n",
      "Epoch 18/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 107ms/step - loss: 0.1149 - val_loss: 0.1554\n",
      "Epoch 19/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 110ms/step - loss: 0.1148 - val_loss: 0.1552\n",
      "Epoch 20/20\n",
      "\u001b[1m871/871\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 88ms/step - loss: 0.1144 - val_loss: 0.1572\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback], validation_data=dataset_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34e041",
   "metadata": {},
   "source": [
    "## Add punctuation\n",
    "\n",
    "Let's try it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "31a23d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(original: str, predicted_logits):\n",
    "\tpredicted_commands = tf.squeeze(tf.random.categorical(predicted_logits, num_samples=1))\n",
    "\treturn reconstruct_from_labels(original, ids_to_chars_out(predicted_commands).numpy())\n",
    "\n",
    "class Punctuator:\n",
    "\tdef __init__(self, model: LanguageModel, temperature: float = 1.0):\n",
    "\t\tself.temperature = temperature\n",
    "\t\tself.model = model\n",
    "\t\tself.last_states = None\n",
    "\n",
    "\t\t# See https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor?hl=en\n",
    "\t\tskip_ids = chars_to_ids_out(['[UNK]'])\n",
    "\t\tout_vocab_size = len(chars_to_ids_out.get_vocabulary())\n",
    "\t\tprint(out_vocab_size, skip_ids)\n",
    "\t\tself.prediction_mask = tf.sparse.to_dense(tf.sparse.reorder(tf.SparseTensor(\n",
    "\t\t\tindices=tf.reshape(skip_ids, [-1, 1]), # shape [N, ndims]. This specifies the nonzero elements' indices.\n",
    "\t\t\tvalues=[float('-inf')] * len(skip_ids),\n",
    "\t\t\tdense_shape=[out_vocab_size],\n",
    "\t\t)))\n",
    "\t\n",
    "\tdef step(self, input: str|Any):\n",
    "\t\t# Data conversion\n",
    "\t\tinput_chars = tf.strings.unicode_split(input, 'UTF-8')\n",
    "\t\tinput_ids = chars_to_ids_in(input_chars)\n",
    "\t\tshifted_input_ids = shift_inputs(input_ids)\n",
    "\t\tshifted_twice_input_ids = shift_inputs(shifted_input_ids)\n",
    "\t\tinputs = (\n",
    "\t\t\ttf.reshape(input_ids, [1, -1]),\n",
    "\t\t\ttf.reshape(shifted_input_ids, [1, -1]),\n",
    "\t\t\ttf.reshape(shifted_twice_input_ids, [1, -1]),\n",
    "\t\t)\n",
    "\n",
    "\t\t# Run it!\n",
    "\t\t# predicted.shape is [batch, char, next_char_logits]\n",
    "\t\tpredicted_commands_raw, states = self.model(inputs=inputs, states=self.last_states, return_state=True)\n",
    "\t\tself.last_states = states\n",
    "\n",
    "\t\tpredicted_logits = (predicted_commands_raw[-1, :, :]) / self.temperature\n",
    "\t\tpredicted_logits += self.prediction_mask # Sets some weights to -inf\n",
    "\n",
    "\t\treturn predicted_logits\n",
    "\n",
    "\tdef step_and_predict(self, original: str):\n",
    "\t\treturn logits_to_text(self.step(self, original))\n",
    "\n",
    "class CombinedPunctuator:\n",
    "\tdef __init__(self, model):\n",
    "\t\tself.punctuators = [\n",
    "\t\t\tPunctuator(model, temperature = 0.8),\n",
    "\t\t\tPunctuator(model, temperature = 0.4),\n",
    "\t\t\tPunctuator(model, temperature = 0.4),\n",
    "\t\t]\n",
    "\n",
    "\tdef step(self, text: str):\n",
    "\t\tstep_size = seq_length - 1\n",
    "\t\tnum_punctuators = len(self.punctuators)\n",
    "\t\ttext_length = len(text)\n",
    "\n",
    "\t\tall_logits = np.zeros([ text_length, vocab_size_out ])\n",
    "\n",
    "\t\tfor i in range(0, text_length, step_size):\n",
    "\t\t\tshift = -5\n",
    "\t\t\tfor punctuator in self.punctuators:\n",
    "\t\t\t\tfrom_idx = max(0, i + shift)\n",
    "\t\t\t\tto_idx = min(text_length - 1, i + step_size + shift)\n",
    "\n",
    "\t\t\t\ttext_shifted = text[from_idx:to_idx]\n",
    "\t\t\t\tpredicted_logits = punctuator.step(text_shifted)\n",
    "\t\t\t\tsliced_prediction = predicted_logits[0:to_idx-from_idx]\n",
    "\n",
    "\t\t\t\tall_logits[from_idx:to_idx] = all_logits[from_idx:to_idx] + (sliced_prediction / num_punctuators)\n",
    "\t\t\t\tshift += 5\n",
    "\t\t\t\t\t\n",
    "\t\treturn logits_to_text(text, all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2d749297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "12 tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "12 tf.Tensor([0], shape=(1,), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "~The punctuator is a small machine learning model for punctuation restoration at present its perfor-mance is rather poor. I do hope however, that with additional training, and very little rearchitecting the punctuator will be a usable and fast model. ~I suspect that I will need to look into the neural machine translation tutorials will the approach taken by the example Seq2seq model for Spanish to English translation Be sufficient will I need to learn about transformers. ~Note there are concerns about model size in addition to performance as Models will need to be run on mobile devices. ~The punctuator, was trained on old books does prose of a similar style work better heres some text from Frankenstein. ~Although it denied WarMtH SAfIe agaTha and Felix departed on a long country walk. ~Interesting perhaps It isnt any better in that Case how unfortunATe ? "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.14 s, sys: 358 ms, total: 6.5 s\n",
      "Wall time: 6.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "punctuator = CombinedPunctuator(model)\n",
    "\n",
    "text = '''~the punctuator is a small machine learning model for punctuation restoration\n",
    "at present its performance is rather poor i do hope however that with additional training\n",
    "and very little rearchitecting the punctuator will be a usable and fast model\n",
    "~i suspect that i will need to look into the neural machine translation tutorials will the\n",
    "approach taken by the example seq2seq model for spanish to english translation be sufficient\n",
    "will i need to learn about transformers\n",
    "~note there are concerns about model size in addition to performance as models will need to be run\n",
    "on mobile devices\n",
    "~the punctuator was trained on old books does prose of a similar style work better heres some text from\n",
    "frankenstein\n",
    "~although it denied warmth safie agatha and felix departed on a long country walk\n",
    "~interesting perhaps it isnt any better in that case how unfortunate \n",
    "'''\n",
    "text = text.replace('\\n', ' ')\n",
    "\n",
    "display(Markdown(punctuator.step(text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892384e9",
   "metadata": {},
   "source": [
    "That isn't working very well. For comparison, let's try an example from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "29ee3931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up somewhere ~down down down there was nothing else to do so alice soon began talking again dinahll miss me very much tonight i should think dinah was the cat i hope theyll remember her saucer of milk at teatime dinah my dear i wish you were down here with me there are no mice in the air im afraid but you might catch a bat and thats very like a mouse you know but do cats eat bats i wonder and here alice began to get rather sleepy and went on saying to herself in a dreamy sort of way do cats eat bats do cats eat bats and sometimes do bats eat cats for you see as she couldnt answer either question it didnt much matter which way she put it she felt that she was dozing off and had just begun to dream that she was walking hand in hand with dinah and saying to her very earnestly now dinah tell me the truth did you ever eat a bat when suddenly thump thump down she came upon a heap of sticks and dry leaves and the fall was over ~alice was not a bit hurt and she jumped up on to her feet in a moment she looked up but it was all dark overhead before her was another long passage and the white ra\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "up somewhere. ~Down down down There was nothing else to do so Alice soon began talking again dinahll miss Me very much tonight, I should think dinah was the cat. I hope they'll remember her saucer of milk at teatime dinah my dear I wish you were down here with me there are no mice in the air I'm Afraid, but you might Catch a bat and thats very like a mouse you know, but do cat's eat bats I wonder and here alice, began to get rather sleepy, and went on saying to herself In a dreamy sort of way do cats eat bats do cat's eat Bats and sometimes do bats eat cats for you. see, as she couldnt answer eitHer question it didn't much Matter which way she put it she felt that she was dozing off and had just begun to dream that she was walking hand in hand with dinah and saying to her very earnestly now dinah tell me the truth Did you ever eat a bat when suddenly thump thump down she came upon a heap of sticks and dry leaves and the fall was over ~Alice was not a bit hurt, and she jumped up on to her feet in a moment she looked up, but it was all dark overhead before her was another long passage, and the white R;a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig = tf.strings.reduce_join(context_raw[4400:5500]).numpy().decode('utf-8')\n",
    "print(orig)\n",
    "display(Markdown(punctuator.step(orig)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a031ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
