{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71899cc9-86b4-4eda-b525-d818661c70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 22:09:27.444218: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-30 22:09:27.447164: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-30 22:09:27.455919: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-30 22:09:27.470911: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-30 22:09:27.475175: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-30 22:09:27.487735: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-30 22:09:28.315800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa459db2-5a48-4bac-8cbd-38496888faee",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention) (and [at least one other](https://www.tensorflow.org/text/tutorials/text_generation) of the Tensorflow tutorials).\n",
    "- This [blog post](https://janakiev.com/blog/jupyter-virtual-envs/) was referenced to set up the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b150ba-48c0-4f33-8d08-b9f363d6fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "from prepare_data import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5faca0eb-c672-4d1c-884f-b5ba38a22b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_raw, context_raw = load_data('./data/en/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890fa88",
   "metadata": {},
   "source": [
    "We store the **expected** output in `target_raw` and the input to our model in `context_raw`. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ea3dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed it was labelled ORANGE MARMALADE , but to her great disappointment it was empty she did not like to drop the jar for fear of killing somebody underneath, so managed to put it into one of the cupboards as she fell past it.',\n",
       " 'either the well was very deep or she fell very slowly for she had plenty of time as she went down to look about her and to wonder what was going to happen next first she tried to look down and make out what she was coming to but it was too dark to see anything then she looked at the sides of the well and noticed that they were filled with cupboards and book shelves here and there she saw maps and pictures hung upon pegs she took down a jar from one of the shelves as she passed it was labelled orange marmalade but to her great disappointment it was empty she did not like to drop the jar for fear of killing somebody underneath so managed to put it into one of the cupboards as she fell past it')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_raw[12], context_raw[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f129de",
   "metadata": {},
   "source": [
    "Notice that `target_raw` contains punctuation, while `context_raw` does not. Each entry is paragraph length.\n",
    "\n",
    "## Creating a dataset\n",
    "\n",
    "We begin by vectorizing our data. For now, we're working with the [text generation](https://www.tensorflow.org/text/tutorials/text_generation) tutorial and so we tokenize by characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2805cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size in chars: 90\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(target_raw)\n",
    "vocab = sorted(set(text))\n",
    "print('vocab size in chars:', len(vocab))\n",
    "\n",
    "chars_to_ids = tf.keras.layers.StringLookup(vocabulary=vocab)\n",
    "# Invert: Map chars to IDs instead of IDs to chars\n",
    "ids_to_chars = tf.keras.layers.StringLookup(vocabulary=chars_to_ids.get_vocabulary(), invert=True)\n",
    "\n",
    "def text_from_ids(ids: list[int]):\n",
    "\treturn tf.strings.reduce_join(ids_to_chars(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99fc264b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4894941,), dtype=int64, numpy=array([27, 59, 59, ..., 19, 36, 46])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = chars_to_ids(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1626f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I l l u s t r a t i o n   A l i c e ' s   A d v e n t u r e s   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 22:09:30.961760: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Separates all_ids along its first dimension into different items in the dataset.\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "# Preview the dataset -- demonstrates converting Tensors to numpy to text\n",
    "for ids in ids_dataset.take(32):\n",
    "\tprint(ids_to_chars(ids).numpy().decode('utf-8'), end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe60be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"Illustration Alice's Adventures in Wonderland by Lewis Carroll THE MILLENNIUM FULCRUM EDITION 3.0 Con\", shape=(), dtype=string)\n",
      "tf.Tensor(b'tents CHAPTER I. Down the Rabbit-Hole CHAPTER II. The Pool of Tears CHAPTER III. A Caucus-Race and a ', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 22:09:31.028949: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "\n",
    "# batch: Convert the dataset to sequences of the target size.\n",
    "# drop_remainder: Drop the last batch if it has fewer than 100 elements\n",
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(2):\n",
    "\tprint(text_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2195f8",
   "metadata": {},
   "source": [
    "We now split into a dataset of $(\\text{input}, \\text{label})$ pairs. Here, `input` is the current character and `label` is the next character (expected output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4022cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['T', 'e', 'x'], ['e', 'x', 't'])\n"
     ]
    }
   ],
   "source": [
    "def split_input_label(sequence):\n",
    "\t# Shifts the label text by 1 with respect to the input so that\n",
    "\t# entrywise pairing creates the desired (input, label).\n",
    "\tinput_text = sequence[:-1]\n",
    "\tlabel_text = sequence[1:]\n",
    "\treturn input_text, label_text\n",
    "\n",
    "print(split_input_label(list('Text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02270d65",
   "metadata": {},
   "source": [
    "We create our final dataset by pairing labels and input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27b4e3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: tf.Tensor(b\"Illustration Alice's Adventures in Wonderland by Lewis Carroll THE MILLENNIUM FULCRUM EDITION 3.0 Co\", shape=(), dtype=string)\n",
      "Example label: tf.Tensor(b\"llustration Alice's Adventures in Wonderland by Lewis Carroll THE MILLENNIUM FULCRUM EDITION 3.0 Con\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_label)\n",
    "\n",
    "for example_input, example_label in dataset.take(1):\n",
    "\tprint('Example input:', text_from_ids(example_input))\n",
    "\tprint('Example label:', text_from_ids(example_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854f4b3",
   "metadata": {},
   "source": [
    "## Final preprocessing\n",
    "\n",
    "We now shuffle the data, then do final batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a50a19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True).prefetch(tf.data.AUTOTUNE)\n",
    "# Break into test and training data (no validation data for now).\n",
    "# Inspired by https://stackoverflow.com/a/74609848.\n",
    "test_size = dataset.cardinality() * 1 // 4\n",
    "dataset_test = dataset.take(test_size)\n",
    "dataset = dataset.skip(test_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3f818",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b47b9620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 91\n",
      "EMBEDDING_DIM 256\n",
      "RNN_UNITS 1024\n"
     ]
    }
   ],
   "source": [
    "# .get_vocabulary: Returns a list of the characters in use.\n",
    "vocab_size = len(chars_to_ids.get_vocabulary())\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024 # Dimensionality of GRU output\n",
    "\n",
    "print('vocab_size', vocab_size)\n",
    "print('EMBEDDING_DIM', EMBEDDING_DIM)\n",
    "print('RNN_UNITS', RNN_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6287f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(tf.keras.Model):\n",
    "\tdef __init__(self, vocab_size: int, embedding_dim: int, rnn_units: int):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\t\t# return_sequences: Return the full sequence of outputs, rather than just the last.\n",
    "\t\t# return_state: Returns the last state in addition to the output\n",
    "\t\tself.gru_layer = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "\t\tself.dense_layer = tf.keras.layers.Dense(vocab_size, activation=tf.keras.activations.log_softmax)\n",
    "\t\n",
    "\tdef call(self, inputs, states = None, return_state = False, training = False):\n",
    "\t\tx = self.embedding_layer(inputs, training = training)\n",
    "\t\tif states is None:\n",
    "\t\t\tbatch_size, _ = inputs.shape\n",
    "\t\t\tstates = self.gru_layer.get_initial_state(batch_size)\n",
    "\n",
    "\t\tx, states = self.gru_layer(x, initial_state = states, training = training)\n",
    "\t\tx = self.dense_layer(x, training = training)\n",
    "\n",
    "\t\tif return_state:\n",
    "\t\t\treturn x, states\n",
    "\t\telse:\n",
    "\t\t\treturn x\n",
    "\n",
    "# We override tf.keras.Model to allow extracting the state later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5188bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(vocab_size, EMBEDDING_DIM, RNN_UNITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d58d5",
   "metadata": {},
   "source": [
    "## Trying the (untrained) model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00dbe598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 91) :: (batch_size, seq_length, vocab_size)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 22:09:35.095353: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"language_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"language_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">23,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>))                 │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">91</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">93,275</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m23,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;34m64\u001b[0m, │     \u001b[38;5;34m3,938,304\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m1024\u001b[0m))                 │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m91\u001b[0m)          │        \u001b[38;5;34m93,275\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,054,875</span> (15.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,054,875\u001b[0m (15.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,054,875</span> (15.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,054,875\u001b[0m (15.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sample_input_batch, sample_label_batch in dataset.take(1):\n",
    "\tsample_predictions = model(sample_input_batch)\n",
    "\tprint(sample_predictions.shape, ':: (batch_size, seq_length, vocab_size)')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa561ba",
   "metadata": {},
   "source": [
    "Now let's inspect `sample_predictions`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1793ee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Take one sample of the (100,91) data, where sample_predictions[0] contains log probability\n",
    "sampled_indices = tf.random.categorical(sample_predictions[0], num_samples = 1)\n",
    "print(sampled_indices.shape)\n",
    "\n",
    "# tf.squeeze: Removes dimensions of size 1.\n",
    "sampled_indices = tf.squeeze(sampled_indices).numpy()\n",
    "print(sampled_indices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ad136ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: arecrow, stuffed with straw. Therefore I have no brains, and I come to you praying that you will put\n",
      "Next predictions: ügKES]FôOnôV]X[4ôy_RfëalêkFJ0-kxS8/nà[UNK]BsdcDEvou]0àLnQïXIYI1GIWPXKDW0nœxfB xiëmeŒBZAEd2_uYQsqyfqCxZmD\n"
     ]
    }
   ],
   "source": [
    "print('Input:', text_from_ids(sample_input_batch[0]).numpy().decode('utf-8'))\n",
    "print('Next predictions:', text_from_ids(sampled_indices).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d276d",
   "metadata": {},
   "source": [
    "Seemingly random output, as expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8707f29",
   "metadata": {},
   "source": [
    "## Training!\n",
    "\n",
    "We can train it now! It's a standard classification problem -- given the previous RNN state and the current character, predict the next character.\n",
    "\n",
    "We're using the `SparseCategoricalCrossentropy` loss. See https://datascience.stackexchange.com/a/41923 and perhaps https://stats.stackexchange.com/a/420730 for commentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b5e4b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss pre-training 90.77330017089844\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "sample_batch_mean_loss = loss_fn(sample_label_batch, sample_predictions)\n",
    "print('loss pre-training', float(tf.exp(sample_batch_mean_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0a3a3",
   "metadata": {},
   "source": [
    "As expected, the initial loss is large.\n",
    "\n",
    "Now we attach the loss function and an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7a42f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222cd99",
   "metadata": {},
   "source": [
    "We're just about ready to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "00f36be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpoints\n",
    "\n",
    "checkpoint_path = './tf_model_checkpoints/checkpoint.weights.h5'\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "\tfilepath=checkpoint_path, monitor='loss', mode='min', save_weights_only=True, save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "41e9c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2ad20dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 67/568\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13:10\u001b[0m 2s/step - loss: 1.2136"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34e041",
   "metadata": {},
   "source": [
    "## Generate text\n",
    "\n",
    "We'll run the model in a loop, keeping track of its internal state as it's executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "31a23d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "\tdef __init__(self, model: LanguageModel, ids_to_chars, chars_to_ids, temperature: float = 1.0):\n",
    "\t\tself.temperature = temperature\n",
    "\t\tself.model = model\n",
    "\t\tself.ids_to_chars = ids_to_chars\n",
    "\t\tself.chars_to_ids = chars_to_ids\n",
    "\t\tself.last_states = None\n",
    "\n",
    "\t\t# See https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor?hl=en\n",
    "\t\tskip_ids = self.chars_to_ids(['[UNK]'])\n",
    "\t\tvocab_size = len(self.chars_to_ids.get_vocabulary())\n",
    "\t\tprint(vocab_size, skip_ids)\n",
    "\t\tself.prediction_mask = tf.sparse.to_dense(tf.SparseTensor(\n",
    "\t\t\tindices=[skip_ids], # shape [N, ndims]. This specifies the nonzero elements' indices.\n",
    "\t\t\tvalues=[float('-inf')] * len(skip_ids),\n",
    "\t\t\tdense_shape=[vocab_size],\n",
    "\t\t))\n",
    "\t\n",
    "\tdef step(self, input: str|Any):\n",
    "\t\t# Data conversion\n",
    "\t\tinput_chars = tf.strings.unicode_split(input, 'UTF-8')\n",
    "\t\tinput_ids = self.chars_to_ids(input_chars)\n",
    "\t\tinput_ids = tf.reshape(input_ids, [-1, 1])\n",
    "\n",
    "\t\t# Run it!\n",
    "\t\t# predicted.shape is [batch, char, next_char_logits]\n",
    "\t\tpredicted, states = self.model(inputs=input_ids, states=self.last_states, return_state=True)\n",
    "\t\tself.last_states = states\n",
    "\n",
    "\t\tpredicted_next_char_logits = predicted[-1, -1, :]\n",
    "\t\tpredicted_next_char_logits /= self.temperature\n",
    "\t\tpredicted_next_char_logits += self.prediction_mask # Sets some weights to -inf\n",
    "\n",
    "\t\tpredicted_ids = tf.random.categorical([predicted_next_char_logits], num_samples=1)\n",
    "\t\tpredicted_ids = tf.squeeze(predicted_ids)\n",
    "\n",
    "\t\tpredicted_chars = self.ids_to_chars(predicted_ids)\n",
    "\t\treturn predicted_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2d749297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "into! said Forthato, as everywhere, said Caderousse alone, do you pain point, but I read it by me? h\n",
      "e evinced his arr for weaken and conveysion. Sire, of doubt, because you would have been regard thei\n",
      "r senses' thanks, said Dantès, in-reating him with harm, I might tupe to make this city to look, att\n",
      "entively may by one with her attainness, here as Care. Linton, my boy, multard she had a very veghin\n",
      "g to opened my cries I said I'm sure, said the Hith and Thomsancles, that was the Tumbus door, if th\n",
      "is person who knows what is it to make up incurning, calmly may have seen at once there the external\n",
      " giver ber. Miss, the attention of Catherine! Pashood you must enough what many disputant countess I\n",
      " have already a prefermilable excellent matter?' He repulsed Franz audest it would have finished you\n",
      "r breath in the master of all the propositive foundess he was creature, and a very possible name. He\n",
      " recommenced came much plainly substitude in his hair, and learning the curboards, when in the same \n",
      "CPU times: user 24.4 s, sys: 967 ms, total: 25.4 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_generator = TextGenerator(model, ids_to_chars, chars_to_ids)\n",
    "next_char = 'Hello, '\n",
    "\n",
    "for i in range(10):\n",
    "\toutput = []\n",
    "\n",
    "\tfor j in range(100):\n",
    "\t\tnext_char = text_generator.step(next_char)\n",
    "\t\toutput.append(next_char)\n",
    "\n",
    "\tprint(tf.strings.join(output).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee3931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
