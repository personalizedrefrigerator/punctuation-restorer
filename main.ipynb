{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71899cc9-86b4-4eda-b525-d818661c70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 11:04:05.959630: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-31 11:04:06.132591: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-31 11:04:06.221688: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-31 11:04:06.353163: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-31 11:04:06.385276: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-31 11:04:06.567566: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-31 11:04:08.091968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa459db2-5a48-4bac-8cbd-38496888faee",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "- This notebook follows [an online tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention) (and [at least one other](https://www.tensorflow.org/text/tutorials/text_generation) of the Tensorflow tutorials).\n",
    "- This [blog post](https://janakiev.com/blog/jupyter-virtual-envs/) was referenced to set up the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b150ba-48c0-4f33-8d08-b9f363d6fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "from prepare_data import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5faca0eb-c672-4d1c-884f-b5ba38a22b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_raw, context_raw = load_data('./data/en/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890fa88",
   "metadata": {},
   "source": [
    "We store the **expected** output in `target_raw` and the input to our model in `context_raw`. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ea3dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. First, she tried to look down and make out what she was coming to, but it was too dark to see anything then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves here and there she saw maps and pictures hung upon pegs. She took down a jar from one of the shelves as she passed it was labelled ORANGE MARMALADE , but to her great disappointment it was empty she did not like to drop the jar for fear of killing somebody underneath, so managed to put it into one of the cupboards as she fell past it.',\n",
       " 'either the well was very deep or she fell very slowly for she had plenty of time as she went down to look about her and to wonder what was going to happen next first she tried to look down and make out what she was coming to but it was too dark to see anything then she looked at the sides of the well and noticed that they were filled with cupboards and book shelves here and there she saw maps and pictures hung upon pegs she took down a jar from one of the shelves as she passed it was labelled orange marmalade but to her great disappointment it was empty she did not like to drop the jar for fear of killing somebody underneath so managed to put it into one of the cupboards as she fell past it')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_raw[12], context_raw[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f129de",
   "metadata": {},
   "source": [
    "Notice that `target_raw` contains punctuation, while `context_raw` does not. Each entry is paragraph length.\n",
    "\n",
    "## Creating a dataset\n",
    "\n",
    "We begin by vectorizing our data. For now, we're working with the [text generation](https://www.tensorflow.org/text/tutorials/text_generation) tutorial and so we tokenize by characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2805cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size in chars: 89\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(target_raw)\n",
    "vocab = sorted(set(text))\n",
    "print('vocab size in chars:', len(vocab))\n",
    "\n",
    "chars_to_ids = tf.keras.layers.StringLookup(vocabulary=vocab)\n",
    "# Invert: Map chars to IDs instead of IDs to chars\n",
    "ids_to_chars = tf.keras.layers.StringLookup(vocabulary=chars_to_ids.get_vocabulary(), invert=True)\n",
    "\n",
    "def text_from_ids(ids: list[int]):\n",
    "\treturn tf.strings.reduce_join(ids_to_chars(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99fc264b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6376070,), dtype=int64, numpy=array([27, 58, 58, ..., 19, 36, 46])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = chars_to_ids(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1626f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I l l u s t r a t i o n   A l i c e ' s   A d v e n t u r e s   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 11:04:12.615895: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Separates all_ids along its first dimension into different items in the dataset.\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "# Preview the dataset -- demonstrates converting Tensors to numpy to text\n",
    "for ids in ids_dataset.take(32):\n",
    "\tprint(ids_to_chars(ids).numpy().decode('utf-8'), end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe60be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"Illustration Alice's Adventures in Wonderland by Lewis Carroll TH\", shape=(), dtype=string)\n",
      "tf.Tensor(b'E MILLENNIUM FULCRUM EDITION 3.0 Contents CHAPTER I. Down the Rab', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 11:04:12.664575: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "seq_length = 64\n",
    "\n",
    "# batch: Convert the dataset to sequences of the target size.\n",
    "# drop_remainder: Drop the last batch if it has fewer than 80 elements\n",
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(2):\n",
    "\tprint(text_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2195f8",
   "metadata": {},
   "source": [
    "We now split into a dataset of $(\\text{input}, \\text{label})$ pairs. Here, `input` is the current character and `label` is the next character (expected output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4022cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['T', 'e', 'x'], ['e', 'x', 't'])\n"
     ]
    }
   ],
   "source": [
    "def split_input_label(sequence):\n",
    "\t# Shifts the label text by 1 with respect to the input so that\n",
    "\t# entrywise pairing creates the desired (input, label).\n",
    "\tinput_text = sequence[:-1]\n",
    "\tlabel_text = sequence[1:]\n",
    "\treturn input_text, label_text\n",
    "\n",
    "print(split_input_label(list('Text')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02270d65",
   "metadata": {},
   "source": [
    "We create our final dataset by pairing labels and input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27b4e3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: tf.Tensor(b\"Illustration Alice's Adventures in Wonderland by Lewis Carroll T\", shape=(), dtype=string)\n",
      "Example label: tf.Tensor(b\"llustration Alice's Adventures in Wonderland by Lewis Carroll TH\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_label)\n",
    "\n",
    "for example_input, example_label in dataset.take(1):\n",
    "\tprint('Example input:', text_from_ids(example_input))\n",
    "\tprint('Example label:', text_from_ids(example_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854f4b3",
   "metadata": {},
   "source": [
    "## Final preprocessing\n",
    "\n",
    "We now shuffle the data, then do final batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a50a19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True).prefetch(tf.data.AUTOTUNE)\n",
    "# Break into test and training data (no validation data for now).\n",
    "# Inspired by https://stackoverflow.com/a/74609848.\n",
    "# test_size = dataset.cardinality() * 1 // 4\n",
    "# dataset_test = dataset.take(test_size)\n",
    "# dataset = dataset.skip(test_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3f818",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b47b9620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 90\n",
      "EMBEDDING_DIM 32\n",
      "RNN_UNITS 64\n"
     ]
    }
   ],
   "source": [
    "# .get_vocabulary: Returns a list of the characters in use.\n",
    "vocab_size = len(chars_to_ids.get_vocabulary())\n",
    "\n",
    "EMBEDDING_DIM = 32\n",
    "RNN_UNITS = 64 # Dimensionality of GRU output\n",
    "\n",
    "print('vocab_size', vocab_size)\n",
    "print('EMBEDDING_DIM', EMBEDDING_DIM)\n",
    "print('RNN_UNITS', RNN_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6287f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(tf.keras.Model):\n",
    "\tdef __init__(self, vocab_size: int, embedding_dim: int, rnn_units: int):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\t\t# return_sequences: Return the full sequence of outputs, rather than just the last.\n",
    "\t\t# return_state: Returns the last state in addition to the output\n",
    "\t\tself.gru_layer = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "\t\tself.dense_layer = tf.keras.layers.Dense(vocab_size, activation=tf.keras.activations.log_softmax)\n",
    "\t\n",
    "\tdef call(self, inputs, states = None, return_state = False, training = False):\n",
    "\t\tx = self.embedding_layer(inputs, training = training)\n",
    "\t\tif states is None:\n",
    "\t\t\tbatch_size, _ = inputs.shape\n",
    "\t\t\tstates = self.gru_layer.get_initial_state(batch_size)\n",
    "\n",
    "\t\tx, states = self.gru_layer(x, initial_state = states, training = training)\n",
    "\t\tx = self.dense_layer(x, training = training)\n",
    "\n",
    "\t\tif return_state:\n",
    "\t\t\treturn x, states\n",
    "\t\telse:\n",
    "\t\t\treturn x\n",
    "\n",
    "# We override tf.keras.Model to allow extracting the state later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5188bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(vocab_size, EMBEDDING_DIM, RNN_UNITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d58d5",
   "metadata": {},
   "source": [
    "## Trying the (untrained) model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00dbe598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 90) :: (batch_size, seq_length, vocab_size)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 11:04:13.831817: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"language_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"language_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,880</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,816</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>))                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,850</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)           │         \u001b[38;5;34m2,880\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;34m64\u001b[0m,    │        \u001b[38;5;34m18,816\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m64\u001b[0m))                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m90\u001b[0m)           │         \u001b[38;5;34m5,850\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,546</span> (107.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,546\u001b[0m (107.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,546</span> (107.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m27,546\u001b[0m (107.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sample_input_batch, sample_label_batch in dataset.take(1):\n",
    "\tsample_predictions = model(sample_input_batch)\n",
    "\tprint(sample_predictions.shape, ':: (batch_size, seq_length, vocab_size)')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa561ba",
   "metadata": {},
   "source": [
    "Now let's inspect `sample_predictions`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1793ee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1)\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "# Take one sample of the (100,91) data, where sample_predictions[0] contains log probability\n",
    "sampled_indices = tf.random.categorical(sample_predictions[0], num_samples = 1)\n",
    "print(sampled_indices.shape)\n",
    "\n",
    "# tf.squeeze: Removes dimensions of size 1.\n",
    "sampled_indices = tf.squeeze(sampled_indices).numpy()\n",
    "print(sampled_indices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ad136ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  your history, she do. I'll tell it her, said the Mock Turtle in\n",
      "Next predictions: ï0po.TŒWüœTLum FPîS525XpnSÉ ïJo6ŒŒQJë4yQlrvRo?PI1î.1â[UNK]?Æhàêoë9R2\n"
     ]
    }
   ],
   "source": [
    "print('Input:', text_from_ids(sample_input_batch[0]).numpy().decode('utf-8'))\n",
    "print('Next predictions:', text_from_ids(sampled_indices).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d276d",
   "metadata": {},
   "source": [
    "Seemingly random output, as expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8707f29",
   "metadata": {},
   "source": [
    "## Training!\n",
    "\n",
    "We can train it now! It's a standard classification problem -- given the previous RNN state and the current character, predict the next character.\n",
    "\n",
    "We're using the `SparseCategoricalCrossentropy` loss. See https://datascience.stackexchange.com/a/41923 and perhaps https://stats.stackexchange.com/a/420730 for commentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b5e4b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss pre-training 89.98168182373047\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "sample_batch_mean_loss = loss_fn(sample_label_batch, sample_predictions)\n",
    "print('loss pre-training', float(tf.exp(sample_batch_mean_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0a3a3",
   "metadata": {},
   "source": [
    "As expected, the initial loss is large.\n",
    "\n",
    "Now we attach the loss function and an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7a42f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222cd99",
   "metadata": {},
   "source": [
    "We're just about ready to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00f36be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpoints\n",
    "\n",
    "checkpoint_path = './tf_model_checkpoints/checkpoint.weights.h5'\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "\tfilepath=checkpoint_path, monitor='loss', mode='min', save_weights_only=True, save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41e9c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ad20dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1532/1532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 50ms/step - loss: 1.6387\n",
      "Epoch 2/5\n",
      "\u001b[1m1532/1532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 47ms/step - loss: 1.6278\n",
      "Epoch 3/5\n",
      "\u001b[1m1532/1532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 50ms/step - loss: 1.6189\n",
      "Epoch 4/5\n",
      "\u001b[1m1532/1532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 50ms/step - loss: 1.6118\n",
      "Epoch 5/5\n",
      "\u001b[1m1532/1532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 47ms/step - loss: 1.6061\n"
     ]
    }
   ],
   "source": [
    "# Note: This block was run >1 times (twice)\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34e041",
   "metadata": {},
   "source": [
    "## Generate text\n",
    "\n",
    "We'll run the model in a loop, keeping track of its internal state as it's executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31a23d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "\tdef __init__(self, model: LanguageModel, ids_to_chars, chars_to_ids, temperature: float = 1.0):\n",
    "\t\tself.temperature = temperature\n",
    "\t\tself.model = model\n",
    "\t\tself.ids_to_chars = ids_to_chars\n",
    "\t\tself.chars_to_ids = chars_to_ids\n",
    "\t\tself.last_states = None\n",
    "\n",
    "\t\t# See https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor?hl=en\n",
    "\t\tskip_ids = self.chars_to_ids(['[UNK]'])\n",
    "\t\tvocab_size = len(self.chars_to_ids.get_vocabulary())\n",
    "\t\tprint(vocab_size, skip_ids)\n",
    "\t\tself.prediction_mask = tf.sparse.to_dense(tf.SparseTensor(\n",
    "\t\t\tindices=[skip_ids], # shape [N, ndims]. This specifies the nonzero elements' indices.\n",
    "\t\t\tvalues=[float('-inf')] * len(skip_ids),\n",
    "\t\t\tdense_shape=[vocab_size],\n",
    "\t\t))\n",
    "\t\n",
    "\tdef step(self, input: str|Any):\n",
    "\t\t# Data conversion\n",
    "\t\tinput_chars = tf.strings.unicode_split(input, 'UTF-8')\n",
    "\t\tinput_ids = self.chars_to_ids(input_chars)\n",
    "\t\tinput_ids = tf.reshape(input_ids, [-1, 1])\n",
    "\n",
    "\t\t# Run it!\n",
    "\t\t# predicted.shape is [batch, char, next_char_logits]\n",
    "\t\tpredicted, states = self.model(inputs=input_ids, states=self.last_states, return_state=True)\n",
    "\t\tself.last_states = states\n",
    "\n",
    "\t\tpredicted_next_char_logits = predicted[-1, -1, :]\n",
    "\t\tpredicted_next_char_logits /= self.temperature\n",
    "\t\tpredicted_next_char_logits += self.prediction_mask # Sets some weights to -inf\n",
    "\n",
    "\t\tpredicted_ids = tf.random.categorical([predicted_next_char_logits], num_samples=1)\n",
    "\t\tpredicted_ids = tf.squeeze(predicted_ids)\n",
    "\n",
    "\t\tpredicted_chars = self.ids_to_chars(predicted_ids)\n",
    "\t\treturn predicted_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d749297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "an encountre. We prieficiI. Amer, I new me what streng in thoughes, jull course of the past, and the\n",
      "y were with accroy. I know mise a secreties is blood of noise and thinks regay Valently. I was Allie\n",
      ", visit in admy of the counter first for that nowsible, count to me. Then sere moment on the paies. \n",
      "He from theet upon the peace is thinh preature. Chattan concious boy to als, Bamp by herd beloarser,\n",
      " Pronaling generence? And read thy pud to his invents dew at to under the hount of fell which Those \n",
      "to not a ladynown methers a gidently beliead had es, or very splied Monte Crissable, would never be \n",
      "hudias before, and the stracefuse to eptrorm had him was should being which would leave mamper of so\n",
      "nvoles. Thessend. The sort would dests and woulf, over hunglune in the bestleen, I went has into any\n",
      "one tongre Master, I busious justy, me for it, and poor it have stere exclised he breart, unce staci\n",
      ", canam, time, now for battless turned Hix the one to be no humberios it the giviture I sawe, the vi\n",
      "CPU times: user 9.48 s, sys: 476 ms, total: 9.95 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text_generator = TextGenerator(model, ids_to_chars, chars_to_ids)\n",
    "next_char = 'Hello, '\n",
    "\n",
    "for i in range(10):\n",
    "\toutput = []\n",
    "\n",
    "\tfor j in range(100):\n",
    "\t\tnext_char = text_generator.step(next_char)\n",
    "\t\toutput.append(next_char)\n",
    "\n",
    "\tprint(tf.strings.join(output).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee3931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
